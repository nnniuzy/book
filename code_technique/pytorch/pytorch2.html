
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>pytorch常用代码段合集 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="pytorch_train.html" />
    
    
    <link rel="prev" href="pytorch1.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../../">
            
                <a href="../../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../../introduction/0.html">
            
                <a href="../../introduction/0.html">
            
                    
                    送给研一入学的你们—炼丹师入门手册
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../../introduction/AI_system.html">
            
                <a href="../../introduction/AI_system.html">
            
                    
                    为什么要使得AI System具备可解释性呢？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../">
            
                <a href="../">
            
                    
                    编程技巧
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../python/python_technique.html">
            
                <a href="../python/python_technique.html">
            
                    
                    python编程技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../python/sort.html">
            
                <a href="../python/sort.html">
            
                    
                    python常见排序
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../python/opencv.html">
            
                <a href="../python/opencv.html">
            
                    
                    opencv-python极速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="pytorch1.html">
            
                <a href="pytorch1.html">
            
                    
                    pytorch常用代码
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.5" data-path="pytorch2.html">
            
                <a href="pytorch2.html">
            
                    
                    pytorch常用代码段合集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="pytorch_train.html">
            
                <a href="pytorch_train.html">
            
                    
                    pytorch训练技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="pytorch_.html">
            
                <a href="pytorch_.html">
            
                    
                    pytorch解冻
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="pytorch_vision.html">
            
                <a href="pytorch_vision.html">
            
                    
                    pytorch网络可视化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="PSNR_SSIM.html">
            
                <a href="PSNR_SSIM.html">
            
                    
                    PSNR&&SSIM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="SPP.html">
            
                <a href="SPP.html">
            
                    
                    SPP
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="Tensor_to_img_imge_to_tensor.html">
            
                <a href="Tensor_to_img_imge_to_tensor.html">
            
                    
                    Tensor to img && imge to tensor
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../../linux/">
            
                <a href="../../linux/">
            
                    
                    Linux
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../../linux/linux_technique.html">
            
                <a href="../../linux/linux_technique.html">
            
                    
                    Linux技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../../linux/linux_GPU.html">
            
                <a href="../../linux/linux_GPU.html">
            
                    
                    Linux显卡驱动修复
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../../paper/">
            
                <a href="../../paper/">
            
                    
                    论文
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../../paper/paper_write.html">
            
                <a href="../../paper/paper_write.html">
            
                    
                    论文写作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../../paper/Image_to_Image.html">
            
                <a href="../../paper/Image_to_Image.html">
            
                    
                    Image-to-Image 的论文汇总（含 GitHub 代码）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="../../paper/GNN.html">
            
                <a href="../../paper/GNN.html">
            
                    
                    GNN综述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.4" data-path="../../paper/Perceptual_GAN_for_Small_Object_Detection.html">
            
                <a href="../../paper/Perceptual_GAN_for_Small_Object_Detection.html">
            
                    
                    Perceptual GAN for Small Object Detection阅读笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.5" data-path="../../paper/GMMN.html">
            
                <a href="../../paper/GMMN.html">
            
                    
                    GAN变体-GMMN 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.6" data-path="../../paper/Deformable_Kernels.html">
            
                <a href="../../paper/Deformable_Kernels.html">
            
                    
                    图像视频去噪中的Deformable Kernels
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.7" data-path="../../paper/Isolating_Sources_of_Disentanglement_in_VAEs.html">
            
                <a href="../../paper/Isolating_Sources_of_Disentanglement_in_VAEs.html">
            
                    
                    Isolating Sources of Disentanglement in VAEs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.8" data-path="../../paper/Spectral_Normalization.html">
            
                <a href="../../paper/Spectral_Normalization.html">
            
                    
                    Spectral Normalization 谱归一化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.9" data-path="../../paper/Unbalanced_sample_loss.html">
            
                <a href="../../paper/Unbalanced_sample_loss.html">
            
                    
                    不均衡样本loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.10" data-path="../../paper/NN.html">
            
                <a href="../../paper/NN.html">
            
                    
                    论文神经网络示意图
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../../super_resolution/">
            
                <a href="../../super_resolution/">
            
                    
                    超分辨率
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../../super_resolution/SR_summarize.html">
            
                <a href="../../super_resolution/SR_summarize.html">
            
                    
                    超分辨率方向综述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../../super_resolution/SR.html">
            
                <a href="../../super_resolution/SR.html">
            
                    
                    超分辨率技术
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../../super_resolution/code_dataset.html">
            
                <a href="../../super_resolution/code_dataset.html">
            
                    
                    超分辨率代码数据集合集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../../super_resolution/baseline.html">
            
                <a href="../../super_resolution/baseline.html">
            
                    
                    超分辨率baseline
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../../super_resolution/loss.html">
            
                <a href="../../super_resolution/loss.html">
            
                    
                    超分辨率的损失函数总结
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../../data/">
            
                <a href="../../data/">
            
                    
                    图片和数据处理
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="../../data/picture.html">
            
                <a href="../../data/picture.html">
            
                    
                    图片处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.2" data-path="../../data/picture_enhance.html">
            
                <a href="../../data/picture_enhance.html">
            
                    
                    图像数据增强
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.3" data-path="../../data/data.html">
            
                <a href="../../data/data.html">
            
                    
                    数据增强
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.4" data-path="../../data/imaaug.html">
            
                <a href="../../data/imaaug.html">
            
                    
                    imaaug 数据增强大杀器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../../math/">
            
                <a href="../../math/">
            
                    
                    数学
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.2.1" data-path="../../math/matrix.html">
            
                <a href="../../math/matrix.html">
            
                    
                    矩阵总结
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.2" data-path="../../math/distribution_show.html">
            
                <a href="../../math/distribution_show.html">
            
                    
                    分布
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.3" data-path="../../math/affine_transformation.html">
            
                <a href="../../math/affine_transformation.html">
            
                    
                    仿射变换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.4" data-path="../../math/graph.html">
            
                <a href="../../math/graph.html">
            
                    
                    图的基本概念
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../../other/">
            
                <a href="../../other/">
            
                    
                    其他
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.3.1" data-path="../../other/discriminator_train.html">
            
                <a href="../../other/discriminator_train.html">
            
                    
                    判别器训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.2" data-path="../../other/FLOPS.html">
            
                <a href="../../other/FLOPS.html">
            
                    
                    网络FLOPS计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../.." >pytorch常用代码段合集</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h3 id="pytorch&#x5E38;&#x7528;&#x4EE3;&#x7801;&#x6BB5;&#x6574;&#x7406;&#x5408;&#x96C6;">PyTorch&#x5E38;&#x7528;&#x4EE3;&#x7801;&#x6BB5;&#x6574;&#x7406;&#x5408;&#x96C6;</h3>
<p> <a href="https://www.ctolib.com/user/pe-12647.html" target="_blank">HudsonEvangeline</a> &#x53D1;&#x5E03;&#x4E8E;2&#x5C0F;&#x65F6;&#x524D; &#x9605;&#x8BFB;61&#x6B21;</p>
<p> 5 &#x4EBA;&#x70B9;&#x8D5E;  <a href="https://www.ctolib.com/topics-140819.html#commentForm" target="_blank"> 0 &#x6761;&#x8BC4;&#x8BBA;</a></p>
<p>&#x672C;&#x6587;&#x4EE3;&#x7801;&#x57FA;&#x4E8E; PyTorch 1.0 &#x7248;&#x672C;&#xFF0C;&#x9700;&#x8981;&#x7528;&#x5230;&#x4EE5;&#x4E0B;&#x5305;</p>
<pre><code>import collections
import os
import shutil
import tqdm

import numpy as np
import PIL.Image
import torch
import torchvision
</code></pre><p>&#x57FA;&#x7840;&#x914D;&#x7F6E;</p>
<p>&#x68C0;&#x67E5; PyTorch &#x7248;&#x672C;</p>
<pre><code>torch.__version__               # PyTorch version
torch.version.cuda              # Corresponding CUDA version
torch.backends.cudnn.version()  # Corresponding cuDNN version
torch.cuda.get_device_name(0)   # GPU type
</code></pre><p>&#x66F4;&#x65B0; PyTorch</p>
<p>PyTorch &#x5C06;&#x88AB;&#x5B89;&#x88C5;&#x5728; anaconda3/lib/python3.7/site-packages/torch/&#x76EE;&#x5F55;&#x4E0B;&#x3002;</p>
<pre><code>conda update pytorch torchvision -c pytorch
</code></pre><p>&#x56FA;&#x5B9A;&#x968F;&#x673A;&#x79CD;&#x5B50;</p>
<pre><code>torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
</code></pre><p>&#x6307;&#x5B9A;&#x7A0B;&#x5E8F;&#x8FD0;&#x884C;&#x5728;&#x7279;&#x5B9A; GPU &#x5361;&#x4E0A;</p>
<p>&#x5728;&#x547D;&#x4EE4;&#x884C;&#x6307;&#x5B9A;&#x73AF;&#x5883;&#x53D8;&#x91CF;</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py
</code></pre><p>&#x6216;&#x5728;&#x4EE3;&#x7801;&#x4E2D;&#x6307;&#x5B9A;</p>
<pre><code>os.environ[&apos;CUDA_VISIBLE_DEVICES&apos;] = &apos;0,1&apos;
</code></pre><p>&#x5224;&#x65AD;&#x662F;&#x5426;&#x6709; CUDA &#x652F;&#x6301;</p>
<pre><code>torch.cuda.is_available()
</code></pre><p>&#x8BBE;&#x7F6E;&#x4E3A; cuDNN benchmark &#x6A21;&#x5F0F;</p>
<p>Benchmark &#x6A21;&#x5F0F;&#x4F1A;&#x63D0;&#x5347;&#x8BA1;&#x7B97;&#x901F;&#x5EA6;&#xFF0C;&#x4F46;&#x662F;&#x7531;&#x4E8E;&#x8BA1;&#x7B97;&#x4E2D;&#x6709;&#x968F;&#x673A;&#x6027;&#xFF0C;&#x6BCF;&#x6B21;&#x7F51;&#x7EDC;&#x524D;&#x9988;&#x7ED3;&#x679C;&#x7565;&#x6709;&#x5DEE;&#x5F02;&#x3002;</p>
<pre><code>torch.backends.cudnn.benchmark = True
</code></pre><p>&#x5982;&#x679C;&#x60F3;&#x8981;&#x907F;&#x514D;&#x8FD9;&#x79CD;&#x7ED3;&#x679C;&#x6CE2;&#x52A8;&#xFF0C;&#x8BBE;&#x7F6E;</p>
<pre><code>torch.backends.cudnn.deterministic = True
</code></pre><p>&#x6E05;&#x9664; GPU &#x5B58;&#x50A8;</p>
<p>&#x6709;&#x65F6; Control-C &#x4E2D;&#x6B62;&#x8FD0;&#x884C;&#x540E; GPU &#x5B58;&#x50A8;&#x6CA1;&#x6709;&#x53CA;&#x65F6;&#x91CA;&#x653E;&#xFF0C;&#x9700;&#x8981;&#x624B;&#x52A8;&#x6E05;&#x7A7A;&#x3002;&#x5728; PyTorch &#x5185;&#x90E8;&#x53EF;&#x4EE5;</p>
<pre><code>torch.cuda.empty_cache()
</code></pre><p>&#x6216;&#x5728;&#x547D;&#x4EE4;&#x884C;&#x53EF;&#x4EE5;&#x5148;&#x4F7F;&#x7528; ps &#x627E;&#x5230;&#x7A0B;&#x5E8F;&#x7684; PID&#xFF0C;&#x518D;&#x4F7F;&#x7528; kill &#x7ED3;&#x675F;&#x8BE5;&#x8FDB;&#x7A0B;</p>
<pre><code>ps aux | grep pythonkill -9 [pid]
</code></pre><p>&#x6216;&#x8005;&#x76F4;&#x63A5;&#x91CD;&#x7F6E;&#x6CA1;&#x6709;&#x88AB;&#x6E05;&#x7A7A;&#x7684; GPU</p>
<pre><code>nvidia-smi --gpu-reset -i [gpu_id]
</code></pre><p>&#x5F20;&#x91CF;&#x5904;&#x7406;</p>
<p>&#x5F20;&#x91CF;&#x57FA;&#x672C;&#x4FE1;&#x606F;</p>
<pre><code>tensor.type()   # Data type
tensor.size()   # Shape of the tensor. It is a subclass of Python tuple
tensor.dim()    # Number of dimensions.
</code></pre><p>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x8F6C;&#x6362;</p>
<pre><code># Set default tensor type. Float in PyTorch is much faster than double.
torch.set_default_tensor_type(torch.FloatTensor)

# Type convertions.
tensor = tensor.cuda()
tensor = tensor.cpu()
tensor = tensor.float()
tensor = tensor.long()
</code></pre><p>torch.Tensor &#x4E0E; np.ndarray &#x8F6C;&#x6362;</p>
<pre><code># torch.Tensor -&gt; np.ndarray.
ndarray = tensor.cpu().numpy()

# np.ndarray -&gt; torch.Tensor.
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float()  # If ndarray has negative stride
</code></pre><p>torch.Tensor &#x4E0E; PIL.Image &#x8F6C;&#x6362;</p>
<p>PyTorch &#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x9ED8;&#x8BA4;&#x91C7;&#x7528; N&#xD7;D&#xD7;H&#xD7;W &#x7684;&#x987A;&#x5E8F;&#xFF0C;&#x5E76;&#x4E14;&#x6570;&#x636E;&#x8303;&#x56F4;&#x5728; [0, 1]&#xFF0C;&#x9700;&#x8981;&#x8FDB;&#x884C;&#x8F6C;&#x7F6E;&#x548C;&#x89C4;&#x8303;&#x5316;&#x3002;</p>
<pre><code># torch.Tensor -&gt; PIL.Image.
image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255
    ).byte().permute(1, 2, 0).cpu().numpy())
image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way

# PIL.Image -&gt; torch.Tensor.
tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))
    ).permute(2, 0, 1).float() / 255
tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))  # Equivalently way
</code></pre><p>np.ndarray &#x4E0E; PIL.Image &#x8F6C;&#x6362;</p>
<pre><code># np.ndarray -&gt; PIL.Image.
image = PIL.Image.fromarray(ndarray.astypde(np.uint8))

# PIL.Image -&gt; np.ndarray.
ndarray = np.asarray(PIL.Image.open(path))
</code></pre><p>&#x4ECE;&#x53EA;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5F20;&#x91CF;&#x4E2D;&#x63D0;&#x53D6;&#x503C;</p>
<p>&#x8FD9;&#x5728;&#x8BAD;&#x7EC3;&#x65F6;&#x7EDF;&#x8BA1; loss &#x7684;&#x53D8;&#x5316;&#x8FC7;&#x7A0B;&#x4E2D;&#x7279;&#x522B;&#x6709;&#x7528;&#x3002;&#x5426;&#x5219;&#x8FD9;&#x5C06;&#x7D2F;&#x79EF;&#x8BA1;&#x7B97;&#x56FE;&#xFF0C;&#x4F7F; GPU &#x5B58;&#x50A8;&#x5360;&#x7528;&#x91CF;&#x8D8A;&#x6765;&#x8D8A;&#x5927;&#x3002;</p>
<pre><code>value = tensor.item()
</code></pre><p>&#x5F20;&#x91CF;&#x5F62;&#x53D8;</p>
<p>&#x5F20;&#x91CF;&#x5F62;&#x53D8;&#x5E38;&#x5E38;&#x9700;&#x8981;&#x7528;&#x4E8E;&#x5C06;&#x5377;&#x79EF;&#x5C42;&#x7279;&#x5F81;&#x8F93;&#x5165;&#x5168;&#x8FDE;&#x63A5;&#x5C42;&#x7684;&#x60C5;&#x5F62;&#x3002;&#x76F8;&#x6BD4; torch.view&#xFF0C;torch.reshape &#x53EF;&#x4EE5;&#x81EA;&#x52A8;&#x5904;&#x7406;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0D;&#x8FDE;&#x7EED;&#x7684;&#x60C5;&#x51B5;&#x3002;</p>
<pre><code>tensor = torch.reshape(tensor, shape)
</code></pre><p>&#x6253;&#x4E71;&#x987A;&#x5E8F;</p>
<pre><code>tensor = tensor[torch.randperm(tensor.size(0))]  # Shuffle the first dimension
</code></pre><p>&#x6C34;&#x5E73;&#x7FFB;&#x8F6C;</p>
<p>PyTorch &#x4E0D;&#x652F;&#x6301; tensor[::-1] &#x8FD9;&#x6837;&#x7684;&#x8D1F;&#x6B65;&#x957F;&#x64CD;&#x4F5C;&#xFF0C;&#x6C34;&#x5E73;&#x7FFB;&#x8F6C;&#x53EF;&#x4EE5;&#x7528;&#x5F20;&#x91CF;&#x7D22;&#x5F15;&#x5B9E;&#x73B0;&#x3002;</p>
<pre><code># Assume tensor has shape N*D*H*W.tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()]
</code></pre><p>&#x590D;&#x5236;&#x5F20;&#x91CF;</p>
<p>&#x6709;&#x4E09;&#x79CD;&#x590D;&#x5236;&#x7684;&#x65B9;&#x5F0F;&#xFF0C;&#x5BF9;&#x5E94;&#x4E0D;&#x540C;&#x7684;&#x9700;&#x6C42;&#x3002;</p>
<pre><code># Operation                 |  New/Shared memory | Still in computation graph |
tensor.clone()            # |        New         |          Yes               |
tensor.detach()           # |      Shared        |          No                |
tensor.detach.clone()()   # |        New         |          No                |
</code></pre><p>&#x62FC;&#x63A5;&#x5F20;&#x91CF;</p>
<p>&#x6CE8;&#x610F; torch.cat &#x548C; torch.stack &#x7684;&#x533A;&#x522B;&#x5728;&#x4E8E; torch.cat &#x6CBF;&#x7740;&#x7ED9;&#x5B9A;&#x7684;&#x7EF4;&#x5EA6;&#x62FC;&#x63A5;&#xFF0C;&#x800C; torch.stack &#x4F1A;&#x65B0;&#x589E;&#x4E00;&#x7EF4;&#x3002;&#x4F8B;&#x5982;&#x5F53;&#x53C2;&#x6570;&#x662F; 3 &#x4E2A; 10&#xD7;5 &#x7684;&#x5F20;&#x91CF;&#xFF0C;torch.cat &#x7684;&#x7ED3;&#x679C;&#x662F; 30&#xD7;5 &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x800C; torch.stack &#x7684;&#x7ED3;&#x679C;&#x662F; 3&#xD7;10&#xD7;5 &#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<pre><code>tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
</code></pre><p>&#x5C06;&#x6574;&#x6570;&#x6807;&#x8BB0;&#x8F6C;&#x6362;&#x6210;&#x72EC;&#x70ED;&#xFF08;one-hot&#xFF09;&#x7F16;&#x7801;</p>
<p>PyTorch &#x4E2D;&#x7684;&#x6807;&#x8BB0;&#x9ED8;&#x8BA4;&#x4ECE; 0 &#x5F00;&#x59CB;&#x3002;</p>
<pre><code>N = tensor.size(0)
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
</code></pre><p>&#x5F97;&#x5230;&#x975E;&#x96F6;/&#x96F6;&#x5143;&#x7D20;</p>
<pre><code>torch.nonzero(tensor)               # Index of non-zero elements
torch.nonzero(tensor == 0)          # Index of zero elements
torch.nonzero(tensor).size(0)       # Number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # Number of zero elements
</code></pre><p>&#x5F20;&#x91CF;&#x6269;&#x5C55;</p>
<pre><code># Expand tensor of shape 64*512 to shape 64*512*7*7.
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
</code></pre><p>&#x77E9;&#x9635;&#x4E58;&#x6CD5;</p>
<pre><code># Matrix multiplication: (m*n) * (n*p) -&gt; (m*p).
result = torch.mm(tensor1, tensor2)

# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p).
result = torch.bmm(tensor1, tensor2)

# Element-wise multiplication.
result = tensor1 * tensor2
</code></pre><p>&#x8BA1;&#x7B97;&#x4E24;&#x7EC4;&#x6570;&#x636E;&#x4E4B;&#x95F4;&#x7684;&#x4E24;&#x4E24;&#x6B27;&#x5F0F;&#x8DDD;&#x79BB;</p>
<pre><code># X1 is of shape m*d.
X1 = torch.unsqueeze(X1, dim=1).expand(m, n, d)
# X2 is of shape n*d.
X2 = torch.unsqueeze(X2, dim=0).expand(m, n, d)
# dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2)
dist = torch.sqrt(torch.sum((X1 - X2) ** 2, dim=2))
</code></pre><p>&#x6A21;&#x578B;&#x5B9A;&#x4E49;</p>
<p>&#x5377;&#x79EF;&#x5C42;</p>
<p>&#x6700;&#x5E38;&#x7528;&#x7684;&#x5377;&#x79EF;&#x5C42;&#x914D;&#x7F6E;&#x662F;</p>
<pre><code>conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)
</code></pre><p>&#x5982;&#x679C;&#x5377;&#x79EF;&#x5C42;&#x914D;&#x7F6E;&#x6BD4;&#x8F83;&#x590D;&#x6742;&#xFF0C;&#x4E0D;&#x65B9;&#x4FBF;&#x8BA1;&#x7B97;&#x8F93;&#x51FA;&#x5927;&#x5C0F;&#x65F6;&#xFF0C;&#x53EF;&#x4EE5;&#x5229;&#x7528;&#x5982;&#x4E0B;&#x53EF;&#x89C6;&#x5316;&#x5DE5;&#x5177;&#x8F85;&#x52A9;</p>
<p>&#x94FE;&#x63A5;&#xFF1A;<a href="https://ezyang.github.io/convolution-visualizer/index.html" target="_blank">https://ezyang.github.io/convolution-visualizer/index.html</a></p>
<p>0GAP&#xFF08;Global average pooling&#xFF09;&#x5C42;</p>
<pre><code>gap = torch.nn.AdaptiveAvgPool2d(output_size=1)
</code></pre><p>&#x53CC;&#x7EBF;&#x6027;&#x6C47;&#x5408;&#xFF08;bilinear pooling&#xFF09;</p>
<pre><code>X = torch.reshape(N, D, H * W)                        # Assume X has shape N*D*H*W
X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling
assert X.size() == (N, D, D)
X = torch.reshape(X, (N, D * D))
X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization
X = torch.nn.functional.normalize(X)                  # L2 normalization
</code></pre><p>&#x591A;&#x5361;&#x540C;&#x6B65; BN&#xFF08;Batch normalization&#xFF09;</p>
<p>&#x5F53;&#x4F7F;&#x7528; torch.nn.DataParallel &#x5C06;&#x4EE3;&#x7801;&#x8FD0;&#x884C;&#x5728;&#x591A;&#x5F20; GPU &#x5361;&#x4E0A;&#x65F6;&#xFF0C;PyTorch &#x7684; BN &#x5C42;&#x9ED8;&#x8BA4;&#x64CD;&#x4F5C;&#x662F;&#x5404;&#x5361;&#x4E0A;&#x6570;&#x636E;&#x72EC;&#x7ACB;&#x5730;&#x8BA1;&#x7B97;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#xFF0C;&#x540C;&#x6B65; BN &#x4F7F;&#x7528;&#x6240;&#x6709;&#x5361;&#x4E0A;&#x7684;&#x6570;&#x636E;&#x4E00;&#x8D77;&#x8BA1;&#x7B97; BN &#x5C42;&#x7684;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#xFF0C;&#x7F13;&#x89E3;&#x4E86;&#x5F53;&#x6279;&#x91CF;&#x5927;&#x5C0F;&#xFF08;batch size&#xFF09;&#x6BD4;&#x8F83;&#x5C0F;&#x65F6;&#x5BF9;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x4F30;&#x8BA1;&#x4E0D;&#x51C6;&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x662F;&#x5728;&#x76EE;&#x6807;&#x68C0;&#x6D4B;&#x7B49;&#x4EFB;&#x52A1;&#x4E2D;&#x4E00;&#x4E2A;&#x6709;&#x6548;&#x7684;&#x63D0;&#x5347;&#x6027;&#x80FD;&#x7684;&#x6280;&#x5DE7;&#x3002;</p>
<p>&#x94FE;&#x63A5;&#xFF1A;<a href="https://github.com/vacancy/Synchronized-BatchNorm-PyTorch" target="_blank">https://github.com/vacancy/Synchronized-BatchNorm-PyTorch</a></p>
<p>&#x7C7B;&#x4F3C; BN &#x6ED1;&#x52A8;&#x5E73;&#x5747;</p>
<p>&#x5982;&#x679C;&#x8981;&#x5B9E;&#x73B0;&#x7C7B;&#x4F3C; BN &#x6ED1;&#x52A8;&#x5E73;&#x5747;&#x7684;&#x64CD;&#x4F5C;&#xFF0C;&#x5728; forward &#x51FD;&#x6570;&#x4E2D;&#x8981;&#x4F7F;&#x7528;&#x539F;&#x5730;&#xFF08;inplace&#xFF09;&#x64CD;&#x4F5C;&#x7ED9;&#x6ED1;&#x52A8;&#x5E73;&#x5747;&#x8D4B;&#x503C;&#x3002;</p>
<pre><code>class BN(torch.nn.Module)
    def __init__(self):
        ...
        self.register_buffer(&apos;running_mean&apos;, torch.zeros(num_features))

    def forward(self, X):
        ...
        self.running_mean += momentum * (current - self.running_mean)
</code></pre><p>&#x8BA1;&#x7B97;&#x6A21;&#x578B;&#x6574;&#x4F53;&#x53C2;&#x6570;&#x91CF;</p>
<pre><code>num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
</code></pre><p>&#x7C7B;&#x4F3C; Keras &#x7684; model.summary() &#x8F93;&#x51FA;&#x6A21;&#x578B;&#x4FE1;&#x606F;</p>
<p>&#x94FE;&#x63A5;&#xFF1A;<a href="https://github.com/sksq96/pytorch-summary" target="_blank">https://github.com/sksq96/pytorch-summary</a></p>
<p>&#x6A21;&#x578B;&#x6743;&#x503C;&#x521D;&#x59CB;&#x5316;</p>
<p>&#x6CE8;&#x610F; model.modules() &#x548C; model.children() &#x7684;&#x533A;&#x522B;&#xFF1A;model.modules() &#x4F1A;&#x8FED;&#x4EE3;&#x5730;&#x904D;&#x5386;&#x6A21;&#x578B;&#x7684;&#x6240;&#x6709;&#x5B50;&#x5C42;&#xFF0C;&#x800C; model.children() &#x53EA;&#x4F1A;&#x904D;&#x5386;&#x6A21;&#x578B;&#x4E0B;&#x7684;&#x4E00;&#x5C42;&#x3002;</p>
<pre><code># Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode=&apos;fan_out&apos;,
                                      nonlinearity=&apos;relu&apos;)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
</code></pre><p>&#x90E8;&#x5206;&#x5C42;&#x4F7F;&#x7528;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;</p>
<p>&#x6CE8;&#x610F;&#x5982;&#x679C;&#x4FDD;&#x5B58;&#x7684;&#x6A21;&#x578B;&#x662F; torch.nn.DataParallel&#xFF0C;&#x5219;&#x5F53;&#x524D;&#x7684;&#x6A21;&#x578B;&#x4E5F;&#x9700;&#x8981;&#x662F;</p>
<pre><code>model.load_state_dict(torch.load(&apos;model,pth&apos;), strict=False)
</code></pre><p>&#x5C06;&#x5728; GPU &#x4FDD;&#x5B58;&#x7684;&#x6A21;&#x578B;&#x52A0;&#x8F7D;&#x5230; CPU</p>
<pre><code>model.load_state_dict(torch.load(&apos;model,pth&apos;, map_location=&apos;cpu&apos;))
</code></pre><p>&#x6570;&#x636E;&#x51C6;&#x5907;&#x3001;&#x7279;&#x5F81;&#x63D0;&#x53D6;&#x4E0E;&#x5FAE;&#x8C03;</p>
<p>&#x5F97;&#x5230;&#x89C6;&#x9891;&#x6570;&#x636E;&#x57FA;&#x672C;&#x4FE1;&#x606F;</p>
<pre><code>import cv2
video = cv2.VideoCapture(mp4_path)
height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
fps = int(video.get(cv2.CAP_PROP_FPS))
video.release()
</code></pre><p>TSN &#x6BCF;&#x6BB5;&#xFF08;segment&#xFF09;&#x91C7;&#x6837;&#x4E00;&#x5E27;&#x89C6;&#x9891;</p>
<pre><code>K = self._num_segments
if is_train:
    if num_frames &gt; K:
        # Random index for each segment.
        frame_indices = torch.randint(
            high=num_frames // K, size=(K,), dtype=torch.long)
        frame_indices += num_frames // K * torch.arange(K)
    else:
        frame_indices = torch.randint(
            high=num_frames, size=(K - num_frames,), dtype=torch.long)
        frame_indices = torch.sort(torch.cat((
            torch.arange(num_frames), frame_indices)))[0]
else:
    if num_frames &gt; K:
        # Middle index for each segment.
        frame_indices = num_frames / K // 2
        frame_indices += num_frames // K * torch.arange(K)
    else:
        frame_indices = torch.sort(torch.cat((                              
            torch.arange(num_frames), torch.arange(K - num_frames))))[0]
assert frame_indices.size() == (K,)
return [frame_indices[i] for i in range(K)]
</code></pre><p>&#x63D0;&#x53D6; ImageNet &#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x67D0;&#x5C42;&#x7684;&#x5377;&#x79EF;&#x7279;&#x5F81;</p>
<pre><code># VGG-16 relu5-3 feature.
model = torchvision.models.vgg16(pretrained=True).features[:-1]
# VGG-16 pool5 feature.
model = torchvision.models.vgg16(pretrained=True).features
# VGG-16 fc7 feature.
model = torchvision.models.vgg16(pretrained=True)
model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])
# ResNet GAP feature.
model = torchvision.models.resnet18(pretrained=True)
model = torch.nn.Sequential(collections.OrderedDict(
    list(model.named_children())[:-1]))

with torch.no_grad():
    model.eval()
    conv_representation = model(image)
</code></pre><p>&#x63D0;&#x53D6; ImageNet &#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x591A;&#x5C42;&#x7684;&#x5377;&#x79EF;&#x7279;&#x5F81;</p>
<pre><code>class FeatureExtractor(torch.nn.Module):
    &quot;&quot;&quot;Helper class to extract several convolution features from the given
    pre-trained model.

    Attributes:
        _model, torch.nn.Module.
        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;

    Example:
        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)
        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(
                list(model.named_children())[:-1]))
        &gt;&gt;&gt; conv_representation = FeatureExtractor(
                pretrained_model=model,
                layers_to_extract={&apos;layer1&apos;, &apos;layer2&apos;, &apos;layer3&apos;, &apos;layer4&apos;})(image)
    &quot;&quot;&quot;
    def __init__(self, pretrained_model, layers_to_extract):
        torch.nn.Module.__init__(self)
        self._model = pretrained_model
        self._model.eval()
        self._layers_to_extract = set(layers_to_extract)

    def forward(self, x):
        with torch.no_grad():
            conv_representation = []
            for name, layer in self._model.named_children():
                x = layer(x)
                if name in self._layers_to_extract:
                    conv_representation.append(x)
            return conv_representation
</code></pre><p>&#x5176;&#x4ED6;&#x9884;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;</p>
<p>&#x94FE;&#x63A5;&#xFF1A;<a href="https://github.com/Cadene/pretrained-models.pytorch" target="_blank">https://github.com/Cadene/pretrained-models.pytorch</a></p>
<p>&#x5FAE;&#x8C03;&#x5168;&#x8FDE;&#x63A5;&#x5C42;</p>
<pre><code>model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
model.fc = nn.Linear(512, 100)  # Replace the last fc layer
optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre><p>&#x4EE5;&#x8F83;&#x5927;&#x5B66;&#x4E60;&#x7387;&#x5FAE;&#x8C03;&#x5168;&#x8FDE;&#x63A5;&#x5C42;&#xFF0C;&#x8F83;&#x5C0F;&#x5B66;&#x4E60;&#x7387;&#x5FAE;&#x8C03;&#x5377;&#x79EF;&#x5C42;</p>
<pre><code>model = torchvision.models.resnet18(pretrained=True)
finetuned_parameters = list(map(id, model.fc.parameters()))
conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)
parameters = [{&apos;params&apos;: conv_parameters, &apos;lr&apos;: 1e-3}, 
              {&apos;params&apos;: model.fc.parameters()}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre><p>&#x6A21;&#x578B;&#x8BAD;&#x7EC3;</p>
<p>&#x5E38;&#x7528;&#x8BAD;&#x7EC3;&#x548C;&#x9A8C;&#x8BC1;&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;</p>
<p>&#x5176;&#x4E2D; ToTensor &#x64CD;&#x4F5C;&#x4F1A;&#x5C06; PIL.Image &#x6216;&#x5F62;&#x72B6;&#x4E3A; H&#xD7;W&#xD7;D&#xFF0C;&#x6570;&#x503C;&#x8303;&#x56F4;&#x4E3A; [0, 255] &#x7684; np.ndarray &#x8F6C;&#x6362;&#x4E3A;&#x5F62;&#x72B6;&#x4E3A; D&#xD7;H&#xD7;W&#xFF0C;&#x6570;&#x503C;&#x8303;&#x56F4;&#x4E3A; [0.0, 1.0] &#x7684; torch.Tensor&#x3002;</p>
<pre><code>train_transform = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(size=224,
                                             scale=(0.08, 1.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                     std=(0.229, 0.224, 0.225)),
 ])
 val_transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize(224),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                     std=(0.229, 0.224, 0.225)),
])
</code></pre><p>&#x8BAD;&#x7EC3;&#x57FA;&#x672C;&#x4EE3;&#x7801;&#x6846;&#x67B6;</p>
<pre><code>for t in epoch(80):
    for images, labels in tqdm.tqdm(train_loader, desc=&apos;Epoch %3d&apos; % (t + 1)):
        images, labels = images.cuda(), labels.cuda()
        scores = model(images)
        loss = loss_function(scores, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre><p>&#x6807;&#x8BB0;&#x5E73;&#x6ED1;&#xFF08;label smoothing&#xFF09;</p>
<pre><code>for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre><p>Mixup</p>
<pre><code>beta_distribution = torch.distributions.beta.Beta(alpha, alpha)
for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()

    # Mixup images.
    lambda_ = beta_distribution.sample([]).item()
    index = torch.randperm(images.size(0)).cuda()
    mixed_images = lambda_ * images + (1 - lambda_) * images[index, :]

    # Mixup loss.    
    scores = model(mixed_images)
    loss = (lambda_ * loss_function(scores, labels) 
            + (1 - lambda_) * loss_function(scores, labels[index]))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre><p>L1 &#x6B63;&#x5219;&#x5316;</p>
<pre><code>l1_regularization = torch.nn.L1Loss(reduction=&apos;sum&apos;)
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
</code></pre><p>&#x4E0D;&#x5BF9;&#x504F;&#x7F6E;&#x9879;&#x8FDB;&#x884C; L2 &#x6B63;&#x5219;&#x5316;/&#x6743;&#x503C;&#x8870;&#x51CF;&#xFF08;weight decay&#xFF09;</p>
<pre><code>bias_list = (param for name, param in model.named_parameters() if name[-4:] == &apos;bias&apos;)
others_list = (param for name, param in model.named_parameters() if name[-4:] != &apos;bias&apos;)
parameters = [{&apos;parameters&apos;: bias_list, &apos;weight_decay&apos;: 0},                
              {&apos;parameters&apos;: others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre><p>&#x68AF;&#x5EA6;&#x88C1;&#x526A;&#xFF08;gradient clipping&#xFF09;</p>
<pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
</code></pre><p>&#x8BA1;&#x7B97; Softmax &#x8F93;&#x51FA;&#x7684;&#x51C6;&#x786E;&#x7387;</p>
<pre><code>score = model(images)
prediction = torch.argmax(score, dim=1)
num_correct = torch.sum(prediction == labels).item()
accuruacy = num_correct / labels.size(0)
</code></pre><p>&#x53EF;&#x89C6;&#x5316;&#x6A21;&#x578B;&#x524D;&#x9988;&#x7684;&#x8BA1;&#x7B97;&#x56FE;</p>
<p>&#x94FE;&#x63A5;&#xFF1A;<a href="https://github.com/szagoruyko/pytorchviz" target="_blank">https://github.com/szagoruyko/pytorchviz</a></p>
<p>&#x53EF;&#x89C6;&#x5316;&#x5B66;&#x4E60;&#x66F2;&#x7EBF;</p>
<p>&#x6709; Facebook &#x81EA;&#x5DF1;&#x5F00;&#x53D1;&#x7684; Visdom &#x548C; Tensorboard &#x4E24;&#x4E2A;&#x9009;&#x62E9;&#x3002;</p>
<p><a href="https://github.com/facebookresearch/visdom" target="_blank">https://github.com/facebookresearch/visdom</a></p>
<p><a href="https://github.com/lanpa/tensorboardX" target="_blank">https://github.com/lanpa/tensorboardX</a></p>
<pre><code># Example using Visdom.
vis = visdom.Visdom(env=&apos;Learning curve&apos;, use_incoming_socket=False)
assert self._visdom.check_connection()
self._visdom.close()
options = collections.namedtuple(&apos;Options&apos;, [&apos;loss&apos;, &apos;acc&apos;, &apos;lr&apos;])(
    loss={&apos;xlabel&apos;: &apos;Epoch&apos;, &apos;ylabel&apos;: &apos;Loss&apos;, &apos;showlegend&apos;: True},
    acc={&apos;xlabel&apos;: &apos;Epoch&apos;, &apos;ylabel&apos;: &apos;Accuracy&apos;, &apos;showlegend&apos;: True},
    lr={&apos;xlabel&apos;: &apos;Epoch&apos;, &apos;ylabel&apos;: &apos;Learning rate&apos;, &apos;showlegend&apos;: True})

for t in epoch(80):
    tran(...)
    val(...)
    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]),
             name=&apos;train&apos;, win=&apos;Loss&apos;, update=&apos;append&apos;, opts=options.loss)
    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]),
             name=&apos;val&apos;, win=&apos;Loss&apos;, update=&apos;append&apos;, opts=options.loss)
    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]),
             name=&apos;train&apos;, win=&apos;Accuracy&apos;, update=&apos;append&apos;, opts=options.acc)
    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]),
             name=&apos;val&apos;, win=&apos;Accuracy&apos;, update=&apos;append&apos;, opts=options.acc)
    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]),
             win=&apos;Learning rate&apos;, update=&apos;append&apos;, opts=options.lr)
</code></pre><p>&#x5F97;&#x5230;&#x5F53;&#x524D;&#x5B66;&#x4E60;&#x7387;</p>
<pre><code># If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))[&apos;lr&apos;]

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group[&apos;lr&apos;])
</code></pre><p>&#x5B66;&#x4E60;&#x7387;&#x8870;&#x51CF;</p>
<pre><code># Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&apos;max&apos;, patience=5, verbose=True)
for t in range(0, 80):
    train(...); val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...); val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...); val(...)
</code></pre><p>&#x4FDD;&#x5B58;&#x4E0E;&#x52A0;&#x8F7D;&#x65AD;&#x70B9;</p>
<p>&#x6CE8;&#x610F;&#x4E3A;&#x4E86;&#x80FD;&#x591F;&#x6062;&#x590D;&#x8BAD;&#x7EC3;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x540C;&#x65F6;&#x4FDD;&#x5B58;&#x6A21;&#x578B;&#x548C;&#x4F18;&#x5316;&#x5668;&#x7684;&#x72B6;&#x6001;&#xFF0C;&#x4EE5;&#x53CA;&#x5F53;&#x524D;&#x7684;&#x8BAD;&#x7EC3;&#x8F6E;&#x6570;&#x3002;</p>
<pre><code># Save checkpoint.
is_best = current_acc &gt; best_acc
best_acc = max(best_acc, current_acc)
checkpoint = {
    &apos;best_acc&apos;: best_acc,    
    &apos;epoch&apos;: t + 1,
    &apos;model&apos;: model.state_dict(),
    &apos;optimizer&apos;: optimizer.state_dict(),
}
model_path = os.path.join(&apos;model&apos;, &apos;checkpoint.pth.tar&apos;)
torch.save(checkpoint, model_path)
if is_best:
    shutil.copy(&apos;checkpoint.pth.tar&apos;, model_path)

# Load checkpoint.
if resume:
    model_path = os.path.join(&apos;model&apos;, &apos;checkpoint.pth.tar&apos;)
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint[&apos;best_acc&apos;]
    start_epoch = checkpoint[&apos;epoch&apos;]
    model.load_state_dict(checkpoint[&apos;model&apos;])
    optimizer.load_state_dict(checkpoint[&apos;optimizer&apos;])
    print(&apos;Load checkpoint at epoch %d.&apos; % start_epoch)
</code></pre><p>&#x8BA1;&#x7B97;&#x51C6;&#x786E;&#x7387;&#x3001;&#x67E5;&#x51C6;&#x7387;&#xFF08;precision&#xFF09;&#x3001;&#x67E5;&#x5168;&#x7387;&#xFF08;recall&#xFF09;</p>
<pre><code># data[&apos;label&apos;] and data[&apos;prediction&apos;] are groundtruth label and prediction 
# for each image, respectively.
accuracy = np.mean(data[&apos;label&apos;] == data[&apos;prediction&apos;]) * 100

# Compute recision and recall for each class.
for c in range(len(num_classes)):
    tp = np.dot((data[&apos;label&apos;] == c).astype(int),
                (data[&apos;prediction&apos;] == c).astype(int))
    tp_fp = np.sum(data[&apos;prediction&apos;] == c)
    tp_fn = np.sum(data[&apos;label&apos;] == c)
    precision = tp / tp_fp * 100
    recall = tp / tp_fn * 100
</code></pre><p>PyTorch &#x5176;&#x4ED6;&#x6CE8;&#x610F;&#x4E8B;&#x9879;</p>
<p>&#x6A21;&#x578B;&#x5B9A;&#x4E49;</p>
<ul>
<li>&#x5EFA;&#x8BAE;&#x6709;&#x53C2;&#x6570;&#x7684;&#x5C42;&#x548C;&#x6C47;&#x5408;&#xFF08;pooling&#xFF09;&#x5C42;&#x4F7F;&#x7528; torch.nn &#x6A21;&#x5757;&#x5B9A;&#x4E49;&#xFF0C;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x76F4;&#x63A5;&#x4F7F;&#x7528; torch.nn.functional&#x3002;torch.nn &#x6A21;&#x5757;&#x548C; torch.nn.functional &#x7684;&#x533A;&#x522B;&#x5728;&#x4E8E;&#xFF0C;torch.nn &#x6A21;&#x5757;&#x5728;&#x8BA1;&#x7B97;&#x65F6;&#x5E95;&#x5C42;&#x8C03;&#x7528;&#x4E86; torch.nn.functional&#xFF0C;&#x4F46; torch.nn &#x6A21;&#x5757;&#x5305;&#x62EC;&#x8BE5;&#x5C42;&#x53C2;&#x6570;&#xFF0C;&#x8FD8;&#x53EF;&#x4EE5;&#x5E94;&#x5BF9;&#x8BAD;&#x7EC3;&#x548C;&#x6D4B;&#x8BD5;&#x4E24;&#x79CD;&#x7F51;&#x7EDC;&#x72B6;&#x6001;&#x3002;&#x4F7F;&#x7528; torch.nn.functional &#x65F6;&#x8981;&#x6CE8;&#x610F;&#x7F51;&#x7EDC;&#x72B6;&#x6001;&#xFF0C;&#x5982;</li>
</ul>
<pre><code>def forward(self, x):
    ...
    x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
</code></pre><ul>
<li>model(x) &#x524D;&#x7528; model.train() &#x548C; model.eval() &#x5207;&#x6362;&#x7F51;&#x7EDC;&#x72B6;&#x6001;&#x3002;</li>
<li>&#x4E0D;&#x9700;&#x8981;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x7684;&#x4EE3;&#x7801;&#x5757;&#x7528; with torch.no_grad() &#x5305;&#x542B;&#x8D77;&#x6765;&#x3002;model.eval() &#x548C; torch.no_grad() &#x7684;&#x533A;&#x522B;&#x5728;&#x4E8E;&#xFF0C;model.eval() &#x662F;&#x5C06;&#x7F51;&#x7EDC;&#x5207;&#x6362;&#x4E3A;&#x6D4B;&#x8BD5;&#x72B6;&#x6001;&#xFF0C;&#x4F8B;&#x5982; BN &#x548C;&#x968F;&#x673A;&#x5931;&#x6D3B;&#xFF08;dropout&#xFF09;&#x5728;&#x8BAD;&#x7EC3;&#x548C;&#x6D4B;&#x8BD5;&#x9636;&#x6BB5;&#x4F7F;&#x7528;&#x4E0D;&#x540C;&#x7684;&#x8BA1;&#x7B97;&#x65B9;&#x6CD5;&#x3002;torch.no_grad() &#x662F;&#x5173;&#x95ED; PyTorch &#x5F20;&#x91CF;&#x7684;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x673A;&#x5236;&#xFF0C;&#x4EE5;&#x51CF;&#x5C11;&#x5B58;&#x50A8;&#x4F7F;&#x7528;&#x548C;&#x52A0;&#x901F;&#x8BA1;&#x7B97;&#xFF0C;&#x5F97;&#x5230;&#x7684;&#x7ED3;&#x679C;&#x65E0;&#x6CD5;&#x8FDB;&#x884C; loss.backward()&#x3002;</li>
<li>torch.nn.CrossEntropyLoss &#x7684;&#x8F93;&#x5165;&#x4E0D;&#x9700;&#x8981;&#x7ECF;&#x8FC7; Softmax&#x3002;torch.nn.CrossEntropyLoss &#x7B49;&#x4EF7;&#x4E8E; torch.nn.functional.log_softmax + torch.nn.NLLLoss&#x3002;</li>
<li>loss.backward() &#x524D;&#x7528; optimizer.zero_grad() &#x6E05;&#x9664;&#x7D2F;&#x79EF;&#x68AF;&#x5EA6;&#x3002;optimizer.zero_grad() &#x548C; model.zero_grad() &#x6548;&#x679C;&#x4E00;&#x6837;&#x3002;</li>
</ul>
<p>PyTorch &#x6027;&#x80FD;&#x4E0E;&#x8C03;&#x8BD5;</p>
<ul>
<li>torch.utils.data.DataLoader &#x4E2D;&#x5C3D;&#x91CF;&#x8BBE;&#x7F6E; pin_memory=True&#xFF0C;&#x5BF9;&#x7279;&#x522B;&#x5C0F;&#x7684;&#x6570;&#x636E;&#x96C6;&#x5982; MNIST &#x8BBE;&#x7F6E; pin_memory=False &#x53CD;&#x800C;&#x66F4;&#x5FEB;&#x4E00;&#x4E9B;&#x3002;num_workers &#x7684;&#x8BBE;&#x7F6E;&#x9700;&#x8981;&#x5728;&#x5B9E;&#x9A8C;&#x4E2D;&#x627E;&#x5230;&#x6700;&#x5FEB;&#x7684;&#x53D6;&#x503C;&#x3002;</li>
<li>&#x7528; del &#x53CA;&#x65F6;&#x5220;&#x9664;&#x4E0D;&#x7528;&#x7684;&#x4E2D;&#x95F4;&#x53D8;&#x91CF;&#xFF0C;&#x8282;&#x7EA6; GPU &#x5B58;&#x50A8;&#x3002;</li>
<li>&#x4F7F;&#x7528; inplace &#x64CD;&#x4F5C;&#x53EF;&#x8282;&#x7EA6; GPU &#x5B58;&#x50A8;&#xFF0C;&#x5982;</li>
</ul>
<pre><code>x = torch.nn.functional.relu(x, inplace=True)
</code></pre><ul>
<li>&#x51CF;&#x5C11; CPU &#x548C; GPU &#x4E4B;&#x95F4;&#x7684;&#x6570;&#x636E;&#x4F20;&#x8F93;&#x3002;&#x4F8B;&#x5982;&#x5982;&#x679C;&#x4F60;&#x60F3;&#x77E5;&#x9053;&#x4E00;&#x4E2A; epoch &#x4E2D;&#x6BCF;&#x4E2A; mini-batch &#x7684; loss &#x548C;&#x51C6;&#x786E;&#x7387;&#xFF0C;&#x5148;&#x5C06;&#x5B83;&#x4EEC;&#x7D2F;&#x79EF;&#x5728; GPU &#x4E2D;&#x7B49;&#x4E00;&#x4E2A; epoch &#x7ED3;&#x675F;&#x4E4B;&#x540E;&#x4E00;&#x8D77;&#x4F20;&#x8F93;&#x56DE; CPU &#x4F1A;&#x6BD4;&#x6BCF;&#x4E2A; mini-batch &#x90FD;&#x8FDB;&#x884C;&#x4E00;&#x6B21; GPU &#x5230; CPU &#x7684;&#x4F20;&#x8F93;&#x66F4;&#x5FEB;&#x3002;</li>
<li>&#x4F7F;&#x7528;&#x534A;&#x7CBE;&#x5EA6;&#x6D6E;&#x70B9;&#x6570; half() &#x4F1A;&#x6709;&#x4E00;&#x5B9A;&#x7684;&#x901F;&#x5EA6;&#x63D0;&#x5347;&#xFF0C;&#x5177;&#x4F53;&#x6548;&#x7387;&#x4F9D;&#x8D56;&#x4E8E; GPU &#x578B;&#x53F7;&#x3002;&#x9700;&#x8981;&#x5C0F;&#x5FC3;&#x6570;&#x503C;&#x7CBE;&#x5EA6;&#x8FC7;&#x4F4E;&#x5E26;&#x6765;&#x7684;&#x7A33;&#x5B9A;&#x6027;&#x95EE;&#x9898;&#x3002;</li>
<li>&#x65F6;&#x5E38;&#x4F7F;&#x7528; assert tensor.size() == (N, D, H, W) &#x4F5C;&#x4E3A;&#x8C03;&#x8BD5;&#x624B;&#x6BB5;&#xFF0C;&#x786E;&#x4FDD;&#x5F20;&#x91CF;&#x7EF4;&#x5EA6;&#x548C;&#x4F60;&#x8BBE;&#x60F3;&#x4E2D;&#x4E00;&#x81F4;&#x3002;</li>
<li>&#x9664;&#x4E86;&#x6807;&#x8BB0; y &#x5916;&#xFF0C;&#x5C3D;&#x91CF;&#x5C11;&#x4F7F;&#x7528;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x4F7F;&#x7528; n*1 &#x7684;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#x4EE3;&#x66FF;&#xFF0C;&#x53EF;&#x4EE5;&#x907F;&#x514D;&#x4E00;&#x4E9B;&#x610F;&#x60F3;&#x4E0D;&#x5230;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x8BA1;&#x7B97;&#x7ED3;&#x679C;&#x3002;</li>
<li>&#x7EDF;&#x8BA1;&#x4EE3;&#x7801;&#x5404;&#x90E8;&#x5206;&#x8017;&#x65F6;</li>
</ul>
<pre><code>with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)
</code></pre><p>&#x6216;&#x8005;&#x5728;&#x547D;&#x4EE4;&#x884C;&#x8FD0;&#x884C;</p>
<pre><code>python -m torch.utils.bottleneck main.py
</code></pre><p>&#x81F4;&#x8C22;</p>
<p>&#x611F;&#x8C22; @&#x4E9B;&#x8BB8;&#x6D41;&#x5E74;&#x548C;@El tnoto&#x7684;&#x52D8;&#x8BEF;&#x3002;&#x7531;&#x4E8E;&#x4F5C;&#x8005;&#x624D;&#x758F;&#x5B66;&#x6D45;&#xFF0C;&#x66F4;&#x517C;&#x65F6;&#x95F4;&#x548C;&#x7CBE;&#x529B;&#x6240;&#x9650;&#xFF0C;&#x4EE3;&#x7801;&#x4E2D;&#x9519;&#x8BEF;&#x4E4B;&#x5904;&#x5728;&#x6240;&#x96BE;&#x514D;&#xFF0C;&#x656C;&#x8BF7;&#x8BFB;&#x8005;&#x6279;&#x8BC4;&#x6307;&#x6B63;&#x3002;</p>
<p>&#x53C2;&#x8003;&#x8D44;&#x6599;</p>
<ul>
<li>PyTorch &#x5B98;&#x65B9;&#x4EE3;&#x7801;&#xFF1A;pytorch/examples (<a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples)" target="_blank">https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples)</a></li>
<li>PyTorch &#x8BBA;&#x575B;&#xFF1A;PyTorch Forums (<a href="https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/latest%3Forder%3Dviews)" target="_blank">https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/latest%3Forder%3Dviews)</a></li>
<li>PyTorch &#x6587;&#x6863;&#xFF1A;<a href="http://pytorch.org/docs/stable/index.html" target="_blank">http://pytorch.org/docs/stable/index.html</a> (<a href="https://link.zhihu.com/?target=http%3A//pytorch.org/docs/stable/index.html)" target="_blank">https://link.zhihu.com/?target=http%3A//pytorch.org/docs/stable/index.html)</a></li>
<li>&#x5176;&#x4ED6;&#x57FA;&#x4E8E; PyTorch &#x7684;&#x516C;&#x5F00;&#x5B9E;&#x73B0;&#x4EE3;&#x7801;&#xFF0C;&#x65E0;&#x6CD5;&#x4E00;&#x4E00;&#x5217;&#x4E3E;</li>
</ul>
<p>&#x5F20;&#x7693;&#xFF1A;&#x5357;&#x4EAC;&#x5927;&#x5B66;&#x8BA1;&#x7B97;&#x673A;&#x7CFB;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E0E;&#x6570;&#x636E;&#x6316;&#x6398;&#x6240;&#xFF08;LAMDA&#xFF09;&#x7855;&#x58EB;&#x751F;&#xFF0C;&#x7814;&#x7A76;&#x65B9;&#x5411;&#x4E3A;&#x8BA1;&#x7B97;&#x673A;&#x89C6;&#x89C9;&#x548C;&#x673A;&#x5668;&#x5B66;&#x4E60;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x89C6;&#x89C9;&#x8BC6;&#x522B;&#x548C;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x3002;&#x4E2A;&#x4EBA;&#x4E3B;&#x9875;&#xFF1A;<a href="http://lamda.nju.edu.cn/zhangh/" target="_blank">http://lamda.nju.edu.cn/zhangh/</a></p>
<p><em>&#x539F;&#x77E5;&#x4E4E;&#x94FE;&#x63A5;&#xFF1A;</em> <a href="https://zhuanlan.zhihu.com/p/59205847?" target="_blank">https://zhuanlan.zhihu.com/p/59205847?</a></p>
<p>&#x67E5;&#x770B;&#x539F;&#x6587;&#xFF1A; <a href="https://www.jiqizhixin.com/articles/2019-04-25-8" target="_blank">&#x70B9;&#x8D5E;&#x6536;&#x85CF;&#xFF1A;PyTorch&#x5E38;&#x7528;&#x4EE3;&#x7801;&#x6BB5;&#x6574;&#x7406;&#x5408;&#x96C6;</a></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="pytorch1.html" class="navigation navigation-prev " aria-label="Previous page: pytorch常用代码">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="pytorch_train.html" class="navigation navigation-next " aria-label="Next page: pytorch训练技巧">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"pytorch常用代码段合集","level":"1.4.5","depth":2,"next":{"title":"pytorch训练技巧","level":"1.4.6","depth":2,"path":"code_technique/pytorch/pytorch_train.md","ref":"code_technique/pytorch/pytorch_train.md","articles":[]},"previous":{"title":"pytorch常用代码","level":"1.4.4","depth":2,"path":"code_technique/pytorch/pytorch1.md","ref":"code_technique/pytorch/pytorch1.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-search","search-pro","back-to-top-button","expandable-chapters-small","back-to-top-button","chapter-fold","expandable-chapters-small","github","katex","include-codeblock","livereload"],"root":"./content","styles":{"website":"assets/styles/website.less","ebook":"assets/styles/ebook.less","pdf":"assets/styles/pdf.less","mobi":"assets/styles/mobi.less","epub":"assets/styles/epub.less"},"pluginsConfig":{"chapter-fold":{},"prism":{"css":["prismjs/themes/prism-solarizedlight.css"],"lang":{"flow":"typescript"}},"github":{"url":"https://github.com/OUCMachineLearning/OUCML"},"livereload":{},"search-pro":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"back-to-top-button":{},"expandable-chapters-small":{},"include-codeblock":{"check":false,"edit":false,"fixlang":false,"lang":"","template":"default","theme":"chrome","unindent":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"language":"zh-hans","gitbook":"*"},"file":{"path":"code_technique/pytorch/pytorch2.md","mtime":"2019-10-24T04:35:18.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-10-25T10:54:29.358Z"},"basePath":"../..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../../gitbook/gitbook.js"></script>
    <script src="../../gitbook/theme.js"></script>
    
        
        <script src="../../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

