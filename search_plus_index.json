{"./":{"url":"./","title":"Introduction","keywords":"","body":"代码技巧汇总 "},"introduction/0.html":{"url":"introduction/0.html","title":"送给研一入学的你们—炼丹师入门手册","keywords":"","body":"送给即将入学的你们—炼丹师修炼手册 标签（空格分隔）： 陈扬 [TOC] 前段时间我一直在外面比赛，好久没有认认真真的坐在电脑前写一写东西了。这一年以来，随着我个人对深度神经网络的学习逐渐深入，也开始看到有所收获。正好这段时间我们实验室新来了8位研一的小伙伴，我也借此分享一些我个人的学习经验和心得给大家吧。 关于深度学习我一点个人见解 我想，你们在选择了进入组里之前想必是应该或多或少是听过现在仍然很火的人工智能，也许你们一时半会对于深度神经网络这样的名词还觉得很高大上，也许你还在思考着 SVM 和 MLP 到底是什么，或许是一知半解的在网上搜着 NLP、CV 等等等。 其实在我看来，深度神经网络并不是什么很复杂的东西，他在本质上和一个二元函数并没有什么太大的区别，其很多的算法也来自于数值计算，比如随机梯度下降法SGD等等。而事实上，你们将来要做的工作也许会比解一个二元函数复杂很多，但是其本质仍然没有改变，只不过也许在你们未来研究的工作中可能遇到的第一个问题就是—我不知道我的目标到底是什么？ 再说一句题外话 我觉得在长篇大论说起 DeepLearning 之前，由于我也不是特别清楚你们大学里学的到底咋样，我再啰嗦一些基础技能，大神这一段就跳过吧…… 代码 python语法 对于一名合格的炼丹师来说，拥有良好的代码能力是炼丹成功的第一步。在大学期间，也许你们并没有接触过 python，大部分学校应该是学的 c 艹或者 java，而在炼丹界，我们最常使用的这是 python。 如何学习 python？ 基本语法：http://funhacks.net/explore-python/File-Directory/text_file_io.html python 的基本语法相对来说很简单，我这里推荐看FunHacks 大神写的基础入门教程，我个人觉得相比菜鸟和廖雪峰老师的来说更为实用简单。 学会了基础的语法后，我推荐你们去 letcode 或者牛客上刷几十题，实战收悉代码语法。 Letcode：https://leetcode-cn.com/problemset/all/ 牛客：https://www.nowcoder.com/ 安装 python python 之所以深受炼丹师们的热爱，除了其简洁易懂的语法外，更离不开其强大的扩展性和多如繁星的第三方开源库，但是作为一名初来乍到的新人，这也许对你们来说第一道坎来了—如何配置好 python 环境？ 这里我推荐Anaconda：https://mirror.tuna.tsinghua.edu.cn/help/anaconda/ 清华大学的源非常快，基本上是一键式配置科学技术框架，注意要按官方要求换tuna的源。 IDE 现在有了基础的语法，有了基础的 python 包，想必是按捺不住想要动手写一下代码了吧？ 市面上也有在非常的的 IDE，我这里分享一些我个人用过觉得比较好的 IDE。 MAC 系统：CodeRunner： CodeRunner：https://coderunnerapp.com/ 即开即用，应该是不用写教程就能看得懂吧…… windows&Linux 下：sublime sublime：https://www.sublimetext.com/ 需要安装插件包：https://www.4spaces.org/how-to-install-package-control-on-sublime-text/ 好电脑：pycharm pycharm：https://www.jetbrains.com/pycharm/ 应该是最好用的 python IDE了吧，不过也有很大的缺点，那就是对电脑配置要求较高，带的动的电脑基本上就是无脑推荐 pycharm 了。 一定要下载PRO 版，因为你将来远程用服务器跑程序，社区版没有远程功能。 最舒服的 IDE--jupyter jupyter：https://jupyter.org/ 但凡是写 python的，jupyter几乎可以说是从入门到精通都一直伴随着你的 IDE 了，其功能扩展性极高，可读性极强，网上非常多的代码教程都是直接用 jupyter 写的，没有 jupyter，你的世界可以说少了一半光明。 写作 出入科研领域的你们，也许要和你们熟悉的 office 三节课说拜拜了。写作在科研在是非常重要的一环，一篇漂亮的论文除了要有好的 idea，同样离不开精美的排版，最重要的是会议和期刊都有着严格的格式要求。 LATEX将接替你们熟悉的 word，承担起写作工具的大任。 学习网站：https://www.latexstudio.net/ 当然了，除了论文的写作，日常学习生活中知识的积累同样非常重要，一般我们在做论文笔记的时候，首选推荐的是Markdown； Markdown 的语法及其简单，简单到你看一遍就能学会：https://markdown-zh.readthedocs.io/en/latest/ 除此之外，你还需要一款好的 Markdown 软件，我当然是推荐Typora：https://www.typora.io/ 论文 开始了炼丹生涯模式，那 paper 自然就少不了，特别是像深度学习这种发展非常快的领域，每天生产的论文可谓是多不胜数(当然绝大部分都是学术垃圾) 所以说，初来乍到的你们，开始的时候都会很困惑—我该读什么？ 我个人推荐三个途径： 你的导师--摸爬滚打了几十年，姿势不知道比我们高到哪里去了，但是可惜的是老板们的时间一般来说都很有限，不是嫡传弟子，很难说能教你多少。 组里的师哥师姐们--你的师哥师姐们好歹也是比你找来几年，基本上你可能遇到的坑他们都踩过了，而且如果是你们将来能接手他们的工作，对你们来说好处实在不要太多(当然了，也可能接手那种祖传的坑……)，不过像我们组里，师哥师姐们简直是人太好了。 我接下来推荐几个和论文有关的网站，逐一介绍怎么用。 Browse state-of-the-art 网站：https://paperswithcode.com/sota 这是 Reddit 的一个用户 rstoj 做的一个网站，将 ArXiv 上的最新机器学习论文与 Github 上的代码（TensorFlow/PyTorch/MXNet/等）对应起来。相比之前推荐的阅读 ArXiv 的网站，这位用户做出了满足更多研究者的最大需求-- 寻找论文算法实现的代码！ 寻找论文算法实现的代码！ 寻找论文算法实现的代码！ 这个项目索引了大约 5 万篇论文（最近 5 年发布在 arxiv 上的论文）和 1 万个 Github 库。 你可以按标题关键词查询，或者研究领域关键词，如图像分类、文本分类等搜索，也可以按流行程度、最新论文以及 Github 上 Star 数量最多来排列。这个网站能让你跟上机器学习社区流行的最新动态。 Papers with Code github：https://github.com/zziz/pwc/blob/master/README.md#---- GitHub 上非常热门的一个开源项目，总结这这几年顶会里优秀的论文并且附带代码实现，高级炼丹师的绝对福利！ Model ZOO 网站：https://modelzoo.co/ Model Zoo 更加注重深度学习算法的应用价值，里面推荐的项目很多都是偏应用的项目，如果说你研究侧重在应用上，那 Model Zoo 一定是你的首选。 其实我大概也只是抛砖引玉的介绍几个我觉得日常比较实用的。 GitHub 为什么我要单独把 github 从代码里摘出来讲呢？因为 GitHub 实在是太重要了，对一个现代炼丹师来说，你几乎每一天都离不开 GitHub。 开源是一件很伟大的事情，这个问题上升到哲学层面，我不展开讲。 我在这里也并不想对于“送去”再说什么，否则太不“摩登”了。我只想鼓吹我们再吝啬一点，“送去”之外，还得“拿来”，是为“拿来主义”。 —鲁迅《拿来主义》 当然了还有就是借机推广一下自己嘛(皮一下2333)，我目前一直在维护我们实验室的 GitHub 账号 OUCML：https://github.com/OUCMachineLearning/OUCML，我个人总结的学习资料基本上都在上面。(小小声：知乎账号：马卡斯·扬) Git的奇技淫巧 Git常用命令集合，Fork于tips项目 Git是一个 “分布式版本管理工具”，简单的理解版本管理工具：大家在写东西的时候都用过 “回撤” 这个功能，但是回撤只能回撤几步，假如想要找回我三天之前的修改，光用 “回撤” 是找不回来的。而 “版本管理工具” 能记录每次的修改，只要提交到版本仓库，你就可以找到之前任何时刻的状态（文本状态）。 关于如何使用GitHub，看这一篇就够了：https://github.com/521xueweihan/git-tips 深度学习入门 前面乱七八糟聊了这么多，我用哲学三问步入正题。 我是谁 WIKI: 深度学习（英語：deep learning）是机器学习的分支，是一種以人工神經網路為架構，對資料進行表徵學習的算法。[1][2][3][4][5] 深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别[6]）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征&action=edit&redlink=1)。[7] 表征学习的目标是寻求更好的表示方法并建立更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地建立在類似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉動神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。[8] 至今已有數种深度学习框架，如深度神经网络、卷积神经网络和深度置信网络和递归神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并取得了极好的效果。 另外，「深度学习」已成為類似術語，或者说是神经网络的品牌重塑。[9][10] Emmmm,还是简单点说吧，把数据作为 X，标签作为 Y，我们深度学习就是找到一个 F(·)，使得y≈f(x)​。 这个 F()很复杂，我得从（宇宙大爆炸开始讲起）机器学习讲起。 我从哪里来 1943年 由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。1 1958年 计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。 我到哪里去 这个问题说实话，我答不上来。 但是，我可以简要介绍一下目前深度学习比较火的几个领域： CV 计算机视觉（Computational Vision）是由相机拍摄图像， 通过电脑对图像中的目标进行识别和检测。可以说是机器学习在视觉领域的应用，是人工智能领域的一个重要部分。 emmm，这个门槛低，应用广，这几年都快挤爆了。 NLP 自然語言處理（英語：Natural Language Processing，缩写作 NLP）是人工智慧和語言學領域的分支學科。此領域探討如何處理及運用自然語言；自然語言處理包括多方面和步骤，基本有认知、理解、生成等部分。 NLP 相对来说垄断性质较高，也就是其实大家算法都差不多，谁数据多谁 NB。 医学 没做过，不知道。。。。 强化学习 不得不提巨坑 AutoML Google 爸爸一直在努力推进 AutoML，说白了就是让算法自动设计神经网络。 推荐 18 年的一篇综述：https://arxiv.org/pdf/1810.13306 图卷积 这两年爆火的，说实话我也不太懂。。。。 他比我懂👉图卷积神经网络(GCN)详解:包括了数学基础(傅里叶，拉普拉斯) 迁移学习 把从数据集 A 上学到的知识迁移到数据集 B 的任务上。 深度学习框架 所谓工欲善其事必先利其器，之前我们提到过 python 有着很多的第三方库，那我接下来将要解释一下常用的深度学习框架： TensorFlow Google 支持的 TensorFlow 框架，应该是最出名的没有之一了吧。优点是大部分模型都是 TensorFlow 写的，代码复现快，缺点是版本迭代产生的 bug 多如繁星。 https://www.tensorflow.org/overview Keras 新手入门神器，相当于在 TensorFlow 的基础上又包了一层皮，API 接口稳定，代码实现简易，缺点是自由度太低了。 https://keras.io/ pytorch 好用的一匹(我在用)，动态计算图谁用谁知道，缺点之一是：老老实实读源代码吧。 PyTorch 1.0重构和统一了Caffe2和PyTorch 0.4框架的代码库，删除了重复的组件并共享上层抽象，得到了一个统一的框架，支持高效的图模式执行、移动部署和广泛的供应商集成等。这让开发人员可以同时拥有PyTorch和Caffe2的优势，同时做到快速验证和优化性能。PyTorch的命令式前端通过其灵活而高效的编程模型实现了更快速的原型设计和实验，又吸取了Caffe2和ONNX的模块化以及面向生产的特点，使得深度学习项目能从研究原型快速无缝衔接到生产部署，在一个框架中统一实验研究和生产能力。 Theano 不想贴图，两个字--有毒 Caffe 据说做网络结构压缩那一块的小伙伴用的比较多，因为可以方便的改底层的实现。 Caffe2 现在叫 pytorch1.0 MXnet 这本书就是李沐大神用 MXnet 写的，听说挺好用的，就是没啥钱做宣传推广，那我帮他推广一下：https://zh.gluon.ai/chapter_introduction/deep-learning-intro.html 今晚其实写这个写了挺久的，现在已经凌晨 1 点多了，其实我真的挺喜欢深度学习这个研究领域的，说实话总感觉还有好多话想说出来，想和大家聊聊动态计算图，自动微分库，网络结构可视化，Lipschitz条件等等等等…… 接下来的大三了，也许课程压力挺大的，希望喜欢一件事情就能坚持下来吧(๑•̀ㅂ•́)و✧ 要是觉得有用麻烦关注点赞转发三连啦 参考文献： [2]：https://zhuanlan.zhihu.com/p/59086302 "},"introduction/AI_system.html":{"url":"introduction/AI_system.html","title":"为什么要使得AI System具备可解释性呢？","keywords":"","body":"为什么要使得AI System具备可解释性呢？ 如在AI医疗领域，如果无法理解及验证AI System做决策的流程机理，那么以默认相信AI判断的方式是不负责任的（Self Driving etc.），更多讨论可以看以下论文。 Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models 重要的网站Heatmapping 这是一个专门整理Explainable AI的相关想法，尤其是去解释那些State of the art模型，我先展示一个可视化的Demo：LRP Demos链接 数字辨识及重要的像素热力图显示 猫咪的分类会比较直观一下： 看样子是学习到了猫咪的轮廓 以下内容来自Heatmapping！ 关于LRP的介绍： A Tutorial on Implementing LRP A Quick Introduction to Deep Taylor Decomposition 关于LRP的软件： Keras Explanation Toolbox (LRP and other Methods) GitHub project page for the LRP Toolbox TensorFlow LRP Wrapper LRP Code for LSTM 关于Deep Model可解释性的talk： CVPR18: Tutorial: Part 1: Interpreting and Explaining Deep Models in Computer Vision EMBC 2019 Tutorial (Website) Explainable ML, Medical Applications Northern Lights Deep Learning Workshop Keynote(Website | Slides) Explainable ML, Applications 2018 Int. Explainable AI Symposium Keynote(Website | Slides) Explainable ML, Applications ICIP 2018 Tutorial (Website | Slides: 1-Intro, 2-Methods, 3-Evaluation, 4-Applications) Explainable ML, Applications MICCAI 2018 Tutorial (Website | Slides) Explainable ML, Medical Applications Talk at Int. Workshop ML & AI 2018 (Slides) Deep Taylor Decomposition, Validating Explanations WCCI 2018 Keynote (Slides) Explainable ML, LRP, Applications GCPR 2017 Tutorial (Slides) ICASSP 2017 Tutorial (Slides 1-Intro, 2-Methods, 3-Applications) Hightlight Papers S Lapuschkin, S Wäldchen, A Binder, G Montavon, W Samek, KR Müller. Unmasking Clever Hans Predictors and Assessing What Machines Really Learn Nature Communications, 10:1096, 2019 [preprint | bibtex] 入门级介绍： G Montavon, W Samek, KR Müller. Methods for Interpreting and Understanding Deep Neural Networks Digital Signal Processing, 73:1-15, 2018 [preprint | bibtex] W Samek, T Wiegand, KR Müller. Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models ITU Journal: ICT Discoveries - Special Issue 1 - The Impact of AI on Communication Networks and Services, 1(1):39-48, 2018 [preprint, bibtex] 方法相关： S Bach, A Binder, G Montavon, F Klauschen, KR Müller, W Samek. On Pixel-wise Explanations for Non-Linear Classifier Decisions by Layer-wise Relevance Propagation PLOS ONE, 10(7):e0130140, 2015 [preprint, bibtex] G Montavon, S Lapuschkin, A Binder, W Samek, KR Müller. Explaining NonLinear Classification Decisions with Deep Taylor Decomposition Pattern Recognition, 65:211–222, 2017 [preprint, bibtex] L Arras, G Montavon, KR Müller, W Samek. Explaining Recurrent Neural Network Predictions in Sentiment Analysis EMNLP Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, 159-168, 2017 [preprint, bibtex] A Binder, G Montavon, S Lapuschkin, KR Müller, W Samek. Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers Artificial Neural Networks and Machine Learning – ICANN 2016, Part II, Lecture Notes in Computer Science, Springer-Verlag, 9887:63-71, 2016 [preprint, bibtex] PJ Kindermans, KT Schütt, M Alber, KR Müller, D Erhan, B Kim, S Dähne. Learning how to explain neural networks: PatternNet and PatternAttribution International Conference on Learning Representations (ICLR), 2018 L Rieger, P Chormai, G Montavon, LK Hansen, KR Müller. Structuring Neural Networks for More Explainable Predictions in Explainable and Interpretable Models in Computer Vision and Machine Learning, 115-131, Springer SSCML, 2018 J Kauffmann, KR Müller, G Montavon. Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models arXiv:1805.06230, 2018 评估解释 Evaluation of Explanation L Arras, A Osman, KR Müller, W Samek. Evaluating Recurrent Neural Network Explanations arXiv:1904.11829, 2019 W Samek, A Binder, G Montavon, S Bach, KR Müller. Evaluating the Visualization of What a Deep Neural Network has Learned IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660-2673, 2017 [preprint, bibtex] 关于软件的Papers: M Alber, S Lapuschkin, P Seegerer, M Hägele, KT Schütt, G Montavon, W Samek, KR Müller, S Dähne, PJ Kindermans iNNvestigate neural networks!. arXiv:1808.04260, 2018 S Lapuschkin, A Binder, G Montavon, KR Müller, W Samek The Layer-wise Relevance Propagation Toolbox for Artificial Neural Networks Journal of Machine Learning Research, 17(114):1−5, 2016 [preprint, bibtex] Application of Science: I Sturm, S Bach, W Samek, KR Müller. Interpretable Deep Neural Networks for Single-Trial EEG Classification Journal of Neuroscience Methods, 274:141–145, 2016 [preprint, bibtex] A Binder, M Bockmayr, M Hägele, S Wienert, D Heim, K Hellweg, A Stenzinger, L Parlow, J Budczies, B Goeppert, D Treue, M Kotani, M Ishii, M Dietel, A Hocke, C Denkert, KR Müller, F Klauschen. Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles arXiv:1805.11178, 2018 F Horst, S Lapuschkin, W Samek, KR Müller, WI Schöllhorn. Explaining the Unique Nature of Individual Gait Patterns with Deep Learning Scientific Reports, 9:2391, 2019 [preprint, bibtex] AW Thomas, HR Heekeren, KR Müller, W Samek. Analyzing Neuroimaging Data Through Recurrent Deep Learning Models arXiv:1810.09945, 2018 文本上的应用： L Arras, F Horn, G Montavon, KR Müller, W Samek. \"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach PLOS ONE, 12(8):e0181142, 2017 [preprint, bibtex] L Arras, G Montavon, KR Müller, W Samek. Explaining Recurrent Neural Network Predictions in Sentiment Analysis EMNLP Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, 159-168, 2017 [preprint, bibtex] L Arras, F Horn, G Montavon, KR Müller, W Samek. Explaining Predictions of Non-Linear Classifiers in NLP ACL Workshop on Representation Learning for NLP, 1-7, 2016 [preprint, bibtex] F Horn, L Arras, G Montavon, KR Müller, W Samek. Exploring text datasets by visualizing relevant words arXiv:1707.05261, 2017 图像及脸部识别的应用： S Lapuschkin, A Binder, G Montavon, KR Müller, W Samek. Analyzing Classifiers: Fisher Vectors and Deep Neural Networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2912-2920, 2016 [preprint, bibtex] S Lapuschkin, A Binder, KR Müller, W Samek. Understanding and Comparing Deep Neural Networks for Age and Gender Classification IEEE International Conference on Computer Vision Workshops (ICCVW), 1629-1638, 2017 [preprint, bibtex] C Seibold, W Samek, A Hilsmann, P Eisert. Accurate and Robust Neural Networks for Security Related Applications Exampled by Face Morphing Attacks arXiv:1806.04265, 2018 S Bach, A Binder, KR Müller, W Samek. Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth Proceedings of the IEEE International Conference on Image Processing (ICIP), 2271-2275, 2016 [preprint, bibtex] A Binder, S Bach, G Montavon, KR Müller, W Samek. Layer-wise Relevance Propagation for Deep Neural Network Architectures Proceedings of the 7th International Conference on Information Science and Applications (ICISA), 6679:913-922, Springer Singapore, 2016 [preprint, bibtex] F Arbabzadah, G Montavon, KR Müller, W Samek. Identifying Individual Facial Expressions by Deconstructing a Neural Network Pattern Recognition - 38th German Conference, GCPR 2016, Lecture Notes in Computer Science, 9796:344-354, 2016 [preprint, bibtex] 视频的应用： C Anders, G Montavon, W Samek, KR Müller. Understanding Patch-Based Learning by Explaining Predictions arXiv:1806.06926, 2018 V Srinivasan, S Lapuschkin, C Hellge, KR Müller, W Samek. Interpretable human action recognition in compressed domain Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1692-1696, 2017 [preprint, bibtex] 语音的应用： S Becker, M Ackermann, S Lapuschkin, KR Müller, W Samek. Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals arXiv:1807.03418, 2018 短Paper W Samek, G Montavon, A Binder, S Lapuschkin, and KR Müller. Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation NIPS Workshop on Interpretable ML for Complex Systems, 1-5, 2016 [preprint, bibtex] G Montavon, S Bach, A Binder, W Samek, KR Müller. Deep Taylor Decomposition of Neural Networks ICML Workshop on Visualization for Deep Learning, 1-3, 2016 [preprint, bibtex] A Binder, W Samek, G Montavon, S Bach, KR Müller. Analyzing and Validating Neural Networks Predictions ICML Workshop on Visualization for Deep Learning, 1-4, 2016 [preprint, bibtex] 最后：BVLC Model Zoo Contributions Pascal VOC 2012 Multilabel Model (see paper): [caffemodel] [prototxt] Age and Gender Classification Models (see paper): [data and models] 相关资料： [1]Samek, Wojciech, Thomas Wiegand, and Klaus-Robert Müller. \"Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models.\"arXiv preprint arXiv:1708.08296(2017). "},"code_technique/":{"url":"code_technique/","title":"编程技巧","keywords":"","body":" python编程技巧 python常见排序 opencv-python极速入门 pytorch常用代码 pytorch常用代码段合集 pytorch训练技巧 pytorch解冻 pytorch网络可视化 PSNR&&SSIM SPP Tensor to img && imge to tensor "},"code_technique/python/python_technique.html":{"url":"code_technique/python/python_technique.html","title":"python编程技巧","keywords":"","body":"Python 3 新特性：类型注解 前几天有同学问到，这个写法是什么意思： def add(x:int, y:int) -> int: return x + y 我们知道 Python 是一种动态语言，变量以及函数的参数是不区分类型。因此我们定义函数只需要这样写就可以了： def add(x, y): return x + y 这样的好处是有极大的灵活性，但坏处就是对于别人代码，无法一眼判断出参数的类型，IDE 也无法给出正确的提示。 于是 Python 3 提供了一个新的特性： 函数注解 也就是文章开头的这个例子： def add(x:int, y:int) -> int: return x + y 用 : 类型 的形式指定函数的参数类型，用 -> 类型 的形式指定函数的返回值类型。 然后特别要强调的是，Python 解释器并不会因为这些注解而提供额外的校验，没有任何的类型检查工作。也就是说，这些类型注解加不加，对你的代码来说没有任何影响： 输出： 但这么做的好处是： 让别的程序员看得更明白 让 IDE 了解类型，从而提供更准确的代码提示、补全和语法检查（包括类型检查，可以看到 str 和 float 类型的参数被高亮提示） 在函数的 __annotations__ 属性中会有你设定的注解： 输出： 在 Python 3.6 中，又引入了对变量类型进行注解的方法： a: int = 123 b: str = 'hello' 更进一步，如果你需要指明一个全部由整数组成的列表： from typing import List l: List[int] = [1, 2, 3] 但同样，这些仅仅是“注解”，不会对代码产生任何影响。 不过，你可以通过 mypy 库来检验最终代码是否符合注解。 安装 mypy： pip install mypy 执行代码： mypy test.py 如果类型都符合，则不会有任何输出，否则就会给出类似输出： 这些新特性也许你并不会在代码中使用，不过当你在别人的代码中看到时，请按照对方的约定进行赋值或调用。 当然，也不排除 Python 以后的版本把类型检查做到解释器里，谁知道呢。 "},"code_technique/python/sort.html":{"url":"code_technique/python/sort.html","title":"python常见排序","keywords":"","body":"排序算法相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面； 不稳定：如果a原本在b的前面，而a=b，排序之后a可能会出现在b的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 常见排序 冒泡排序 冒泡排序重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换。因为每次遍历，最大的元素都会被送到最右端，故名冒泡排序。 步骤： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 代码实现： def bubble_sort(nums): size = len(nums) for i in range(size): for j in range(size-i-1): if nums[j] > nums[j+1]: nums[j+1], nums[j] = nums[j], nums[j+1] return nums 我们可以考虑设置一标志性变量pos，用于记录每趟排序中最后一次进行交换的位置。由于pos位置之后的记录均已交换到位，故在进行下一趟排序时只要扫描到pos位置即可。 改进后的冒泡排序： def bubble_sort2(nums): size = len(nums) i = size - 1 while i > 0: pos = 0 for j in range(i): if nums[j] > nums[j+1]: pos = j nums[j], nums[j+1] = nums[j+1], nums[j] i = pos return nums 冒泡排序动图演示： 选择排序 选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理是：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 代码实现如下： def select_sort(nums): size = len(nums) for i in range(size-1): # 找出最小的数 min_index = i for j in range(i+1, size): if nums[j] 选择排序动图演示如下： 插入排序 插入排序（Insertion-Sort）的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 具体步骤： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 代码实现： def insertion_sort(nums): size = len(nums) for i in range(1, size): cur_val = nums[i] j = i - 1 while j >= 0 and nums[j] > cur_val: nums[j+1] = nums[j] j -= 1 nums[j+1] = cur_val # 找到位置进行插入 return nums 可以考虑使用二分查找来寻找插入的位置： def insertion_sort2(nums): size = len(nums) for i in range(1, size): val = nums[i] left, right = 0, i-1 while left 插入排序动图演示如下： 归并排序 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 代码实现如下： def merge_sort(nums): size = len(nums) if size 其动图演示如下： 快速排序 快速排序是处理大数据最快的排序算法之一。它的基本思想是，通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 快速排序基本步骤： 从数列中挑出一个元素，称为 \"基准\"（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 其代码实现如下： def partition(nums, left, right): pivot = left # 使用左端元素作为基准 for i in range(left+1, right+1): if nums[i] = right: return pivot = partition(nums, left, right) quick_sort_helper(nums, left, pivot-1) quick_sort_helper(nums, pivot+1, right) return quick_sort_helper(nums, left, right) 如果不要求在原地修改数组： def quick_sort2(arr): if len(arr) = arr[0]]) 快速排序的动图演示如下： 后面的几种排序方法比较少见，仅在概念上进行讲解。 希尔排序 先将整个待排元素序列分割成若干子序列（由相隔某个“增量”的元素组成的）分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序（增量足够小）时，再对全体元素进行一次直接插入排序（增量为1）。 堆排序 堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 步骤： 创建最大堆:将堆所有数据重新排序，使其成为最大堆 最大堆调整:作用是保持最大堆的性质，是创建最大堆的核心子程序 堆排序:移除位在第一个数据的根节点，并做最大堆调整的递归运算 计数排序 计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。 算法的步骤如下： 找出待排序的数组中最大和最小的元素 统计数组中每个值为i的元素出现的次数，存入数组C的第i项 对所有的计数累加（从C中的位置为1的元素开始，每一项和前一项相加） 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。 桶排序 桶排序 (Bucket sort)或所谓的箱排序，是一个排序算法，工作的原理是将数组分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序） 桶排序以下列程序进行： 设置一个定量的数组当作空桶子。 寻访串行，并且把项目一个一个放到对应的桶子去。（hash） 对每个不是空的桶子进行排序。 从不是空的桶子里把项目再放回原来的串行中。 基数排序 基数排序（英语：Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 延伸-外部排序 从是否使用外存方面来看，我们可以将排序算法分为内部排序和外部排序： 上面讲的十种排序算法都属于内部排序算法，也就是排序的整个过程都在内存中完成。而当待排序的文件比内存的可使用容量还大时，文件无法一次性放到内存中进行排序，需要借助于外部存储器（例如硬盘、U盘、光盘），这时就需要用到外部排序算法来解决。下面简单介绍一下外部排序算法。 外部排序算法由两个阶段构成： 按照内存大小，将大文件分成若干长度为 l 的子文件（l 应小于内存的可使用容量），然后将各个子文件依次读入内存，使用适当的内部排序算法对其进行排序（排好序的子文件统称为“归并段”或者“顺段”），将排好序的归并段重新写入外存，为下一个子文件排序腾出内存空间； 对得到的顺段进行合并，直至得到整个有序的文件为止。 例如，我们要对一个大文件（无法放进内存）进行排序，可以将其分成多个大小可以放进内存的临时文件，然后将这些较小的临时文件依次进入内存，采取适当的内存排序算法对其中的记录进行排序，将得到的有序文件（初始归并段）移至外存；之后再对这些排序好的临时文件两两归并，直至得到一个完整的有序文件。如下图所示： 总结 其中，n表示数据规模，k表示桶的个数，In-place表示不占用额外内存，Out-place表示占用额外内存。 "},"code_technique/python/opencv.html":{"url":"code_technique/python/opencv.html","title":"opencv-python极速入门","keywords":"","body":"opencv-python 极速入门 什么是OpenCV-Python? OpenCV是一个开源的计算机视觉（computer vision）和机器学习库。它拥有超过2500个优化算法，包括经典和最先进的计算机视觉和机器学习算法。它有很多语言接口，包括Python、Java、c++和Matlab。 这里，我们将处理Python接口。 安装 在Windows上, 请在这里查看指南。地址：https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_setup_in_windows/py_setup_in_windows.html 在 Linux上, 请在这里查看指南。地址：https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html 图像导入&显示 警告1: 通过openCV读取图像时，它不是以RGB 颜色空间来读取，而是以BGR 颜色空间。有时候这对你来说不是问题，只有当你想在图片中添加一些颜色时，你才会遇到问题。 有两种解决方案: 将R — 第一个颜色值(红色)和B  — 第三个颜色值(蓝色) 交换, 这样红色就是 (0,0,255) 而不是(255,0,0)。 将颜色空间变成RGB: 使用rgb_image代替image继续处理代码。 警告2: 要关闭显示图像的窗口，请按任意按钮。如果你使用关闭按钮，它可能会导致窗口冻结(我在Jupyter笔记本上运行代码时发生了这种情况)。 为了简单起见，在整个教程中，我将使用这种方法来查看图像: 来源：Pixabay 裁剪 来源：Pixabay 裁剪后的狗狗 其中： image[10:500,500:200] 是 image[y:y+h,x:x+w]。 调整大小 来源：Pexels 调整大小到20%后 这个调整大小函数会保持原始图像的尺寸比例。 更多图像缩放函数，请查看这里。（https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/ ） 旋转 左图: 图片来自Pexels的Jonathan Meyer。右图: 进行180度旋转之后的狗狗。 image.shape输出高度、宽度和通道。M是旋转矩阵——它将图像围绕其中心旋转180度。 -ve表示顺时针旋转图像的角度 & +ve逆表示逆时针旋转图像的角度。 灰度和阈值(黑白效果) 来源：Pexels gray_image 是灰度图像的单通道版本。 这个threshold函数将把所有比127深(小)的像素点阴影值设定为0，所有比127亮(大)的像素点阴影值设定为255。 另一个例子: 这将把所有阴影值小于150的像素点设定为10和所有大于150的像素点设定为200。 更多有关thresholding函数的内容，请查看这里。（https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html ） 左图：灰阶狗狗。右图：黑白狗狗。 模糊/平滑 左图：图像来自Pixabay。右图：模糊后的狗狗。 高斯模糊函数接受3个参数: 第一个参数是要模糊的图像。 第二个参数必须是一个由两个正奇数组成的元组。当它们增加，模糊效果也会增加。 第三个参数是sigmaX和sigmaY。当左边位于0时，它们会自动从内部大小计算出来。 更多关于模糊函数的内容，请查看这里。（https://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html ） 在图像上绘制矩形框或边框 左图：图像来自Pixabay。右图：脸上有一个矩形框的狗狗。 rectangle函数接受5个参数: 第一个参数是图像。 第二个参数是x1, y1 -左上角坐标。 第三个参数是x2, y2 -右下角坐标。 第四个参数是矩形颜色(GBR/RGB，取决于你如何导入图像)。 第五个参数是矩形线宽。 绘制一条线 左图：图像来自Pixabay。右图：两只狗狗用一条线分开。 line函数接受5个参数: 第一个参数是要画的线所在的图像。 第二个参数是x1, y1。 第三个参数是x2, y2。 第四个参数是线条颜色(GBR/RGB，取决于你如何导入图像)。 第五个参数是线宽。 在图片上写入文字 左图：图像来自Pixabay。右图：两只狗狗用一条线分开。 putText函数接受 七个参数： 第一个参数是要写入文本的图像。 第二个参数是待写入文本。 第三个参数是x, y——文本开始的左下角坐标。 第四个参数是字体类型。 第五个参数是字体大小。 第六个参数是颜色(GBR/RGB，取决于你如何导入图像)。 第七个参数是文本线条的粗细。 人脸检测 这里没有找到狗狗照片，很遗憾：（ 图片来自Pixabay,作者：Free-Photos。 detectMultiScale函数是一种检测对象的通用函数。因为我们调用的是人脸级联，所以它会检测到人脸。 detectMultiScale函数接受4个参数： 第一个参数是灰阶图像。 第二个参数是scaleFactor。因为有些人脸可能离镜头更近，所以看起来会比后台的人脸更大。比例系数弥补了这一点。 检测算法使用一个移动窗口来检测对象。minNeighbors定义在当前对象附近检测到多少对象，然后再声明检测到人脸。 与此同时，minsize给出了每个窗口的大小。 检测到两张人脸。 轮廓——一种对象检测方法 使用基于颜色的图像分割，你可以来检测对象。 cv2.findContours & cv2.drawContours 这两个函数可以帮助你做到这一点。 最近，我写了一篇非常详细的文章，叫做《使用Python通过基于颜色的图像分割来进行对象检测》。你需要知道的关于轮廓的一切都在那里。（https://towardsdatascience.com/object-detection-via-color-based-image-segmentation-using-python-e9b7c72f0e11 ） 最终,保存图片 总结 OpenCV是一个非常容易使用的算法库，可以用于3D建模、高级图像和视频编辑、跟踪视频中的标识对象、对视频中正在做某个动作的人进行分类、从图像数据集中找到相似的图像，等等。 最重要的是，学习OpenCV对于那些想要参与与图像相关的机器学习项目的人来说是至关重要的。 "},"code_technique/pytorch/pytorch1.html":{"url":"code_technique/pytorch/pytorch1.html","title":"pytorch常用代码","keywords":"","body":"本文代码基于PyTorch 1.0版本，需要用到以下包 import collections import os import shutil import tqdm import numpy as np import PIL.Image import torch import torchvision 1. 基础配置 检查PyTorch版本 torch.__version__ # PyTorch version torch.version.cuda # Corresponding CUDA version torch.backends.cudnn.version() # Corresponding cuDNN version torch.cuda.get_device_name(0) # GPU type 更新PyTorch PyTorch将被安装在anaconda3/lib/python3.7/site-packages/torch/目录下。 conda update pytorch torchvision -c pytorch 固定随机种子 torch.manual_seed(0) torch.cuda.manual_seed_all(0) 指定程序运行在特定GPU卡上 在命令行指定环境变量 CUDA_VISIBLE_DEVICES=0,1 python train.py 或在代码中指定 os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' 判断是否有CUDA支持 torch.cuda.is_available() 设置为cuDNN benchmark模式 Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。 torch.backends.cudnn.benchmark = True 如果想要避免这种结果波动，设置 torch.backends.cudnn.deterministic = True 清除GPU存储 有时Control-C中止运行后GPU存储没有及时释放，需要手动清空。在PyTorch内部可以 torch.cuda.empty_cache() 或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程 ps aux | grep python kill -9 [pid] 或者直接重置没有被清空的GPU nvidia-smi --gpu-reset -i [gpu_id] 2. 张量处理 张量基本信息 tensor.type() # Data type tensor.size() # Shape of the tensor. It is a subclass of Python tuple tensor.dim() # Number of dimensions. 数据类型转换 # Set default tensor type. Float in PyTorch is much faster than double. torch.set_default_tensor_type(torch.FloatTensor) # Type convertions. tensor = tensor.cuda() tensor = tensor.cpu() tensor = tensor.float() tensor = tensor.long() torch.Tensor与np.ndarray转换 # torch.Tensor -> np.ndarray. ndarray = tensor.cpu().numpy() # np.ndarray -> torch.Tensor. tensor = torch.from_numpy(ndarray).float() tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride torch.Tensor与PIL.Image转换 PyTorch中的张量默认采用N×D×H×W的顺序，并且数据范围在[0, 1]，需要进行转置和规范化。 # torch.Tensor -> PIL.Image. image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255 ).byte().permute(1, 2, 0).cpu().numpy()) image = torchvision.transforms.functional.to_pil_image(tensor) # Equivalently way # PIL.Image -> torch.Tensor. tensor = torch.from_numpy(np.asarray(PIL.Image.open(path)) ).permute(2, 0, 1).float() / 255 tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way np.ndarray与PIL.Image转换 # np.ndarray -> PIL.Image. image = PIL.Image.fromarray(ndarray.astypde(np.uint8)) # PIL.Image -> np.ndarray. ndarray = np.asarray(PIL.Image.open(path)) 从只包含一个元素的张量中提取值 这在训练时统计loss的变化过程中特别有用。否则这将累积计算图，使GPU存储占用量越来越大。 value = tensor.item() 张量形变 张量形变常常需要用于将卷积层特征输入全连接层的情形。相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。 tensor = torch.reshape(tensor, shape) 打乱顺序 tensor = tensor[torch.randperm(tensor.size(0))] # Shuffle the first dimension 水平翻转 PyTorch不支持tensor[::-1]这样的负步长操作，水平翻转可以用张量索引实现。 # Assume tensor has shape N*D*H*W. tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()] 复制张量 有三种复制的方式，对应不同的需求。 # Operation | New/Shared memory | Still in computation graph | tensor.clone() # | New | Yes | tensor.detach() # | Shared | No | tensor.detach.clone()() # | New | No | 拼接张量 注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，而torch.stack会新增一维。例如当参数是3个10×5的张量，torch.cat的结果是30×5的张量，而torch.stack的结果是3×10×5的张量。 tensor = torch.cat(list_of_tensors, dim=0) tensor = torch.stack(list_of_tensors, dim=0) 将整数标记转换成独热（one-hot）编码 PyTorch中的标记默认从0开始。 N = tensor.size(0) one_hot = torch.zeros(N, num_classes).long() one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long()) 得到非零/零元素 torch.nonzero(tensor) # Index of non-zero elements torch.nonzero(tensor == 0) # Index of zero elements torch.nonzero(tensor).size(0) # Number of non-zero elements torch.nonzero(tensor == 0).size(0) # Number of zero elements 判断两个张量相等 torch.allclose(tensor1, tensor2) # float tensor torch.equal(tensor1, tensor2) # int tensor 张量扩展 # Expand tensor of shape 64*512 to shape 64*512*7*7. torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7) 矩阵乘法 # Matrix multiplication: (m*n) * (n*p) -> (m*p). result = torch.mm(tensor1, tensor2) # Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p). result = torch.bmm(tensor1, tensor2) # Element-wise multiplication. result = tensor1 * tensor2 计算两组数据之间的两两欧式距离 # X1 is of shape m*d, X2 is of shape n*d. dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2)) 3. 模型定义 卷积层 最常用的卷积层配置是 conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True) conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True) 如果卷积层配置比较复杂，不方便计算输出大小时，可以利用如下可视化工具辅助 Convolution Visualizerezyang.github.io GAP（Global average pooling）层 gap = torch.nn.AdaptiveAvgPool2d(output_size=1) 双线性汇合（bilinear pooling）[1] X = torch.reshape(N, D, H * W) # Assume X has shape N*D*H*W X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W) # Bilinear pooling assert X.size() == (N, D, D) X = torch.reshape(X, (N, D * D)) X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5) # Signed-sqrt normalization X = torch.nn.functional.normalize(X) # L2 normalization 多卡同步BN（Batch normalization） 当使用torch.nn.DataParallel将代码运行在多张GPU卡上时，PyTorch的BN层默认操作是各卡上数据独立地计算均值和标准差，同步BN使用所有卡上的数据一起计算BN层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。 vacancy/Synchronized-BatchNorm-PyTorchgithub.com 现在PyTorch官方已经支持同步BN操作 sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 将已有网络的所有BN层改为同步BN层 def convertBNtoSyncBN(module, process_group=None): '''Recursively replace all BN layers to SyncBN layer. Args: module[torch.nn.Module]. Network ''' if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group) sync_bn.running_mean = module.running_mean sync_bn.running_var = module.running_var if module.affine: sync_bn.weight = module.weight.clone().detach() sync_bn.bias = module.bias.clone().detach() return sync_bn else: for name, child_module in module.named_children(): setattr(module, name) = convert_syncbn_model(child_module, process_group=process_group)) return module 类似BN滑动平均 如果要实现类似BN滑动平均的操作，在forward函数中要使用原地（inplace）操作给滑动平均赋值。 class BN(torch.nn.Module) def __init__(self): ... self.register_buffer('running_mean', torch.zeros(num_features)) def forward(self, X): ... self.running_mean += momentum * (current - self.running_mean) 计算模型整体参数量 num_parameters = sum(torch.numel(parameter) for parameter in model.parameters()) 类似Keras的model.summary()输出模型信息 sksq96/pytorch-summarygithub.com 模型权值初始化 注意model.modules()和model.children()的区别：model.modules()会迭代地遍历模型的所有子层，而model.children()只会遍历模型下的一层。 # Common practise for initialization. for layer in model.modules(): if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.BatchNorm2d): torch.nn.init.constant_(layer.weight, val=1.0) torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.Linear): torch.nn.init.xavier_normal_(layer.weight) if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) # Initialization with given tensor. layer.weight = torch.nn.Parameter(tensor) 部分层使用预训练模型 注意如果保存的模型是torch.nn.DataParallel，则当前的模型也需要是torch.nn.DataParallel。torch.nn.DataParallel(model).module == model。 model.load_state_dict(torch.load('model,pth'), strict=False) 将在GPU保存的模型加载到CPU model.load_state_dict(torch.load('model,pth', map_location='cpu')) 4. 数据准备、特征提取与微调 图像分块打散（image shuffle）/区域混淆机制（region confusion mechanism，RCM）[2] # X is torch.Tensor of size N*D*H*W. # Shuffle rows Q = (torch.unsqueeze(torch.arange(num_blocks), dim=1) * torch.ones(1, num_blocks).long() + torch.randint(low=-neighbour, high=neighbour, size=(num_blocks, num_blocks))) Q = torch.argsort(Q, dim=0) assert Q.size() == (num_blocks, num_blocks) X = [torch.chunk(row, chunks=num_blocks, dim=2) for row in torch.chunk(X, chunks=num_blocks, dim=1)] X = [[X[Q[i, j].item()][j] for j in range(num_blocks)] for i in range(num_blocks)] # Shulle columns. Q = (torch.ones(num_blocks, 1).long() * torch.unsqueeze(torch.arange(num_blocks), dim=0) + torch.randint(low=-neighbour, high=neighbour, size=(num_blocks, num_blocks))) Q = torch.argsort(Q, dim=1) assert Q.size() == (num_blocks, num_blocks) X = [[X[i][Q[i, j].item()] for j in range(num_blocks)] for i in range(num_blocks)] Y = torch.cat([torch.cat(row, dim=2) for row in X], dim=1) 得到视频数据基本信息 import cv2 video = cv2.VideoCapture(mp4_path) height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)) num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) fps = int(video.get(cv2.CAP_PROP_FPS)) video.release() TSN每段（segment）采样一帧视频[3] K = self._num_segments if is_train: if num_frames > K: # Random index for each segment. frame_indices = torch.randint( high=num_frames // K, size=(K,), dtype=torch.long) frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.randint( high=num_frames, size=(K - num_frames,), dtype=torch.long) frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), frame_indices)))[0] else: if num_frames > K: # Middle index for each segment. frame_indices = num_frames / K // 2 frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), torch.arange(K - num_frames))))[0] assert frame_indices.size() == (K,) return [frame_indices[i] for i in range(K)] 提取ImageNet预训练模型某层的卷积特征 # VGG-16 relu5-3 feature. model = torchvision.models.vgg16(pretrained=True).features[:-1] # VGG-16 pool5 feature. model = torchvision.models.vgg16(pretrained=True).features # VGG-16 fc7 feature. model = torchvision.models.vgg16(pretrained=True) model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3]) # ResNet GAP feature. model = torchvision.models.resnet18(pretrained=True) model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) with torch.no_grad(): model.eval() conv_representation = model(image) 提取ImageNet预训练模型多层的卷积特征 class FeatureExtractor(torch.nn.Module): \"\"\"Helper class to extract several convolution features from the given pre-trained model. Attributes: _model, torch.nn.Module. _layers_to_extract, list or set Example: >>> model = torchvision.models.resnet152(pretrained=True) >>> model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) >>> conv_representation = FeatureExtractor( pretrained_model=model, layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image) \"\"\" def __init__(self, pretrained_model, layers_to_extract): torch.nn.Module.__init__(self) self._model = pretrained_model self._model.eval() self._layers_to_extract = set(layers_to_extract) def forward(self, x): with torch.no_grad(): conv_representation = [] for name, layer in self._model.named_children(): x = layer(x) if name in self._layers_to_extract: conv_representation.append(x) return conv_representation 其他预训练模型 Cadene/pretrained-models.pytorchgithub.com 微调全连接层 model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False model.fc = nn.Linear(512, 100) # Replace the last fc layer optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4) 以较大学习率微调全连接层，较小学习率微调卷积层 model = torchvision.models.resnet18(pretrained=True) finetuned_parameters = list(map(id, model.fc.parameters())) conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters) parameters = [{'params': conv_parameters, 'lr': 1e-3}, {'params': model.fc.parameters()}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) 5. 模型训练 常用训练和验证数据预处理 其中ToTensor操作会将PIL.Image或形状为H×W×D，数值范围为[0, 255]的np.ndarray转换为形状为D×H×W，数值范围为[0.0, 1.0]的torch.Tensor。 train_transform = torchvision.transforms.Compose([ torchvision.transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0)), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) val_transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(256), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) 训练基本代码框架 for t in epoch(80): for images, labels in tqdm.tqdm(train_loader, desc='Epoch %3d' % (t + 1)): images, labels = images.cuda(), labels.cuda() scores = model(images) loss = loss_function(scores, labels) optimizer.zero_grad() loss.backward() optimizer.step() 标记平滑（label smoothing）[4] for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() N = labels.size(0) # C is the number of classes. smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda() smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9) score = model(images) log_prob = torch.nn.functional.log_softmax(score, dim=1) loss = -torch.sum(log_prob * smoothed_labels) / N optimizer.zero_grad() loss.backward() optimizer.step() Mixup[5] beta_distribution = torch.distributions.beta.Beta(alpha, alpha) for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() # Mixup images. lambda_ = beta_distribution.sample([]).item() index = torch.randperm(images.size(0)).cuda() mixed_images = lambda_ * images + (1 - lambda_) * images[index, :] # Mixup loss. scores = model(mixed_images) loss = (lambda_ * loss_function(scores, labels) + (1 - lambda_) * loss_function(scores, labels[index])) optimizer.zero_grad() loss.backward() optimizer.step() L1正则化 l1_regularization = torch.nn.L1Loss(reduction='sum') loss = ... # Standard cross-entropy loss for param in model.parameters(): loss += lambda_ * torch.sum(torch.abs(param)) loss.backward() 不对偏置项进行L2正则化/权值衰减（weight decay） bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias') others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias') parameters = [{'parameters': bias_list, 'weight_decay': 0}, {'parameters': others_list}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) 梯度裁剪（gradient clipping） torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20) 计算Softmax输出的准确率 score = model(images) prediction = torch.argmax(score, dim=1) num_correct = torch.sum(prediction == labels).item() accuruacy = num_correct / labels.size(0) 可视化模型前馈的计算图 szagoruyko/pytorchvizgithub.com 可视化学习曲线 有Facebook自己开发的Visdom和Tensorboard（仍处于实验阶段）两个选择。 facebookresearch/visdomgithub.com torch.utils.tensorboard - PyTorch master documentationpytorch.org # Example using Visdom. vis = visdom.Visdom(env='Learning curve', use_incoming_socket=False) assert self._visdom.check_connection() self._visdom.close() options = collections.namedtuple('Options', ['loss', 'acc', 'lr'])( loss={'xlabel': 'Epoch', 'ylabel': 'Loss', 'showlegend': True}, acc={'xlabel': 'Epoch', 'ylabel': 'Accuracy', 'showlegend': True}, lr={'xlabel': 'Epoch', 'ylabel': 'Learning rate', 'showlegend': True}) for t in epoch(80): tran(...) val(...) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]), name='train', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]), name='val', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]), name='train', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]), name='val', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]), win='Learning rate', update='append', opts=options.lr) 得到当前学习率 # If there is one global learning rate (which is the common case). lr = next(iter(optimizer.param_groups))['lr'] # If there are multiple learning rates for different layers. all_lr = [] for param_group in optimizer.param_groups: all_lr.append(param_group['lr']) 学习率衰减 # Reduce learning rate when validation accuarcy plateau. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True) for t in range(0, 80): train(...); val(...) scheduler.step(val_acc) # Cosine annealing learning rate. scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80) # Reduce learning rate by 10 at given epochs. scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1) for t in range(0, 80): scheduler.step() train(...); val(...) # Learning rate warmup by 10 epochs. scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10) for t in range(0, 10): scheduler.step() train(...); val(...) 保存与加载断点 注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。 # Save checkpoint. is_best = current_acc > best_acc best_acc = max(best_acc, current_acc) checkpoint = { 'best_acc': best_acc, 'epoch': t + 1, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), } model_path = os.path.join('model', 'checkpoint.pth.tar') torch.save(checkpoint, model_path) if is_best: shutil.copy('checkpoint.pth.tar', model_path) # Load checkpoint. if resume: model_path = os.path.join('model', 'checkpoint.pth.tar') assert os.path.isfile(model_path) checkpoint = torch.load(model_path) best_acc = checkpoint['best_acc'] start_epoch = checkpoint['epoch'] model.load_state_dict(checkpoint['model']) optimizer.load_state_dict(checkpoint['optimizer']) print('Load checkpoint at epoch %d.' % start_epoch) 计算准确率、查准率（precision）、查全率（recall） # data['label'] and data['prediction'] are groundtruth label and prediction # for each image, respectively. accuracy = np.mean(data['label'] == data['prediction']) * 100 # Compute recision and recall for each class. for c in range(len(num_classes)): tp = np.dot((data['label'] == c).astype(int), (data['prediction'] == c).astype(int)) tp_fp = np.sum(data['prediction'] == c) tp_fn = np.sum(data['label'] == c) precision = tp / tp_fp * 100 recall = tp / tp_fn * 100 6. 模型测试 计算每个类别的查准率（precision）、查全率（recall）、F1和总体指标 import sklearn.metrics all_label = [] all_prediction = [] for images, labels in tqdm.tqdm(data_loader): # Data. images, labels = images.cuda(), labels.cuda() # Forward pass. score = model(images) # Save label and predictions. prediction = torch.argmax(score, dim=1) all_label.append(labels.cpu().numpy()) all_prediction.append(prediction.cpu().numpy()) # Compute RP and confusion matrix. all_label = np.concatenate(all_label) assert len(all_label.shape) == 1 all_prediction = np.concatenate(all_prediction) assert all_label.shape == all_prediction.shape micro_p, micro_r, micro_f1, _ = sklearn.metrics.precision_recall_fscore_support( all_label, all_prediction, average='micro', labels=range(num_classes)) class_p, class_r, class_f1, class_occurence = sklearn.metrics.precision_recall_fscore_support( all_label, all_prediction, average=None, labels=range(num_classes)) # Ci,j = #{y=i and hat_y=j} confusion_mat = sklearn.metrics.confusion_matrix( all_label, all_prediction, labels=range(num_classes)) assert confusion_mat.shape == (num_classes, num_classes) 将各类结果写入电子表格 import csv # Write results onto disk. with open(os.path.join(path, filename), 'wt', encoding='utf-8') as f: f = csv.writer(f) f.writerow(['Class', 'Label', '# occurence', 'Precision', 'Recall', 'F1', 'Confused class 1', 'Confused class 2', 'Confused class 3', 'Confused 4', 'Confused class 5']) for c in range(num_classes): index = np.argsort(confusion_mat[:, c])[::-1][:5] f.writerow([ label2class[c], c, class_occurence[c], '%4.3f' % class_p[c], '%4.3f' % class_r[c], '%4.3f' % class_f1[c], '%s:%d' % (label2class[index[0]], confusion_mat[index[0], c]), '%s:%d' % (label2class[index[1]], confusion_mat[index[1], c]), '%s:%d' % (label2class[index[2]], confusion_mat[index[2], c]), '%s:%d' % (label2class[index[3]], confusion_mat[index[3], c]), '%s:%d' % (label2class[index[4]], confusion_mat[index[4], c])]) f.writerow(['All', '', np.sum(class_occurence), micro_p, micro_r, micro_f1, '', '', '', '', '']) 7. PyTorch其他注意事项 模型定义 建议有参数的层和汇合（pooling）层使用torch.nn模块定义，激活函数直接使用torch.nn.functional。torch.nn模块和torch.nn.functional的区别在于，torch.nn模块在计算时底层调用了torch.nn.functional，但torch.nn模块包括该层参数，还可以应对训练和测试两种网络状态。使用torch.nn.functional时要注意网络状态，如 def forward(self, x): ... x = torch.nn.functional.dropout(x, p=0.5, training=self.training) model(x)前用model.train()和model.eval()切换网络状态。 不需要计算梯度的代码块用with torch.nograd()包含起来。model.eval()和torch.no**grad()的区别在于，model.eval()是将网络切换为测试状态，例如BN和随机失活（dropout）在训练和测试阶段使用不同的计算方法。torch.no_grad()是关闭PyTorch张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行loss.backward()。 torch.nn.CrossEntropyLoss的输入不需要经过Softmax。torch.nn.CrossEntropyLoss等价于torch.nn.functional.log_softmax + torch.nn.NLLLoss。 loss.backward()前用optimizer.zero_grad()清除累积梯度。optimizer.zero_grad()和model.zero_grad()效果一样。 PyTorch性能与调试 torch.utils.data.DataLoader中尽量设置pin_memory=True，对特别小的数据集如MNIST设置pin_memory=False反而更快一些。num_workers的设置需要在实验中找到最快的取值。 用del及时删除不用的中间变量，节约GPU存储。 使用inplace操作可节约GPU存储，如 x = torch.nn.functional.relu(x, inplace=True) 此外，还可以通过torch.utils.checkpoint前向传播时只保留一部分中间结果来节约GPU存储使用，在反向传播时需要的内容从最近中间结果中计算得到。 减少CPU和GPU之间的数据传输。例如如果你想知道一个epoch中每个mini-batch的loss和准确率，先将它们累积在GPU中等一个epoch结束之后一起传输回CPU会比每个mini-batch都进行一次GPU到CPU的传输更快。 使用半精度浮点数half()会有一定的速度提升，具体效率依赖于GPU型号。需要小心数值精度过低带来的稳定性问题。 时常使用assert tensor.size() == (N, D, H, W)作为调试手段，确保张量维度和你设想中一致。 除了标记y外，尽量少使用一维张量，使用n*1的二维张量代替，可以避免一些意想不到的一维张量计算结果。 统计代码各部分耗时 with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile: ... print(profile) 或者在命令行运行 python -m torch.utils.bottleneck main.py 致谢 感谢 些许流年 、 El tnoto 、 FlyCharles 的勘误，感谢 oatmeal 提供的更简洁的方法。由于作者才疏学浅，更兼时间和精力所限，代码中错误之处在所难免，敬请读者批评指正。 参考资料 PyTorch官方代码：pytorch/examples PyTorch论坛：PyTorch Forums PyTorch文档：http://pytorch.org/docs/stable/index.html 其他基于PyTorch的公开实现代码，无法一一列举 参考 ^T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models for fine-grained visual recognition. In ICCV, 2015. ^Y. Chen, Y. Bai, W. Zhang, and T. Mei. Destruction and construction learning for fine-grained image recognition. In CVPR, 2019. ^L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016. ^C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna: Rethinking the Inception architecture for computer vision. In CVPR, 2016. ^H. Zhang, M. Cissé, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. "},"code_technique/pytorch/pytorch2.html":{"url":"code_technique/pytorch/pytorch2.html","title":"pytorch常用代码段合集","keywords":"","body":"PyTorch常用代码段整理合集 HudsonEvangeline 发布于2小时前 阅读61次 5 人点赞 0 条评论 本文代码基于 PyTorch 1.0 版本，需要用到以下包 import collections import os import shutil import tqdm import numpy as np import PIL.Image import torch import torchvision 基础配置 检查 PyTorch 版本 torch.__version__ # PyTorch version torch.version.cuda # Corresponding CUDA version torch.backends.cudnn.version() # Corresponding cuDNN version torch.cuda.get_device_name(0) # GPU type 更新 PyTorch PyTorch 将被安装在 anaconda3/lib/python3.7/site-packages/torch/目录下。 conda update pytorch torchvision -c pytorch 固定随机种子 torch.manual_seed(0) torch.cuda.manual_seed_all(0) 指定程序运行在特定 GPU 卡上 在命令行指定环境变量 CUDA_VISIBLE_DEVICES=0,1 python train.py 或在代码中指定 os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' 判断是否有 CUDA 支持 torch.cuda.is_available() 设置为 cuDNN benchmark 模式 Benchmark 模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。 torch.backends.cudnn.benchmark = True 如果想要避免这种结果波动，设置 torch.backends.cudnn.deterministic = True 清除 GPU 存储 有时 Control-C 中止运行后 GPU 存储没有及时释放，需要手动清空。在 PyTorch 内部可以 torch.cuda.empty_cache() 或在命令行可以先使用 ps 找到程序的 PID，再使用 kill 结束该进程 ps aux | grep pythonkill -9 [pid] 或者直接重置没有被清空的 GPU nvidia-smi --gpu-reset -i [gpu_id] 张量处理 张量基本信息 tensor.type() # Data type tensor.size() # Shape of the tensor. It is a subclass of Python tuple tensor.dim() # Number of dimensions. 数据类型转换 # Set default tensor type. Float in PyTorch is much faster than double. torch.set_default_tensor_type(torch.FloatTensor) # Type convertions. tensor = tensor.cuda() tensor = tensor.cpu() tensor = tensor.float() tensor = tensor.long() torch.Tensor 与 np.ndarray 转换 # torch.Tensor -> np.ndarray. ndarray = tensor.cpu().numpy() # np.ndarray -> torch.Tensor. tensor = torch.from_numpy(ndarray).float() tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride torch.Tensor 与 PIL.Image 转换 PyTorch 中的张量默认采用 N×D×H×W 的顺序，并且数据范围在 [0, 1]，需要进行转置和规范化。 # torch.Tensor -> PIL.Image. image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255 ).byte().permute(1, 2, 0).cpu().numpy()) image = torchvision.transforms.functional.to_pil_image(tensor) # Equivalently way # PIL.Image -> torch.Tensor. tensor = torch.from_numpy(np.asarray(PIL.Image.open(path)) ).permute(2, 0, 1).float() / 255 tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way np.ndarray 与 PIL.Image 转换 # np.ndarray -> PIL.Image. image = PIL.Image.fromarray(ndarray.astypde(np.uint8)) # PIL.Image -> np.ndarray. ndarray = np.asarray(PIL.Image.open(path)) 从只包含一个元素的张量中提取值 这在训练时统计 loss 的变化过程中特别有用。否则这将累积计算图，使 GPU 存储占用量越来越大。 value = tensor.item() 张量形变 张量形变常常需要用于将卷积层特征输入全连接层的情形。相比 torch.view，torch.reshape 可以自动处理输入张量不连续的情况。 tensor = torch.reshape(tensor, shape) 打乱顺序 tensor = tensor[torch.randperm(tensor.size(0))] # Shuffle the first dimension 水平翻转 PyTorch 不支持 tensor[::-1] 这样的负步长操作，水平翻转可以用张量索引实现。 # Assume tensor has shape N*D*H*W.tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()] 复制张量 有三种复制的方式，对应不同的需求。 # Operation | New/Shared memory | Still in computation graph | tensor.clone() # | New | Yes | tensor.detach() # | Shared | No | tensor.detach.clone()() # | New | No | 拼接张量 注意 torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，而 torch.stack 会新增一维。例如当参数是 3 个 10×5 的张量，torch.cat 的结果是 30×5 的张量，而 torch.stack 的结果是 3×10×5 的张量。 tensor = torch.cat(list_of_tensors, dim=0) tensor = torch.stack(list_of_tensors, dim=0) 将整数标记转换成独热（one-hot）编码 PyTorch 中的标记默认从 0 开始。 N = tensor.size(0) one_hot = torch.zeros(N, num_classes).long() one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long()) 得到非零/零元素 torch.nonzero(tensor) # Index of non-zero elements torch.nonzero(tensor == 0) # Index of zero elements torch.nonzero(tensor).size(0) # Number of non-zero elements torch.nonzero(tensor == 0).size(0) # Number of zero elements 张量扩展 # Expand tensor of shape 64*512 to shape 64*512*7*7. torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7) 矩阵乘法 # Matrix multiplication: (m*n) * (n*p) -> (m*p). result = torch.mm(tensor1, tensor2) # Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p). result = torch.bmm(tensor1, tensor2) # Element-wise multiplication. result = tensor1 * tensor2 计算两组数据之间的两两欧式距离 # X1 is of shape m*d. X1 = torch.unsqueeze(X1, dim=1).expand(m, n, d) # X2 is of shape n*d. X2 = torch.unsqueeze(X2, dim=0).expand(m, n, d) # dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2) dist = torch.sqrt(torch.sum((X1 - X2) ** 2, dim=2)) 模型定义 卷积层 最常用的卷积层配置是 conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True) 如果卷积层配置比较复杂，不方便计算输出大小时，可以利用如下可视化工具辅助 链接：https://ezyang.github.io/convolution-visualizer/index.html 0GAP（Global average pooling）层 gap = torch.nn.AdaptiveAvgPool2d(output_size=1) 双线性汇合（bilinear pooling） X = torch.reshape(N, D, H * W) # Assume X has shape N*D*H*W X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W) # Bilinear pooling assert X.size() == (N, D, D) X = torch.reshape(X, (N, D * D)) X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5) # Signed-sqrt normalization X = torch.nn.functional.normalize(X) # L2 normalization 多卡同步 BN（Batch normalization） 当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。 链接：https://github.com/vacancy/Synchronized-BatchNorm-PyTorch 类似 BN 滑动平均 如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。 class BN(torch.nn.Module) def __init__(self): ... self.register_buffer('running_mean', torch.zeros(num_features)) def forward(self, X): ... self.running_mean += momentum * (current - self.running_mean) 计算模型整体参数量 num_parameters = sum(torch.numel(parameter) for parameter in model.parameters()) 类似 Keras 的 model.summary() 输出模型信息 链接：https://github.com/sksq96/pytorch-summary 模型权值初始化 注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。 # Common practise for initialization. for layer in model.modules(): if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.BatchNorm2d): torch.nn.init.constant_(layer.weight, val=1.0) torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.Linear): torch.nn.init.xavier_normal_(layer.weight) if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) # Initialization with given tensor. layer.weight = torch.nn.Parameter(tensor) 部分层使用预训练模型 注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是 model.load_state_dict(torch.load('model,pth'), strict=False) 将在 GPU 保存的模型加载到 CPU model.load_state_dict(torch.load('model,pth', map_location='cpu')) 数据准备、特征提取与微调 得到视频数据基本信息 import cv2 video = cv2.VideoCapture(mp4_path) height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)) num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) fps = int(video.get(cv2.CAP_PROP_FPS)) video.release() TSN 每段（segment）采样一帧视频 K = self._num_segments if is_train: if num_frames > K: # Random index for each segment. frame_indices = torch.randint( high=num_frames // K, size=(K,), dtype=torch.long) frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.randint( high=num_frames, size=(K - num_frames,), dtype=torch.long) frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), frame_indices)))[0] else: if num_frames > K: # Middle index for each segment. frame_indices = num_frames / K // 2 frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), torch.arange(K - num_frames))))[0] assert frame_indices.size() == (K,) return [frame_indices[i] for i in range(K)] 提取 ImageNet 预训练模型某层的卷积特征 # VGG-16 relu5-3 feature. model = torchvision.models.vgg16(pretrained=True).features[:-1] # VGG-16 pool5 feature. model = torchvision.models.vgg16(pretrained=True).features # VGG-16 fc7 feature. model = torchvision.models.vgg16(pretrained=True) model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3]) # ResNet GAP feature. model = torchvision.models.resnet18(pretrained=True) model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) with torch.no_grad(): model.eval() conv_representation = model(image) 提取 ImageNet 预训练模型多层的卷积特征 class FeatureExtractor(torch.nn.Module): \"\"\"Helper class to extract several convolution features from the given pre-trained model. Attributes: _model, torch.nn.Module. _layers_to_extract, list or set Example: >>> model = torchvision.models.resnet152(pretrained=True) >>> model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) >>> conv_representation = FeatureExtractor( pretrained_model=model, layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image) \"\"\" def __init__(self, pretrained_model, layers_to_extract): torch.nn.Module.__init__(self) self._model = pretrained_model self._model.eval() self._layers_to_extract = set(layers_to_extract) def forward(self, x): with torch.no_grad(): conv_representation = [] for name, layer in self._model.named_children(): x = layer(x) if name in self._layers_to_extract: conv_representation.append(x) return conv_representation 其他预训练模型 链接：https://github.com/Cadene/pretrained-models.pytorch 微调全连接层 model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False model.fc = nn.Linear(512, 100) # Replace the last fc layer optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4) 以较大学习率微调全连接层，较小学习率微调卷积层 model = torchvision.models.resnet18(pretrained=True) finetuned_parameters = list(map(id, model.fc.parameters())) conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters) parameters = [{'params': conv_parameters, 'lr': 1e-3}, {'params': model.fc.parameters()}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) 模型训练 常用训练和验证数据预处理 其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。 train_transform = torchvision.transforms.Compose([ torchvision.transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0)), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) val_transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(224), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) 训练基本代码框架 for t in epoch(80): for images, labels in tqdm.tqdm(train_loader, desc='Epoch %3d' % (t + 1)): images, labels = images.cuda(), labels.cuda() scores = model(images) loss = loss_function(scores, labels) optimizer.zero_grad() loss.backward() optimizer.step() 标记平滑（label smoothing） for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() N = labels.size(0) # C is the number of classes. smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda() smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9) score = model(images) log_prob = torch.nn.functional.log_softmax(score, dim=1) loss = -torch.sum(log_prob * smoothed_labels) / N optimizer.zero_grad() loss.backward() optimizer.step() Mixup beta_distribution = torch.distributions.beta.Beta(alpha, alpha) for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() # Mixup images. lambda_ = beta_distribution.sample([]).item() index = torch.randperm(images.size(0)).cuda() mixed_images = lambda_ * images + (1 - lambda_) * images[index, :] # Mixup loss. scores = model(mixed_images) loss = (lambda_ * loss_function(scores, labels) + (1 - lambda_) * loss_function(scores, labels[index])) optimizer.zero_grad() loss.backward() optimizer.step() L1 正则化 l1_regularization = torch.nn.L1Loss(reduction='sum') loss = ... # Standard cross-entropy loss for param in model.parameters(): loss += torch.sum(torch.abs(param)) loss.backward() 不对偏置项进行 L2 正则化/权值衰减（weight decay） bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias') others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias') parameters = [{'parameters': bias_list, 'weight_decay': 0}, {'parameters': others_list}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) 梯度裁剪（gradient clipping） torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20) 计算 Softmax 输出的准确率 score = model(images) prediction = torch.argmax(score, dim=1) num_correct = torch.sum(prediction == labels).item() accuruacy = num_correct / labels.size(0) 可视化模型前馈的计算图 链接：https://github.com/szagoruyko/pytorchviz 可视化学习曲线 有 Facebook 自己开发的 Visdom 和 Tensorboard 两个选择。 https://github.com/facebookresearch/visdom https://github.com/lanpa/tensorboardX # Example using Visdom. vis = visdom.Visdom(env='Learning curve', use_incoming_socket=False) assert self._visdom.check_connection() self._visdom.close() options = collections.namedtuple('Options', ['loss', 'acc', 'lr'])( loss={'xlabel': 'Epoch', 'ylabel': 'Loss', 'showlegend': True}, acc={'xlabel': 'Epoch', 'ylabel': 'Accuracy', 'showlegend': True}, lr={'xlabel': 'Epoch', 'ylabel': 'Learning rate', 'showlegend': True}) for t in epoch(80): tran(...) val(...) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]), name='train', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]), name='val', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]), name='train', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]), name='val', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]), win='Learning rate', update='append', opts=options.lr) 得到当前学习率 # If there is one global learning rate (which is the common case). lr = next(iter(optimizer.param_groups))['lr'] # If there are multiple learning rates for different layers. all_lr = [] for param_group in optimizer.param_groups: all_lr.append(param_group['lr']) 学习率衰减 # Reduce learning rate when validation accuarcy plateau. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True) for t in range(0, 80): train(...); val(...) scheduler.step(val_acc) # Cosine annealing learning rate. scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80) # Reduce learning rate by 10 at given epochs. scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1) for t in range(0, 80): scheduler.step() train(...); val(...) # Learning rate warmup by 10 epochs. scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10) for t in range(0, 10): scheduler.step() train(...); val(...) 保存与加载断点 注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。 # Save checkpoint. is_best = current_acc > best_acc best_acc = max(best_acc, current_acc) checkpoint = { 'best_acc': best_acc, 'epoch': t + 1, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), } model_path = os.path.join('model', 'checkpoint.pth.tar') torch.save(checkpoint, model_path) if is_best: shutil.copy('checkpoint.pth.tar', model_path) # Load checkpoint. if resume: model_path = os.path.join('model', 'checkpoint.pth.tar') assert os.path.isfile(model_path) checkpoint = torch.load(model_path) best_acc = checkpoint['best_acc'] start_epoch = checkpoint['epoch'] model.load_state_dict(checkpoint['model']) optimizer.load_state_dict(checkpoint['optimizer']) print('Load checkpoint at epoch %d.' % start_epoch) 计算准确率、查准率（precision）、查全率（recall） # data['label'] and data['prediction'] are groundtruth label and prediction # for each image, respectively. accuracy = np.mean(data['label'] == data['prediction']) * 100 # Compute recision and recall for each class. for c in range(len(num_classes)): tp = np.dot((data['label'] == c).astype(int), (data['prediction'] == c).astype(int)) tp_fp = np.sum(data['prediction'] == c) tp_fn = np.sum(data['label'] == c) precision = tp / tp_fp * 100 recall = tp / tp_fn * 100 PyTorch 其他注意事项 模型定义 建议有参数的层和汇合（pooling）层使用 torch.nn 模块定义，激活函数直接使用 torch.nn.functional。torch.nn 模块和 torch.nn.functional 的区别在于，torch.nn 模块在计算时底层调用了 torch.nn.functional，但 torch.nn 模块包括该层参数，还可以应对训练和测试两种网络状态。使用 torch.nn.functional 时要注意网络状态，如 def forward(self, x): ... x = torch.nn.functional.dropout(x, p=0.5, training=self.training) model(x) 前用 model.train() 和 model.eval() 切换网络状态。 不需要计算梯度的代码块用 with torch.no_grad() 包含起来。model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和随机失活（dropout）在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。 torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。 loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。optimizer.zero_grad() 和 model.zero_grad() 效果一样。 PyTorch 性能与调试 torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。 用 del 及时删除不用的中间变量，节约 GPU 存储。 使用 inplace 操作可节约 GPU 存储，如 x = torch.nn.functional.relu(x, inplace=True) 减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。 使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。 时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。 除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。 统计代码各部分耗时 with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile: ... print(profile) 或者在命令行运行 python -m torch.utils.bottleneck main.py 致谢 感谢 @些许流年和@El tnoto的勘误。由于作者才疏学浅，更兼时间和精力所限，代码中错误之处在所难免，敬请读者批评指正。 参考资料 PyTorch 官方代码：pytorch/examples (https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples) PyTorch 论坛：PyTorch Forums (https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/latest%3Forder%3Dviews) PyTorch 文档：http://pytorch.org/docs/stable/index.html (https://link.zhihu.com/?target=http%3A//pytorch.org/docs/stable/index.html) 其他基于 PyTorch 的公开实现代码，无法一一列举 张皓：南京大学计算机系机器学习与数据挖掘所（LAMDA）硕士生，研究方向为计算机视觉和机器学习，特别是视觉识别和深度学习。个人主页：http://lamda.nju.edu.cn/zhangh/ 原知乎链接： https://zhuanlan.zhihu.com/p/59205847? 查看原文： 点赞收藏：PyTorch常用代码段整理合集 "},"code_technique/pytorch/pytorch_train.html":{"url":"code_technique/pytorch/pytorch_train.html","title":"pytorch训练技巧","keywords":"","body":"Pytorch训练技巧 [TOC] 1、指定GPU编号 设置当前使用的GPU设备仅为0号设备，设备名称为 /gpu:0：os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" 设置当前使用的GPU设备为0,1号两个设备，名称依次为 /gpu:0、/gpu:1： os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" ，根据顺序表示优先使用0号设备,然后使用1号设备。 指定GPU的命令需要放在和神经网络相关的一系列操作的前面。 2、查看模型每层输出详情 Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。 使用很简单，如下用法： from torchsummary import summary summary(your_model, input_size=(channels, H, W)) input_size 是根据你自己的网络模型的输入尺寸进行设置。 pytorch-summarygithub.com 3、梯度裁剪（Gradient Clipping） import torch.nn as nn outputs = model(data) loss= loss_fn(outputs, target) optimizer.zero_grad() loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) optimizer.step() nn.utils.clip_grad_norm_ 的参数： parameters – 一个基于变量的迭代器，会进行梯度归一化 max_norm – 梯度的最大范数 norm_type – 规定范数的类型，默认为L2 4、扩展单张图片维度 因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法： import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.view(1, *image.size()) print(img.size()) # output: # torch.Size([h, w, c]) # torch.Size([1, h, w, c]) 或 import cv2 import numpy as np image = cv2.imread(img_path) print(image.shape) img = image[np.newaxis, :, :, :] print(img.shape) # output: # (h, w, c) # (1, h, w, c) 或（感谢知乎用户coldleaf的补充） import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.unsqueeze(dim=0) print(img.size()) img = img.squeeze(dim=0) print(img.size()) # output: # torch.Size([(h, w, c)]) # torch.Size([1, h, w, c]) # torch.Size([h, w, c]) tensor.unsqueeze(dim)：扩展维度，dim指定扩展哪个维度。 tensor.squeeze(dim)：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。 5、独热编码 在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。 import torch class_num = 8 batch_size = 4 def one_hot(label): \"\"\" 将一维列表转换为独热编码 \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # 从 value 中取值，然后根据 dim 和 index 给相应位置赋值 onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor -> Numpy label = torch.LongTensor(batch_size).random_() % class_num # 对随机数取余 print(one_hot(label)) # output: [[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] Convert int into one-hot formatdiscuss.pytorch.org 6、防止验证模型时爆显存 验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。 with torch.no_grad(): # 使用model进行预测的代码 pass Pytorch 训练时无用的临时变量可能会越来越多，导致 out of memory ，可以使用下面语句来清理这些不需要的变量。 torch.cuda.empty_cache() 更详细的优化可以查看 优化显存使用 和 显存利用问题。 7、学习率衰减 import torch.optim as optim from torch.optim import lr_scheduler # 训练前的初始化 optimizer = optim.Adam(net.parameters(), lr=0.001) scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # 每过10个epoch，学习率乘以0.1 # 训练过程中 for n in n_epoch: scheduler.step() ... 8、冻结某些层的参数 参考：Pytorch 冻结预训练模型的某一层 在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。 我们需要先知道每一层的名字，通过如下代码打印： net = Network() # 获取自定义网络结构 for name, value in net.named_parameters(): print('name: {0},\\t grad: {1}'.format(name, value.requires_grad)) 假设前几层信息如下： name: cnn.VGG_16.convolution1_1.weight, grad: True name: cnn.VGG_16.convolution1_1.bias, grad: True name: cnn.VGG_16.convolution1_2.weight, grad: True name: cnn.VGG_16.convolution1_2.bias, grad: True name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True 后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表： no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias' ] 冻结方法如下： net = Net.CTPN() # 获取网络结构 for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True 冻结后我们再打印每层的信息： name: cnn.VGG_16.convolution1_1.weight, grad: False name: cnn.VGG_16.convolution1_1.bias, grad: False name: cnn.VGG_16.convolution1_2.weight, grad: False name: cnn.VGG_16.convolution1_2.bias, grad: False name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True 可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。 最后在定义优化器时，只对requires_grad为True的层的参数进行更新。 optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) "},"code_technique/pytorch/pytorch_.html":{"url":"code_technique/pytorch/pytorch_.html","title":"pytorch解冻","keywords":"","body":"作者：ycszen 链接：https://zhuanlan.zhihu.com/p/25980324 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 加载部分预训练模型 其实大多数时候我们需要根据我们的任务调节我们的模型，所以很难保证模型和公开的模型完全一样，但是预训练模型的参数确实有助于提高训练的准确率，为了结合二者的优点，就需要我们加载部分预训练模型。 pretrained_dict = model_zoo.load_url(model_urls['resnet152']) model_dict = model.state_dict() # 将pretrained_dict里不属于model_dict的键剔除掉 pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict} # 更新现有的model_dict model_dict.update(pretrained_dict) # 加载我们真正需要的state_dict model.load_state_dict(model_dict) 因为需要剔除原模型中不匹配的键，也就是层的名字，所以我们的新模型改变了的层需要和原模型对应层的名字不一样，比如：resnet最后一层的名字是fc(PyTorch中)，那么我们修改过的resnet的最后一层就不能取这个名字，可以叫fc_ 微改基础模型 PyTorch中的torchvision里已经有很多常用的模型了，可以直接调用： AlexNet VGG ResNet SqueezeNet DenseNet import torchvision.models as models resnet18 = models.resnet18() alexnet = models.alexnet() squeezenet = models.squeezenet1_0() densenet = models.densenet_161() 但是对于我们的任务而言有些层并不是直接能用，需要我们微微改一下，比如，resnet最后的全连接层是分1000类，而我们只有21类；又比如，resnet第一层卷积接收的通道是3， 我们可能输入图片的通道是4，那么可以通过以下方法修改： resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False) resnet.fc = nn.Linear(2048, 21) "},"code_technique/pytorch/pytorch_vision.html":{"url":"code_technique/pytorch/pytorch_vision.html","title":"pytorch网络可视化","keywords":"","body":"Pytorch网络结构可视化 安装 可以通过以下的命令进行安装 conda install pytorch-nightly -c pytorch conda install graphviz conda install torchvision conda install tensorwatch 本教程基于以下的版本： torchvision.__version__ '0.2.1' torch.__version__ '1.2.0.dev20190610' sys.version '3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]' 载入库 import sys import torch import tensorwatch as tw import torchvision.models 网络结构可视化 alexnet_model = torchvision.models.alexnet() tw.draw_model(alexnet_model, [1, 3, 224, 224]) 载入alexnet，draw_model函数需要传入三个参数，第一个为model，第二个参数为input_shape，第三个参数为orientation，可以选择'LR'或者'TB'，分别代表左右布局与上下布局。 在notebook中，执行完上面的代码会显示如下的图，将网络的结构及各个层的name和shape进行了可视化。 统计网络参数 可以通过model_stats方法统计各层的参数情况。 tw.model_stats(alexnet_model, [1, 3, 224, 224]) [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! alexnet_model.features Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) alexnet_model.classifier Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) 参考 https://github.com/microsoft/te "},"code_technique/pytorch/PSNR_SSIM.html":{"url":"code_technique/pytorch/PSNR_SSIM.html","title":"PSNR&&SSIM","keywords":"","body":" import torch import torch.nn.functional as F from math import exp import numpy as np ​ # 计算一维的高斯分布向量 def gaussian(window_size, sigma): gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)]) return gauss/gauss.sum() ​ # 创建高斯核，通过两个一维高斯分布向量进行矩阵乘法得到 # 可以设定channel参数拓展为3通道 def create_window(window_size, channel=1): _1D_window = gaussian(window_size, 1.5).unsqueeze(1) _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0) window = _2D_window.expand(channel, 1, window_size, window_size).contiguous() return window ​ # 计算SSIM # 直接使用SSIM的公式，但是在计算均值时，不是直接求像素平均值，而是采用归一化的高斯核卷积来代替。 # 在计算方差和协方差时用到了公式Var(X)=E[X^2]-E[X]^2, cov(X,Y)=E[XY]-E[X]E[Y]. # 正如前面提到的，上面求期望的操作采用高斯核卷积代替。 def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None): # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh). if val_range is None: if torch.max(img1) > 128: max_val = 255 else: max_val = 1 if torch.min(img1) ​​ # Classes to re-use window class SSIM(torch.nn.Module): def __init__(self, window_size=11, size_average=True, val_range=None): super(SSIM, self).__init__() self.window_size = window_size self.size_average = size_average self.val_range = val_range # Assume 1 channel for SSIM self.channel = 1 self.window = create_window(window_size) def forward(self, img1, img2): (_, channel, _, _) = img1.size() if channel == self.channel and self.window.dtype == img1.dtype: window = self.window else: window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype) self.window = window self.channel = channel return ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average) 我写好的文件夹下名字对其 PSNR/SSIM ''' calculate the PSNR and SSIM. same as MATLAB's results ''' import os import math import numpy as np import cv2 import glob import sys def main(): # Configurations # GT - Ground-truth; # Gen: Generated / Restored / Recovered images # folder_GT = 'DMDM40256no_gen' # folder_GT = 'testDNDN256no_gen' folder_GT = 'testDWNW_199_256no_gen' folder_Gen = 'testB' crop_border = 4 suffix = '/' # suffix for Gen images test_Y = False # True: test Y channel only; False: test RGB channels PSNR_all = [] SSIM_all = [] img_list = sorted(glob.glob(folder_GT + '/*')) if test_Y: print('Testing Y channel.') else: print('Testing RGB channels.') print(len(img_list)) # fo = open(\"DN-gt.txt\", \"w\") fo = open(\"DW-gt.txt\", \"w\") # fo = open(\"DM-gt.txt\", \"w\") for i, img_path in enumerate(img_list): base_name = os.path.splitext(os.path.basename(img_path))[0] # print(base_name,img_path,folder_Gen,suffix) im_GT = cv2.imread(img_path) # print(type(im_GT)) im_GT = im_GT / 255 im_Gen = cv2.imread(os.path.join(folder_Gen, base_name + '.jpg')) # print(type(im_Gen),os.path.join(folder_Gen, base_name + '.jpg')) im_Gen = im_Gen / 255 if test_Y and im_GT.shape[2] == 3: # evaluate on Y channel in YCbCr color space im_GT_in = bgr2ycbcr(im_GT) im_Gen_in = bgr2ycbcr(im_Gen) else: im_GT_in = im_GT im_Gen_in = im_Gen # crop borders if im_GT_in.ndim == 3: cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border, :] cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border, :] elif im_GT_in.ndim == 2: cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border] cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border] else: raise ValueError('Wrong image dimension: {}. Should be 2 or 3.'.format(im_GT_in.ndim)) # calculate PSNR and SSIM PSNR = calculate_psnr(cropped_GT * 255, cropped_Gen * 255) SSIM = calculate_ssim(cropped_GT * 255, cropped_Gen * 255) # print('{:3d} - {:25}. \\tPSNR: {:.6f} dB, \\tSSIM: {:.6f}'.format(i + 1, base_name, PSNR, SSIM)) # print (\"文件名: \", fo.name) fo.write('{:3d} - {:25}. \\tPSNR: {:.6f} dB, \\tSSIM: {:.6f}\\n'.format( i + 1, base_name, PSNR, SSIM)) # print(i,\"/\",len(img_list),i/len(img_list),\"%\") print(i) PSNR_all.append(PSNR) SSIM_all.append(SSIM) # print('Average: PSNR: {:.6f} dB, SSIM: {:.6f}'.format(sum(PSNR_all) / len(PSNR_all),sum(SSIM_all) / len(SSIM_all))) fo.write('Average: PSNR: {:.6f} dB, SSIM: {:.6f}\\n'.format( sum(PSNR_all) / len(PSNR_all), sum(SSIM_all) / len(SSIM_all))) # 关闭打开的文件 fo.close() def calculate_psnr(img1, img2): # img1 and img2 have range [0, 255] img1 = img1.astype(np.float64) img2 = img2.astype(np.float64) mse = np.mean((img1 - img2)**2) if mse == 0: return float('inf') return 20 * math.log10(255.0 / math.sqrt(mse)) def ssim(img1, img2): C1 = (0.01 * 255)**2 C2 = (0.03 * 255)**2 img1 = img1.astype(np.float64) img2 = img2.astype(np.float64) kernel = cv2.getGaussianKernel(11, 1.5) window = np.outer(kernel, kernel.transpose()) mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5] # valid mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5] mu1_sq = mu1**2 mu2_sq = mu2**2 mu1_mu2 = mu1 * mu2 sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2 ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)) return ssim_map.mean() def calculate_ssim(img1, img2): '''calculate SSIM the same outputs as MATLAB's img1, img2: [0, 255] ''' if not img1.shape == img2.shape: raise ValueError('Input images must have the same dimensions.') if img1.ndim == 2: return ssim(img1, img2) elif img1.ndim == 3: if img1.shape[2] == 3: ssims = [] for i in range(3): ssims.append(ssim(img1, img2)) return np.array(ssims).mean() elif img1.shape[2] == 1: return ssim(np.squeeze(img1), np.squeeze(img2)) else: raise ValueError('Wrong input image dimensions.') def bgr2ycbcr(img, only_y=True): '''same as matlab rgb2ycbcr only_y: only return Y channel Input: uint8, [0, 255] float, [0, 1] ''' in_img_type = img.dtype img.astype(np.float32) if in_img_type != np.uint8: img *= 255. # convert if only_y: rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0 else: rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128] if in_img_type == np.uint8: rlt = rlt.round() else: rlt /= 255. return rlt.astype(in_img_type) if __name__ == '__main__': main() ß #PSNR per_image_mse_loss = F.mse_loss (gen_hr,imgs_hr, reduction='none') per_image_psnr = 10 * torch.log10 (10 / per_image_mse_loss) tensor_average_psnr = torch.mean (per_image_psnr).item () #SSIM import pytorch_ssim import torch from torch.autograd import Variable img1 = Variable(torch.rand(1, 1, 256, 256)) img2 = Variable(torch.rand(1, 1, 256, 256)) if torch.cuda.is_available(): img1 = img1.cuda() img2 = img2.cuda() print(pytorch_ssim.ssim(img1, img2)) ssim_loss = pytorch_ssim.SSIM(window_size = 11) print(ssim_loss(img1, img2)) #MSSSIM import pytorch_ssim import torch from torch.autograd import Variable device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') m = pytorch_msssim.MSSSIM() img1 = torch.rand(1, 1, 256, 256) img2 = torch.rand(1, 1, 256, 256) print(pytorch_msssim.msssim(img1, img2)) print(m(img1, img2))) "},"code_technique/pytorch/SPP.html":{"url":"code_technique/pytorch/SPP.html","title":"SPP","keywords":"","body":"使用 Spatial Pyramid Pooling 让 CNN 接受可变尺寸的图像 https://oidiotlin.com/sppnet-tutorial/ 目录 传统 CNN 的弊端 SPP-Net 概述 SPP-Net 结构细节 SPP-Net 训练方法 在 pytorch 框架中实现 SPP 参考论文：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 传统 CNN 的弊端 在传统 CNN 中，由于 Fully-Connected 层的存在，输入图像的尺寸受到了严格限制。通常情况下，我们需要对原始图片进行裁剪（crop）或变形（warp）的操作来调整其尺寸使其适配于 CNN。然而裁剪过的图片可能包含不了所需的所有信息，而改变纵横比的变形操作也可能会使关键部分产生非期望的形变。由于图片内容的丢失或失真，模型的准确度会受到很大的影响。 上图中分别表现了两种 resize 的方法：裁剪（左）、变形（右）。它们对原图都造成了非期望的影响。 SPP-Net 概述 从 CNN 的结构来看，我们需要让图像在进入 FC 层前就将尺度固定到指定大小。通过修改卷积层或池化层参数可以改变图片大小，其中池化层不具有学习功能，其参数不会随着训练过程变化，自然而然承担起改变 spatial size 的工作。我们在第一个 FC 层前加入一个特殊的池化层，其参数是随着输入大小而成比例变化的。 图1. 使用 crop 或 warp 方法的 CNN 的层级结构。图2. 在卷积层与第一个全连接层之间加入 SPP 层。 SPP-Net 中有若干个并行的池化层，将卷积层的结果 [w×h×d][w×h×d] 池化成 [1×1],[2×2],[4×4],⋯[1×1],[2×2],[4×4],⋯ 的一层层结果，再将其所有结果与 FC 层相连。 当输入为任意大小的图片时，我们可以随意进行卷积、池化，在 FC 层之前，通过 SPP 层，将图片抽象出固定大小的特征（即多尺度特征下的固定特征向量抽取）。 SPP-Net 结构细节 结构如上所示，已知输入 conv5 的大小是 [w×h×d][w×h×d]，SPP 中某一层输出结果大小为 [n×n×d][n×n×d]，那么如何设定该层的参数呢？ 感受野大小 [wr×hr][wr×hr]：wr=⌈wn⌉wr=⌈wn⌉，hr=⌈hn⌉hr=⌈hn⌉ 步长 (sw,sh)(sw,sh)：sw=⌊wn⌋sw=⌊wn⌋，sh=⌊hn⌋sh=⌊hn⌋ 假设输入是 [30×42×256][30×42×256]，对于 SPP 中 [4×4][4×4] 的层而言，其： 感受野大小应为 [⌈304⌉×⌈424⌉]=[8×11][⌈304⌉×⌈424⌉]=[8×11] 步长应为 (⌊304⌋,⌊424⌋)=(7,10)(⌊304⌋,⌊424⌋)=(7,10) 最后再将 SPP 中所有层的池化结果（池化操作通常是取感受野内的 max）变成 1 维向量，并与 FC 层中的神经元连接。 如上图中的 SPP 有三层([1×1],[2×2],[4×4][1×1],[2×2],[4×4])，则通过 SPP 后的特征有 (1+4+16)×256(1+4+16)×256 个。 SPP-Net 训练方法 虽然使用了 SPP，理论上可以直接用变尺度的图像集作为输入进行训练，但是常用的一些框架（如 CUDA-convnet、Caffe等）在底层实现中更适合固定尺寸的计算（效率更高）。原论文中提及了两种训练方法： Single-Size：将所有的图片固定到同一尺度。 Multi-Size：将原图片通过 crop 得到某一尺度 A，再把 A 通过 warp 放缩成更小的尺寸 B。之后用 A 尺度训练一个 epoch，再用 B 尺度训练一个 epoch，交替迭代。 由何凯明等人的实验结果可以发现，采用 Multi-Size 方法训练得到的模型错误率更低，且收敛速度更快。 在 pytorch 框架中实现 SPP class SpatialPyramidPool2D(nn.Module): \"\"\" Args: out_side (tuple): Length of side in the pooling results of each pyramid layer. Inputs: - `input`: the input Tensor to invert ([batch, channel, width, height]) \"\"\" def __init__(self, out_side): super(SpatialPyramidPool2D, self).__init__() self.out_side = out_side def forward(self, x): out = None for n in self.out_side: w_r, h_r = map(lambda s: math.ceil(s/n), x.size()[2:]) # Receptive Field Size s_w, s_h = map(lambda s: math.floor(s/n), x.size()[2:]) # Stride max_pool = nn.MaxPool2d(kernel_size=(w_r, h_r), stride=(s_w, s_h)) y = max_pool(x) if out is None: out = y.view(y.size()[0], -1) else: out = torch.cat((out, y.view(y.size()[0], -1)), 1) return out 可以在模型中插入该模块，如： nn.Sequential( nn.Conv2d( in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2, ), nn.ReLU(), SpatialPyramidPool2D(out_side=(1,2,4)) ) 在 pytorch 中建立自己的图片数据集 目录 图片文件在同一目录下 图片文件在不同目录下 通常情况下，待处理的图片数据有两种存放方式： 所有图片在同一目录下，另有一份文本文件记录了标签。 不同标签的图片放在不同目录下，文件夹名就是标签。 对于这两种情况，我们有不同的解决方法。 图片文件在同一目录下 假设在 ./data/ 目录下有所需的所有的图片，以及一份标记了图片标签的文本文件（列为图片路径+标签）./labels.txt ./data/IZvVCYcuOkcu6Ufj.jpg 0 ./data/2wuPp4yYoc2wJbZI.jpg 0 ./data/vzlBbG4Z1KKJ4P6L.jpg 1 ./data/nR8VZBPbjF92wNGC.jpg 2 ...... 思路是继承 torch.utils.data.Dataset，并重点重写其 __getitem__ 方法，示例代码如下： class CustomDataset(Dataset): def __init__(self, label_file_path): with open(label_file_path, 'r') as f: # (image_path(str), image_label(str)) self.imgs = list(map(lambda line: line.strip().split(' '), f)) def __getitem__(self, index): path, label = self.imgs[index] img = transforms.Compose([transforms.Scale(224), transforms.CenterCrop(224), transforms.ToTensor(),])(Image.open(path).convert('RGB')) label = int(label) return img, label def __len__(self): return len(self.imgs) dataset = CustomDataset('./labels.txt') loader = DataLoader(dataset, batch_size=64, shuffle=True) 至此，可以用 enumerate(loader) 的方式迭代数据了。需要注意的是，在 __getitem__时要确保 batch 内图片尺寸相同（上面的例子用了 Scale+CenterCrop 的方法），否则会出现 RuntimeError: inconsistent tensor sizes at ... 的错误。 图片文件在不同目录下 当图片文件依据 label 处于不同文件下时，如： ─── data ├── 虾饺 │ ├── 00856315f0df13536183d8ae6cbaf8d6a54f37ce.jpg │ └── 00ce9dccdf9a218d3b891e006c81f8e66524b1b3.jpg ├── 八宝粥 │ ├── 055133235f649411e599ce5dba83627d58996209.jpg │ └── 0a72473884cb6c03191ca929a9aa0b2bbe4abb3d.jpg └── 钵仔糕 ├── 1237b1e7b7e7da0ac78f9e1c8317b9462fe92803.jpg └── 14a7d6c1a881d1dcfe855bf783064ad2c9d5aba4.jpg 此时我们可以利用 torchvision.datasets.ImageFolder 来直接构造出 dataset，代码如下： dataset = ImageFolder(path) loader = DataLoader(dataset) ImageFolder 会将目录中的文件夹名自动转化成序列，那么 loader 载入时，标签就是整数序列了。 作者：龙鹏-言有三 链接：https://zhuanlan.zhihu.com/p/39455807 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 下面一个一个解释，完整代码请移步 Git 工程。 （1）datasets.ImageFolder Pytorch 的 torchvision 模块中提供了一个 dataset 包，它包含了一些基本的数据集如 mnist、coco、imagenet 和一个通用的数据加载器 ImageFolder。 它会以这样的形式组织数据，具体的请到 Git 工程中查看。 root/left/1.png root/left/2.png root/left/3.png root/right/1.png root/right/2.png root/right/3.png imagefolder 有3个成员变量。 self.classes：用一个 list 保存类名，就是文件夹的名字。 self.class_to_idx：类名对应的索引，可以理解为 0、1、2、3 等。 self.imgs：保存（imgpath，class），是图片和类别的数组。 不同文件夹下的图，会被当作不同的类，天生就用于图像分类任务。 （2）Transforms 这一点跟 Caffe 非常类似，就是定义了一系列数据集的预处理和增强操作。到此，数据接口就定义完毕了，接下来在训练代码中看如何使用迭代器进行数据读取就可以了，包括 scale、减均值等。 （3）torch.utils.data.DataLoader 这就是创建了一个 batch，生成真正网络的输入。关于更多 Pytorch 的数据读取方法，请移步知乎专栏和公众号，链接在前面的课程中有给出。 2.2 模型定义 如下： import torch import torch.nn as nn import torch.nn.functional as F import numpy as np class simpleconv3(nn.Module):` def __init__(self): super(simpleconv3,self).__init__() self.conv1 = nn.Conv2d(3, 12, 3, 2) self.bn1 = nn.BatchNorm2d(12) self.conv2 = nn.Conv2d(12, 24, 3, 2) self.bn2 = nn.BatchNorm2d(24) self.conv3 = nn.Conv2d(24, 48, 3, 2) self.bn3 = nn.BatchNorm2d(48) self.fc1 = nn.Linear(48 * 5 * 5 , 1200) self.fc2 = nn.Linear(1200 , 128) self.fc3 = nn.Linear(128 , 2) def forward(self , x): x = F.relu(self.bn1(self.conv1(x))) #print \"bn1 shape\",x.shape x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = x.view(-1 , 48 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 这三节课的任务，都是采用一个简单的 3 层卷积 + 2 层全连接层的网络结构。根据上面的网络结构的定义，需要做以下事情。 （1）simpleconv3(nn.Module) 继承 nn.Module，前面已经说过，Pytorch 的网络层是包含在 nn.Module 里，所以所有的网络定义，都需要继承该网络层。 并实现 super 方法，如下： super(simpleconv3,self).__init__() 这个，就当作一个标准，执行就可以了。 （2）网络结构的定义 都在 nn 包里，举例说明： torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 完整的接口如上，定义的第一个卷积层如下： nn.Conv2d(3, 12, 3, 2) 即输入通道为3，输出通道为12，卷积核大小为3，stride=2，其他的层就不一一介绍了，大家可以自己去看 nn 的 API。 （3）forward backward 方法不需要自己实现，但是 forward 函数是必须要自己实现的，从上面可以看出，forward 函数也是非常简单，串接各个网络层就可以了。 对比 Caffe 和 TensorFlow 可以看出，Pytorch 的网络定义更加简单，初始化方法都没有显示出现，因为 Pytorch 已经提供了默认初始化。 如果我们想实现自己的初始化，可以这么做： init.xavier_uniform(self.conv1.weight)init.constant(self.conv1.bias, 0.1) 它会对 conv1 的权重和偏置进行初始化。如果要对所有 conv 层使用 xavier 初始化呢？可以定义一个函数： def weights_init(m): if isinstance(m, nn.Conv2d): xavier(m.weight.data) xavier(m.bias.data) net = Net() net.apply(weights_init) 模型训练 网络定义和数据加载都定义好之后，就可以进行训练了，老规矩先上代码： def train_model(model, criterion, optimizer, scheduler, num_epochs=25): for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) else: model.train(False) running_loss = 0.0 running_corrects = 0.0 for data in dataloders[phase]: inputs, labels = data if use_gpu: inputs = Variable(inputs.cuda()) labels = Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) optimizer.zero_grad() outputs = model(inputs) _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) if phase == 'train': loss.backward() optimizer.step() running_loss += loss.data.item() running_corrects += torch.sum(preds == labels).item() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] if phase == 'train': writer.add_scalar('data/trainloss', epoch_loss, epoch) writer.add_scalar('data/trainacc', epoch_acc, epoch) else: writer.add_scalar('data/valloss', epoch_loss, epoch) writer.add_scalar('data/valacc', epoch_acc, epoch) print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) writer.export_scalars_to_json(\"./all_scalars.json\") writer.close() return model 分析一下上面的代码，外层循环是 epoches，然后利用 for data in dataloders[phase] 循环取一个 epoch 的数据，并塞入 variable，送入 model。需要注意的是，每一次 forward 要将梯度清零，即 optimizer.zero_grad()，因为梯度会记录前一次的状态，然后计算 loss，反向传播。 loss.backward() optimizer.step() 下面可以分别得到预测结果和 loss，每一次 epoch 完成计算。 epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) "},"code_technique/pytorch/Tensor_to_img_imge_to_tensor.html":{"url":"code_technique/pytorch/Tensor_to_img_imge_to_tensor.html","title":"Tensor to img && imge to tensor","keywords":"","body":"Tensor to img && imge to tensor 在pytorch中经常会遇到图像格式的转化，例如将PIL库读取出来的图片转化为Tensor，亦或者将Tensor转化为numpy格式的图片。而且使用不同图像处理库读取出来的图片格式也不相同，因此，如何在pytorch中正确转化各种图片格式(PIL、numpy、Tensor)是一个在调试中比较重要的问题。 本文主要说明在pytorch中如何正确将图片格式在各种图像库读取格式以及tensor向量之间转化的问题。以下代码经过测试都可以在Pytorch-0.4.0或0.3.0版本直接使用。 对python不同的图像库读取格式有疑问可以看这里：https://oldpan.me/archives/pytorch-transforms-opencv-scikit-image 格式转换 我们一般在pytorch或者python中处理的图像无非这几种格式： PIL：使用python自带图像处理库读取出来的图片格式 numpy：使用python-opencv库读取出来的图片格式 tensor：pytorch中训练时所采取的向量格式（当然也可以说图片） 注意，之后的讲解图片格式皆为RGB三通道，24-bit真彩色，也就是我们平常使用的图片形式。 PIL与Tensor PIL与Tensor的转换相对容易些，因为pytorch已经提供了相关的代码，我们只需要搭配使用即可： 所有代码都已经引用了（之后的代码省略引用部分）： import torch from PIL import Image import matplotlib.pyplot as plt # loader使用torchvision中自带的transforms函数 loader = transforms.Compose([ transforms.ToTensor()]) unloader = transforms.ToPILImage() 1 PIL读取图片转化为Tensor # 输入图片地址 # 返回tensor变量 def image_loader(image_name): image = Image.open(image_name).convert('RGB') image = loader(image).unsqueeze(0) return image.to(device, torch.float) 2 将PIL图片转化为Tensor # 输入PIL格式图片 # 返回tensor变量 def PIL_to_tensor(image): image = loader(image).unsqueeze(0) return image.to(device, torch.float) 3 Tensor转化为PIL图片 # 输入tensor变量 # 输出PIL格式图片 def tensor_to_PIL(tensor): image = tensor.cpu().clone() image = image.squeeze(0) image = unloader(image) return image 4 直接展示tensor格式图片 def imshow(tensor, title=None): image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated 5 直接保存tensor格式图片 def save_image(tensor, **para): dir = 'results' image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) if not osp.exists(dir): os.makedirs(dir) image.save('results_{}/s{}-c{}-l{}-e{}-sl{:4f}-cl{:4f}.jpg' .format(num, para['style_weight'], para['content_weight'], para['lr'], para['epoch'], para['style_loss'], para['content_loss'])) numpy与Tensor numpy格式是使用cv2，也就是python-opencv库读取出来的图片格式，需要注意的是用python-opencv读取出来的图片和使用PIL读取出来的图片数据略微不同，经测试用python-opencv读取出来的图片在训练时的效果比使用PIL读取出来的略差一些(详细过程之后发布)。 之后所有代码引用： import cv2 import torch import matplotlib.pyplot as plt numpy转化为tensor def toTensor(img): assert type(img) == np.ndarray,'the img type is {}, but ndarry expected'.format(type(img)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = torch.from_numpy(img.transpose((2, 0, 1))) return img.float().div(255).unsqueeze(0) # 255也可以改为256 tensor转化为numpy def tensor_to_np(tensor): img = tensor.mul(255).byte() img = img.cpu().numpy().squeeze(0).transpose((1, 2, 0)) return img 展示numpy格式图片 def show_from_cv(img, title=None): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.figure() plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) 展示tensor格式图片 def show_from_tensor(tensor, title=None): img = tensor.clone() img = tensor_to_np(img) plt.figure() plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) 注意 上面介绍的都是一张图片的转化，如果是n张图片一起的话，只需要修改一下相应代码即可。 举个例子，将之前说过的修改略微修改一下即可： # 将 N x H x W X C 的numpy格式图片转化为相应的tensor格式 def toTensor(img): img = torch.from_numpy(img.transpose((0, 3, 1, 2))) return img.float().div(255).unsqueeze(0) orchvision transforms 总结 2018年11月16日 17:00:04 Hansry 阅读数：2399 一.torchvision.transforms Transfoms 是很常用的图片变换方式，可以通过compose将各个变换串联起来 1. class torchvision.transforms.Compose (transforms) 这个类将多个变换方式结合在一起 参数：各个变换的实例对象 举例： transforms.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ]) 1234 二. 在PIL格式图片上的转换 1.class torchvision.transforms.CenterCrop(size) 剪切并返回PIL图片上中心区域 参数：size (序列或者整型)　—　输出的中心区域的大小。如果输入的size是整型而不是类似于 (h,w)的序列，那么将会转成类似(size, size)的序列。 2.class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) 随机改变图片的亮度、对比度和饱和度 参数： brightness(亮度，float类型)——调整亮度的程度，亮度因子(brightness_factor)从 [max(0,1-brightness), 1+brightness] 中均匀选取。 contrast(对比度，float类型)——调整对比度的程度，对比度因子(contrast_factor)从 [max(0,1-contrast),1+contrast] 中均匀选取。 saturation(饱和度，float类型)——调整饱和度的程度，饱和度因子(saturation_factor) [max(0,1-saturation),1+saturation] 中均匀选取。 hue(色相，float类型) —— 调整色相的程度，色相因子(hue_factor)从 [-hue,hue] 等均匀选择, 其中hue的大小为 [0, 0.5]。 对比度：　对比度指不同颜色之间的差别。对比度越大，不同颜色之间的反差越大，所谓黑白分明，对比度过大，图像就会显得很刺眼。对比度越小，不同颜色之间的反差就越小。 亮度：　亮度是指照射在景物或者图像上光线的明暗程度，图像亮度增加时，会显得刺眼或耀眼，亮度越小，会显得灰暗。 色相：　色相就是颜色，调整色相就是调整景物的颜色。 饱和度：　饱和度指图像颜色的浓度。饱和度越高，颜色越饱满，所谓的青翠欲滴的感觉。饱和度越低，颜色就会越陈旧，惨淡，饱和度为0时，图像就为灰度图像。 3. class torchvision.transforms.FiveCrop(size) 将给定的PIL图像剪裁成四个角落区域和中心区域 注意：　这个变换返回的是一个图像元组(tuple of images), 因此其输出跟输出的数量会不匹配。 参数：　size(序列或者整型) —— 需要返回的剪裁区域的尺寸。如果输入的是整型，那么会被转成(size,size)序列。 例子： transform = Compose([ FiveCrop(size), Lambda(lambda crops:torch.stack([ToTensor()(crop) for crop in crops])) #return a 4D tensor ]) #in your test loop you can do the following: input ,target = batch #input is a 5d tensor, target is 2d　 bs, ncrops,c ,h ,w = input.size() result = model(input.view(-1,c,h,w)) 　#fuse batch size and ncrops 转成(bs*ncrops, c, h , w) result_avg = result.view(bs, ncrops, -1).mean(1) #avg over crops 转成(bs，ncrops, c*h*w) 123456789 ４. class torchvision.transforms.Ｇrayscale(num_output_channels=1) 将图片转成灰度图 参数：　num_output_channels(int) ——　(1或者3)，输出图片的通道数量 返回：　输入图片的灰度图，如果num_output_channels=1, 返回的图片为单通道. 如果 num_output_channels=3, 返回的图片为3通道图片，且r=g=b 返回类型：PIL图片类型 5.　class torchvision.transforms.Pad(padding, fill=0, padding_mode=‘constant’) 对给定的PIL图像的边缘进行填充，填充的数值为给定填充数值 参数： padding(int或者tuple)——填充每一个边界。如果只输入了一个int类型的数值，那么这个数值会被用来填充所有的边界。如果输入的是tuple且长度为2，那么俩个数值分别被用于填充left/right 和 top/bottom。如果输入的数组为4，那么分别被用来填充left, top ,right 和 bottom边界。 fill (int 或者 tuple) ——　填充的像素的数值为fill。默认为0，如果输入的元组的长度为3，那么分别被用来填充R,G,B通道。这个数值当padding_mode 等于‘constant’　的时候才会被使用。 padding_mode (string) ——　填充的类型，必须为：constant, edge, reflect or symmetric，默认为 constant. constant: 以常量值进行填充，常量值由 fill 确定。 edge: 用图片边界最后一个值进行填充 reflect: pads with reflection of image without repeating the last value on the edge (这句不知怎么翻译，看下面例子) 例子: 用俩个元素填充[1,2,3,4], 将会返回[3,2,1,2,3,4,3,2] symmetric: pads with reflection of image repeating the last value on the edge 例子：用俩个函数元素填充 [1,2,3,4]，将会返回[2,1,1,2,3,4,4,3] 6. class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None) 保持中心不变的对图片进行随机仿射变化 参数：添加链接描述 degree (旋转，squence或者float或者int) ——　旋转的角度范围。如果角度是数值而不是类似于(min,max)的序列，那么将会转换成(-degree, +degree)序列。设为0则取消旋转。 transalate (平移，tuple，可选)　——　数组，其中元素为代表水平和垂直变换的最大绝对分数。例如translate=(a,b),那么水平位移数值为从　-image_widtha　随机采样的，同时垂直位移是从　-img_heightb 随机采样的。默认情况下没有平移。 scale (缩放，tuple, 可选)　——　缩放因子区间。若scale=(a,b), 则缩放的值在a 随机采样。默认情况下没有缩放。 shear (错切，sequence 或者 float 或者 int, 可选)　——　错切的程度。如果错切的程度是一个值，那么将会转换为序列即(—degree, +degree)。默认情况下不使用错切。 resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, 可选)。 fillcolor(整型) —— 可选择的在输出图片中填充变换以外的区域。(Pillow>=5.0.0) 7.torchvision.transforms.RandomApply(transforms, p=0.5) 随机选取变换中(各种变换存储在列表中)的其中一个，同时给定一定的概率 参数：　 变换（list或者tuple） ——　转换的列表 p (float 类型) —— 概率,选取某个变化需要的概率 8.transforms.RandomSizedCrop() RandomApply() RandomChoice() RadomCrop RamdomGrayscale() RamdomHorizontalFlip(p=0.5) RamdomRotation() … 还有各种Random，详细请查看torch.transforms 9.torchvision.transforms.Resize(size,interpolation=2) 将输入的PIL图片转换成给定的尺寸的大小 参数： size(sequence 或者 int)　——　需要输出的图片的大小。如果size是类似于(h,w)的序列，输出的尺寸将会跟(h,w)一致。如果size是整型，图片较小的边界将会被置为这个尺寸。例如，如果height->width, 图片将会被置为 (size*height/width, size) Interpolation (int, 可选) —— 默认为 PIL.Image.BILINEAR 三. 在torch.*Tensor上的转换 1. class torchvision.transforms.Normalize(mean,std) 用均值和标准差对张量图像进行标准化处理。给定n通道的均值(M1, … , Mn) 和标准差(S1, … ,Sn), 这个变化将会归一化根据均值和标准差归一化每个通道值。例如，input[channel] = (input[channel]-mean[channel])/std(channel) 参数： mean (squence) ——　每个通道的均值 std (sequence) —— 每个通道的标准差 __call__(tensor) 参数：tensor(Tensor) , 尺寸为(C,H,W)的图片将会被归一化 ; 返回：归一化后的Tensor类型图片 ; 返回类型：Ｔensor 四. 类型转换变换 (Conversion Transforms) 1. class torchvision.transforms.ToPILImage(mode=None) 将tensor类型或者ndarray转换成PIL图片 将 CxHxW大小的torch.*Tensor或者ＨxWxC 大小的numpy 矩阵转成PIL图片 参数：如果model为Ｎone,那么如果输入有三个通道，那么mode为RGB; 如果input有4个通道，mode为RGBA. 如果输入是1通道，mode为数据类型，如int, float, short __call__(pic) 参数：pic (Tensor或者numpy.ndarray类型的)　——　转换成PIL图片; 返回PIL图片; 返回类型为PIL类型 1 2. torchvision.transforms.ToTensor 将PIL图片或者numpy.ndarray转成Tensor类型的 将PIL图片或者numpy.ndarray(HxWxC) (范围在0-255) 转成torch.FloatTensor (CxHxW) (范围为0.0-1.0) __call__(pic) 参数：pic(PIL图片或者numpy.ndarray) —— 将图片转成向量; 返回Tensor类型的图片 1 五. 一般变换 (Generic Transforms) 1. torchvision.transforms.Lambda(lambd) 使用用户定义的lambda作为转换 参数：lambd(function) ——　用Lambda/funtion 作为变换 2. torchvision.transforms.functional.adjust_brightness(img, brightness_factor) 调整图片的亮度 参数： img(PIL 图片)——PIL图片 brightness_factor(float)——亮度调整程度。不能为负数, ０代表黑色图片，1代表原始图片，2代表增加了2个因子的亮度。 returns: 返回调整完的图片 3.torchvision.transforms.functional.adjust_contrast(img,contrast_factor) 调整图片的对比度 参数： img —— 需要调整的PIL 图片 constrast_factor(float) —— 调整对比度的程度。可以是非负的数。0为灰度图，1为原图，2为增加图片2个对比因子的图片。 returns　——　返回调整后的对比度图片 4. torchvison.transforms.function.adjust_gamma(img, gamma, gain=1) 对图片进行gamma校正，gamma校正详情 Iout=255∗gain∗(Iin/255)γI{out}=255gain(I{in}/255)^{\\gamma}Iou**t​=255∗gai**n∗(Iin​/255)γ 参数： img(PIL图片)——需要调整的PIL图片 gamma (float类型)——非零实数，公式中的γ\\gammaγ也是非零实数。gamma大于1使得阴影部分更暗，gamma小于1使得暗的区域亮些。 gain(float) ——　常量乘数 5.torchvision.transforms.functional.ajust_hue (img,hue_factor) 调整图片的色相 通过将图像转换为HSV来调整图像的色调，并在色调通道(H)中循环移动强度，然后将图像转换回原始图像模式。 色相因子是Ｈ通道平移量，其必须在区间[-0.5,0.5]中。 参数 img (PIL 图片)　——　需要调整的PIL图片 hue_factor (float类型) —— 色相通道平移的量，必须在[-0.5,0.5]之间。0.5和-0.5分别代表在HSV空间中正负方向完全相反的色相通道。0代表没有平移。 5. tochvision.transforms.functional.adjust_saturation(img, hue_factor) 调整图片的颜色饱和度 参数： img (PIL图片)——需要调整的PIL图片 饱和度因子(float类型)——调整饱和度的程度。0将会输出黑白图片，1将会输出原始图片，2将会增强2个因子的饱和度。 返回调整后的图片。 6. torchvision.transforms.functional.affine(img, angle, translate, scale, shear, resample=0, fillcolor=None) 对图片进行放射变换，保持中心不变。 参数： img (PIL图片)——需要变换的PIL图片 angle(float 或者 int)——旋转的的角度，角度范围为 (-180,180), 正方向为顺时针方向。 translate(list 或者 tuple)——水平或者垂直平移 scale(float)——总体缩放 shear(错切，float)——错切的角度位于(-180,180)，顺时针方向。 resample（这个有点看不懂，应该比较少用到——PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional fillcolor (int) —— 填充输出图片中超过变换的区域(Pillow>=5.0) 7.torchvision.transforms.functional.crop(img,i,j,h,w) 剪裁给定的PIL图片 参数： img(PIL图片)——被剪裁的图片 （i, j) ——左上角图片坐标 (h,w)——剪裁的图片的高和宽 returns: 返回剪彩的图片 8. torchvision.transforms.functional.normalize(tensor, mean, std) 根据给定的标准差和方差归一化tensor图片 参数： tensor(Tensor)—— 形状为(C,H,W)的Tensor图片 mean(squence) —— 每个通道的均值，序列 std (sequence) —— 每个通道的标准差，序列 返回：返回归一化后的Tensor图片。 9.torchvision.transforms.functional.pad(img, padding, fill=0, padding_mode=‘constant’)、torchvision.transforms.functional.resize(img, size, interpolation=2)、torchvision.transforms.functional.rotate(img, angle, resample=False, expand=False, center=None)、torchvision.transforms.functional.to_grayscale(img, num_output_channels＝１) 等均与上述函数类似，这里不再重复。 10.torchvision.transforms.functional.to_pil_image(pic, mode=None) 将tensor或者numpy.ndarray转成PIL图片torchvision.transforms.functional.to_tensor(pic) 将PIL图片或者numpy.ndarray转成tensor "},"linux/":{"url":"linux/","title":"Linux","keywords":"","body":" Linux技巧 Linux显卡驱动修复 "},"linux/linux_technique.html":{"url":"linux/linux_technique.html","title":"Linux技巧","keywords":"","body":"作者：程序员客栈 链接：https://www.zhihu.com/question/41115077/answer/602854935 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 推荐几个堪称神器的命令行软件，记得看完哈，越到后面越精彩！ WordGrinder：它是一款使用起来很简单，但拥有足够的编写和发布功能的文字编辑器。它支持基本的格式和样式，并且你可以将你的文字以 Markdown、ODT、LaTeX 或者 HTML 等格式导出； \\2. Proselint：它是一款全能的实时检查工具。它会找出行话、大话、不正确日期和时间格式、滥用的术语等等。它也很容易运行并忽略文本中的标记； \\3. GNU Aspell：它能够交互式地检测文本文档，能高亮显示拼写错误，还能在拼写错误的上方提供正确的拼写建议。Aspell 在进行拼写检查的时候，同样能够忽略许多语法标记； \\4. tldr：你能通过这个工具，快速查看查看各种命令的常用命令行例子： \\5. Alex：它是一个简单但很有用的小工具。适用于明文文本或者格式为 Markdown 或 HTML 的文档。Alex 会对“性别偏好、极端主义、种族相关、宗教，或者文章中其他不平等的措辞”产生警告。如果你想要试试看 Alex，这里有一个在线 demo； \\6. nmon：它能够帮你进行电脑的性能监控，包括 CPU，内存，磁盘 IO，网络 IO，并且界面很炫酷，是不是很像黑客，快去试试吧 nmon for Linux | Main \\7. axel：多线程断点下载工具，非常好用。例如下图中这样，指定了 8 个线程同时下载。 \\8. SpaceVim：这是一个 vim 插件，使你的 Vim 变成带代码自动补全等功能的更加强大的代码编辑器！ \\9. thefuck：你 git branch 打成 branch 了，然后命令行报错，你是不是心里会冒出一句 fuck？那你就在命令行里输入 fuck 然后回车！咦，成功了！ apt-get update 打成 aptget update 报错？输入 fuck 然后回车！就解决了！爽吧哈哈哈 "},"linux/linux_GPU.html":{"url":"linux/linux_GPU.html","title":"Linux显卡驱动修复","keywords":"","body":"nvidia-smi报错（重装Nvidia驱动） 遇到一个莫名其妙的问题： NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. 解决方案：重装NVIDIA驱动（非cuda） 首先在官网下载你自己显卡对应的驱动NVIDIA-Linux-x86_64-xxx.xx.run，拷贝到Linux某个目录后先改权限 chomod 777 NVIDIA-Linux-x86_64-xxx.xx.run 1 卸载原驱动 sudo apt-get remove --purge nvidia* # 提示有残留可以接 sudo apt autoremove 1 临时关闭显示服务 sudo service lightdm stop 1 运行安装程序 sudo ./NVIDIA-Linux-x86_64-375.66.run 安装后再重启显示 sudo service lightdm start "},"paper/":{"url":"paper/","title":"论文","keywords":"","body":" 论文写作 Image-to-Image 的论文汇总（含 GitHub 代码） GNN综述 Perceptual GAN for Small Object Detection阅读笔记 GAN变体-GMMN 网络 图像视频去噪中的Deformable Kernels Isolating Sources of Disentanglement in VAEs Spectral Normalization 谱归一化 不均衡样本loss 论文神经网络示意图 "},"paper/paper_write.html":{"url":"paper/paper_write.html","title":"论文写作","keywords":"","body":"科学写作的技巧——以机器学习为例 2018 年 2 月 2 日 论智 Zachary Lipton 作者：Zachary Lipton 编译：Bing 编者按：本文原作者Zachary Chase Lipton（http://zacklipton.com/）曾是美国加州大学圣迭戈分校计算机科学工程系的博士生，现在CMU Tepper商学院担任助理教授，同时还在机器学习部门任教，专注于机器学习领域的理论和实践研究。本文翻译自他的博文Heuristics for Scientific Writing (a Machine Learning Perspective)，为我们介绍了机器学习论文写作时的注意事项。 原文地址：http://approximatelycorrect.com/2018/01/29/heuristics-technical-scientific-writing-machine-learning-perspective/ 春节将至，大家还能静下心来写论文吗？随着新年的开启，各大顶会的论文提交截止日期也即将到来：ICML截稿日期为2月9日，KDD截止日期为2月11日，在这之后还有ACL、COLT、ECML、UAI以及NIPS……每场大会都会收到数千份论文， 随着开源软件、网络课程以及预印版文章的普及，越来越多的人开始对机器学习感兴趣，虽然成果不断丰富，但一个无法避免的事实是，很多论文由于书写的格式或技巧的不恰当，导致可读性不强，最终有可能影响评选结果，甚至被拒。即使在一些公认的有影响力的论文中，粗心大意的写作也会迷惑读者，甚至会被误以为是为了蒙骗某些奖学金而糊弄的论文。 但是，在我的学术生涯中，我对论文写作已经总结了一套十分详尽的攻略（在有些地方你可能会有不同意见）。在我读博期间从Charles Elkan教授那里学到了很多关于科学论文写作的重要启发式方法，每种都能提炼成精炼的语言。现在，当我和年轻的学生一起工作，指导他们如何写出清晰明了的论文时，我发现自己仍然在重复当年的写作方法，并且偶尔会有新发现。 文章周的每个建议都非常好记，都附有简短的解释。下面就让我们开始吧： 介绍 摘要尽量简短 摘要不可能包括全文所有内容，它应该是能让人两分钟就看完的“广告”，是对整篇论文的精准提炼。详细来说有四条原则： 用一句话或一个短语把你的问题描述清楚 明确现有方法存在的问题 表明你的主要成果（也可以在开头写明） 用两三句话说说细节和主要数据等 下面是我读过的机器学习论文中最棒的摘要： Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. The algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user, with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians. -Sanjoy Dasgupta in “Learning Mixtures of Gaussians” 如果这里Sanjoy把开头两句话合并起来会不会更紧凑呢？“Current techniques for learning mixtures of Gaussians from data are local search heuristics with weak performance guarantees.” 也许有的人觉得不错，有的人会反对，认为本文的关键词“Mixture of Gaussians”就不起眼了。 别想耍读者 如果论文结果需要定量表示，那么就在摘要和介绍中体现数字；如果论文里就一个简单公式，那就把它放在介绍里吧。人们只有感兴趣才会继续读下去，不要把这些信息都隐藏在论文中间。 删掉老套的开场白 “The last 10 years have witnessed tremendous growth in data and computers.” “Deep learning has had many successes at many things”.如果你的开头是这种通用型的，建议直接删掉。第一印象很重要，论文的第一句话往往是最关键的，千万不要浪费。 先提问再回答 如果没有问题直接写出解决方法将会非常无聊，如果你的论文特别抽象，完全不接地气，那么在读者看来就像一篇纯数学论文。如果可以的话，用实际案例作为开场，将抽象的问题具体化，然后用实验丰富这一论文。 你的方法能做什么，而不是不能做什么 有的时候可能需要建立对照，但是不要重点描述反面的对象，尤其是你自己的想法。当你客观地描述论文时，丢掉那些间接的描述，直接说清楚你的目标是什么，不要说某物不是什么。 结构 层次要分明 一篇论文分为好几部分，每部分又包括好几段，段落是由句子构成的，句子又是由单词构成的。有些论文只看一眼结构就知道质量高不高。每一节应该像PPT上的目录一样清晰地排列，而且它们的名字应该属于同一类别。有时一段话可以只有两个句子，但是最好不少于三句。 数据要有代表性 即使一位“小白”读者略过了图表中的一些数字，他也应该明确地了解你在讲什么。任何关键的推论或技术细节都要体现在正文中，其中可以利用图表增强可视化。 同样的，数字也要与主题紧密相关。如果读者（或审稿人）跳过文字直接看图表，他们也应该大致理解讲了什么，并了解研究结果的意义。如果不明白y轴的分数是越高越好还是越低越好，则应配有说明文字。 但是也不要太夸张，说明文字不能太长，最好在1到3行之间。注意，计算机视觉领域的论文有时一整页都被图表占据，后面也没有说明。我个人不喜欢这种风格，但是也要根据实际情况决定。 快速展示论文结果 论文铺垫不宜太长：（1）审稿人在每场会议上会阅读5至10篇论文，一年大概要读50至100篇相似领域的论文。重复的基础知识部分会让他们厌烦。（2）如果你的论文一共有8页，主要成果到第5页才展示出来的话，估计审稿人已经没有耐心再看下去了。 所以，一定要了解你的读者和论文的定位。你的摘要、介绍以及整篇论文都应该清晰地叙述主题。 在论文中解答读者的疑问 一个好的审稿人会提出相关质疑来挑战论文，比如会问：“有没有可能这种方法仅仅是因为X才能使用？”如果你回答：“我不知道”或者“不是”的话，你的论文有可能会被拒绝。如果你能提前预料到会被问哪些问题，就写下来。如果你不知道答案，就做个试验找找答案。希望大家能意识到，做研究和写作是分不开的。 风格 多用“我们（we）” 在科学写作中，要用“我们（we）”作为叙述主体。这里的“我们”代表读者和作者双方一起。有的时候你可能需要阐述观点，所以要在文中表达清楚这些情况。 别给自己挖坑 你必须保证，任何有相关知识的读者在读完你的整篇论文后，即使不认同你的观点、方法论选择或者价值观，也无法找出某一句话在表述上的错误。比如：“我们的方法X在大多数数据上比Y表现得要好。”这是真的吗？“大多数”什么数据集？审稿人是否能找到一个推翻这一结论的数据集？所以，最好把“大多数（most）”改为“很多（many）”，这样不管是定义还是反驳都更容易一些。 多一句不如少一句 与上文类似，如果你对某一结论并不是100%肯定，那就不要轻易做出。少写一句也许不会被拒，但多写一句就很有可能被审稿人pass掉。 如何表达自己的观点 你可能会问：“审稿人也许不同意我的观点，是不是就意味着我不能在论文中写出来呢？”并非这样，例如你认为未来GANs在异常检测方面很有前景，但在文章中你应该这样写：“在我看来（in my opinion），GANs……” 语言 切分长句 经验不足的写作者们总会错误地认为，长句能反映其遣词造句的能力。但是出色的科学论文大多用短句构成。如果不能把你的观点浓缩在一句话里，就试试把它们分开。技术协作要尽量简洁清楚，结论可以复杂，但是表述结论的话不能复杂。 删除强调词和空洞的动词 例如：extremely、very、incredibly、completely、barely、essentially、rather、quite、definitely…… 这些强调词有两点不好。首先，它们改变了句子的目的：“algorithm X provides a tight approximation”这听上去很有信心。如果加上一个修饰语“very”，就变成：“algorithm X provides a very tight approximation”总觉得有那么一点不确定性。另外，它们还能表达意见。例如：“Is the algorithm better？”是的。如果改成：“Is it much better？”这就是一种意见了，也许是在给自己挖坑。 主语、动词、修饰语必须保持一致 写作中常见的错误是将动词和修饰语放到错误的主语上。例如：“the algorithm tries to X, or the data is biased.”这句话中，算法不能自己尝试（tries），主语应该是我们（we）建模者，而不是算法。 推论：每个动词都应有所属。没有主语的动词通常用于被动结构中（主要动词是“to be”）。比如：“LSTMs are claimed to X, Y, Z”，这句话里“claimed”的主语是谁？这一信息最好在其他地方交代清楚，一种方法是在后面加括号，附上解释信息；另一种方法要作者清楚地说明。 参考文献 一般引用 你引用的文献的作者有可能就是你的审稿人哦。审稿人通常都会问你为什么没有引用某一作者的另外几个作品。如果与你的论文不相关，那就不要引用。如果它们是相关的，引用一下也没什么损失。这样做的话，你的论文结果不会太差，并且审稿人也许是你未来想要一起工作的人，引用他们的作品会引起他们的注意。 整体引用 审稿人通常都比较懒，而且没有高强的记忆力。如果你的作品是建立在别人的成果之上，那么不要只引用与你相关的部分——那么只是对你工作内容的总结。而是当你在读到优于你自己的方法时引用整段文字。尤其是近些年（5~10年）的作品，它们可能还不常见，适合出现在引用最多的“相关研究”版块里。 尽量多引用 这是一个非常实用的方法，适用于会议出版物，通常会议论文会限制参考文献的页数（一般是1或2页）。如果你漏了最重要的参考文献，审稿人是绝对不会放过你的。但是，如果漏了一些不太重要的文献，你可以说超出数目限制了（这是一个很好的借口）。所以如果你的参考文献太少的话，可就说不过去了。 "},"paper/Image_to_Image.html":{"url":"paper/Image_to_Image.html","title":"Image-to-Image 的论文汇总（含 GitHub 代码）","keywords":"","body":"Image-to-Image 的论文汇总（含 GitHub 代码） 图像生成一直是计算机视觉领域非常有意思的方向，图像到图像的变换是其中一个非常重要的应用，使用图像到图像的变换，可以完成非常多有趣的应用，可以把黑熊变成熊猫，把你的照片换成别人的表情，还可以把普通的照片变成毕加索风格的油画，自从GAN横空出世之后，这方面的应用也越来越多，下面是对这个领域的相关论文的一个整理，而且大部分都有代码！ github地址：https://github.com/ExtremeMart/image-to-image-papers 这是一个图像到图像的论文的汇总。论文按照arXiv上第一次提交时间排序。 监督学习 Note Model Paper Conference paper link code link pix2pix Image-to-Image Translation with Conditional Adversarial Networks CVPR 2017 1611.07004 junyanz/pytorch-CycleGAN-and-pix2pix texture guided TextureGAN TextureGAN: Controlling Deep Image Synthesis with Texture Patches CVPR 2018 1706.02823 janesjanes/Pytorch-TextureGAN Contextual GAN Image Generation from Sketch Constraint Using Contextual GAN ECCV 2018 1711.08972 pix2pix-HD High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs CVPR 2018 1711.11585 NVIDIA/pix2pixHD one-to-many BicycleGAN Toward Multimodal Image-to-Image Translation NIPS 2017 1711.11586 junyanz/BicycleGAN keypoint guided G2-GAN Geometry Guided Adversarial Facial Expression Synthesis MM 2018 1712.03474 contour2im Smart, Sparse Contours to Represent and Edit Images CVPR 2018 1712.08232 website disentangle Cross-domain disentanglement networks Image-to-image translation for cross-domain disentanglement NIPS 2018 1805.09730 video vid2vid Video-to-Video Synthesis NIPS 2018 1808.06601 NVIDIA/vid2vid video pix2pix-HD + Temporal Smoothing + faceGAN Everybody Dance Now ECCVW 2018 1808.07371 website 非监督学习 非监督学习-通用 Note Model Paper Conference paper link code link DTN Unsupervised Cross-Domain Image Generation ICLR 2017 1611.02200 yunjey/domain-transfer-network (unofficial) UNIT Unsupervised image-to-image translation networks NIPS 2017 1703.00848 mingyuliutw/UNIT DiscoGAN Learning to Discover Cross-Domain Relations with Generative Adversarial Networks ICML 2017 1703.05192 SKTBrain/DiscoGAN CycleGAN Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks ICCV 2017 1703.10593 junyanz/pytorch-CycleGAN-and-pix2pix DualGAN DualGAN: Unsupervised Dual Learning for Image-to-Image Translation ICCV 2017 1704.02510 duxingren14/DualGAN DistanceGAN One-Sided Unsupervised Domain Mapping NIPS 2017 1706.00826 sagiebenaim/DistanceGAN semi supervised Triangle GAN Triangle Generative Adversarial Networks NIPS 2017 1709.06548 LiqunChen0606/Triangle-GAN CartoonGAN CartoonGAN: Generative Adversarial Networks for Photo Cartoonization CVPR 2018 thecvf unofficial test, unofficial pytorch non-adversarial NAM NAM: Non-Adversarial Unsupervised Domain Mapping ECCV 2018 1806.00804 facebookresearch/nam SCAN Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks ECCV 2018 1807.08536 dilated conv, improve shape deform. GANimorph Improved Shape Deformation in Unsupervised Image to Image Translation ECCV 2018 1808.04325 brownvc/ganimorph instance aware InstaGAN Instance-aware image-to-image translation ICLR 2019 (in review) openreview 非监督学习-注意力机制或者模板导向机制 Note Model Paper Conference paper link code link mask ContrastGAN Generative Semantic Manipulation with Mask-Contrasting GAN ECCV 2018 1708.00315 attention DA-GAN DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks CVPR 2018 1802.06454 mask / attention Attention-GAN Attention-GAN for Object Transﬁguration in Wild Images 1803.06798 attention Attention guided GAN Unsupervised Attention-guided Image to Image Translation NIPS 2018 1806.02311 AlamiMejjati/Unsupervised-Attention-guided-Image-to-Image-Translation attention, one-sided Show, Attend and Translate: Unsupervised Image Translation with Self-Regularization and Attention 1806.06195 非监督学习-多对多（属性） Note Model Paper Conference paper link code link Conditional CycleGAN Conditional CycleGAN for Attribute Guided Face Image Generation ECCV 2018 1705.09966 StarGAN StarGAN: Uniﬁed Generative Adversarial Networks for Multi-Domain Image-to-Image Translation CVPR 2018 1711.09020 yunjey/StarGAN AttGAN AttGAN: Facial Attribute Editing by Only Changing What You Want 1711.10678 LynnHo/AttGAN-Tensorflow ComboGAN ComboGAN: Unrestrained Scalability for Image Domain Translation CVPRW 2018 1712.06909 AAnoosheh/ComboGAN AugCGAN (Augmented CycleGAN) Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data ICML 2018 1802.10151 aalmah/augmented_cyclegan sparsely grouped dataset SG-GAN Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation MM 2018 1805.07509 zhangqianhui/Sparsely-Grouped-GAN GANimation GANimation: Anatomically-aware Facial Animation from a Single Image ECCV 2018 (honorable mention) 1807.09251 albertpumarola/GANimation 非监督学习-分离（与/或样本导向） Note Model Paper Conference paper link code link XGAN XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings ICML 2018 1711.05139 dataset ELEGANT ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes ECCV 2018 1803.10562 Prinsphield/ELEGANT MUNIT Multimodal Unsupervised Image-to-Image Translation ECCV 2018 1804.04732 NVlabs/MUNIT cd-GAN (Conditional DualGAN) Conditional Image-to-Image Translation CVPR 2018 1805.00251 EG-UNIT Exemplar Guided Unsupervised Image-to-Image Translation 1805.11145 DRIT Diverse Image-to-Image Translation via Disentangled Representations ECCV 2018 1808.00948 HsinYingLee/DRIT non-disentangle, face makeup guided BeautyGAN BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network MM 2018 author UFDN A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation NIPS 2018 1809.01361 Alexander-H-Liu/UFDN "},"paper/GNN.html":{"url":"paper/GNN.html","title":"GNN综述","keywords":"","body":"GNN 综述 最近，清华大学孙茂松教授组在 arXiv 发布了论文 Graph Neural Networks: A Review of Methods and Applications，作者对现有的GNN模型做了详尽且全面的综述。 作者：周界、崔淦渠、张正彦*，杨成，刘知远，孙茂松 “图神经网络是连接主义与符号主义的有机结合，不仅使深度学习模型能够应用在图这种非欧几里德结构上，还为深度学习模型赋予了一定的因果推理能力。”论文的共同第一作者周界说。 “在深度学习方法的鲁棒性与可解释性受到质疑的今天，图神经网络可能为今后人工智能的发展提供了一个可行的方向。” GNN最近在深度学习领域受到了广泛关注。然而，对于想要快速了解这一领域的研究人员来说，可能会面临着模型复杂、应用门类众多的问题。 “本文希望为读者提供一个更高层次的视角，快速了解GNN领域不同模型的动机与优势。”周界告诉新智元：“同时，通过对不同的应用进行分类，方便不同领域的研究者快速了解将GNN应用到不同领域的文献。” 毫不夸张地说，论文中的图表对于想要了解学习GNN乃至因果推理等方向的研究者来说，简直应该高清打印过塑然后贴在墙上以作参考—— GNN的各种变体，通过比对各自的 aggregator & updater，就能轻松分辨不同的GNN模型。这只是这篇综述强大图表的一个示例。 想要快速了解GNN，看这篇文章绝对没错 在内容上，模型方面，本文从GNN原始模型的构建方式与存在的问题出发，介绍了对其进行不同改进的GNN变体，包括如何处理不同的图的类型、如何进行高效的信息传递以及如何加速训练过程。最后介绍了几个近年来提出的通用框架，它们总结概括了多个现有的方法，具有较强的表达能力。 在应用上，文章将GNN的应用领域分为了结构化场景、非结构化场景以及其他场景并介绍了诸如物理、化学、图像、文本、图生成模型、组合优化问题等经典的GNN应用。 典型应用场景介绍 文章最后提出了四个开放性问题，包括如何处理堆叠多层GNN造成的平滑问题，如何处理动态变化的图结构，如何使用通用的方法处理非结构化的数据以及如何将其扩展到更大规模的网络上。 作者还整理了一个GNN论文列表： https://github.com/thunlp/GNNPapers 以下是新智元对这篇综述的部分摘译，点击阅读原文查看 arXiv 论文。 原始GNN及其局限性 GNN的概念首先是在F. Scarselli等人的论文The graph neural network model（F. Scarselli et. al. 2009）中提出的。在这里，我们描述了原始的GNN，并列举了原始GNN在表示能力和训练效率方面的局限性。 接着，我们介绍了几种不同的GNN变体，这些变体具有不同的图形类型，利用不同的传播函数和训练方法。 最后，我们介绍了三个通用框架，分别是message passing neural network (MPNN)，non-local neural network (NLNN)，以及graph network(GN)。MPNN结合了各种图神经网络和图卷积网络方法；NLNN结合了几种“self-attention”类型的方法；而图网络GN可以概括本文提到的几乎所有图神经网络变体。 图神经网络 如前所述，图神经网络(GNN)的概念最早是Scarselli等人在2009年提出的，它扩展了现有的神经网络，用于处理图(graph)中表示的数据。在图中，每个节点是由其特性和相关节点定义的。 虽然实验结果表明，GNN是建模结构化数据的强大架构，但原始GNN仍存在一些局限性。 首先，对于固定节点，原始GNN迭代更新节点的隐藏状态是低效的。如果放宽了固定点的假设，我们可以设计一个多层的GNN来得到节点及其邻域的稳定表示。 其次，GNN在迭代中使用相同的参数，而大多数流行的神经网络在不同的层中使用不同的参数，这是一种分层特征提取方法。此外，节点隐藏状态的更新是一个顺序过程，可以从RNN内核(如GRU 和 LSTM)中获益。 第三，在边上也有一些无法在原始GNN中建模的信息特征。此外，如何学习边的隐藏状态也是一个重要的问题。 最后，如果我们把焦点放在节点的表示上而不是图形上，就不适合使用固定点，因为在固定点上的表示的分布在数值上是平滑的，区分每个节点的信息量也比较少。 图神经网络的变体 在这一节，我们提出图神经网络的几种变体。首先是在不同图类型上运行的变体，这些变体扩展了原始模型的表示能力。其次，我们列出了在传播步骤进行修改（卷积、门机制、注意力机制和skip connection）的几种变体，这些模型可以更好地学习表示。最后，我们描述了使用高级训练方法的标题，这些方法提高了训练效率。 图2概述了GNN的不同变体。 一览GNN的不同变体 图的类型(Graph Types) 在原始GNN中，输入的图由带有标签信息的节点和无向的边组成，这是最简单的图形格式。然而，世界上有许多不同的图形。这里，我们将介绍一些用于建模不同类型图形的方法。 图类型的变体 有向图(Directed Graphs ) 图形的第一个变体是有向图。无向边可以看作是两个有向边，表明两个节点之间存在着关系。然而，有向边比无向边能带来更多的信息。例如，在一个知识图中，边从head实体开始到tail实体结束，head实体是tail实体的父类，这表明我们应该区别对待父类和子类的信息传播过程。有向图的实例有ADGPM (M. Kampffmeyer et. al. 2018)。 异构图(Heterogeneous Graphs) 图的第二个变体是异构图，异构图有几种类型的节点。处理异构图最简单的方法是将每个节点的类型转换为与原始特征连接的一个one-hot特征向量。异构图如GraphInception。 带边信息的图(Edge-informative Graph) 图的另外一个变体是，每条边都有信息，比如权值或边的类型。例如G2S和R-GCN。 使用不同训练方法的图变体 训练方法变体 在传播步骤进行修改的GNN变体 传播步骤变体 GNN的三大通用框架 除了图神经网络的不同变体之外，我们还介绍了几个通用框架，旨在将不同的模型集成到一个框架中。 J. Gilmer等人(J. Gilmer et. al. 2017)提出了消息传递神经网络(message passing neural network， MPNN)，统一了各种图神经网络和图卷积网络方法。 X. Wang等人(X. Wang et. al. 2017)提出了非局部神经网络(non-local neural network, NLNN)，它结合了几种“self-attention”风格的方法。 P. W. Battaglia等人(P. W. Battaglia et. al. 2018)提出了图网络(graph network, GN)，它统一了统一了MPNN和NLNN方法以及许多其他变体，如交互网络(Interaction Networks)，神经物理引擎(Neural Physics Engine)，CommNet，structure2vec，GGNN，关系网络(Relation Network)，Deep Sets和Point Net。 几个尚未解决的问题 尽管GNN在不同领域取得了巨大成功，但值得注意的是，GNN模型还不能在任何条件下，为任何图任务提供令人满意的解决方案。这里，我们将陈述一些开放性问题以供进一步研究。 浅层结构 传统的深度神经网络可以堆叠数百层，以获得更好的性能，因为更深的结构具备更多的参数，可以显著提高网络的表达能力。然而，GNN总是很浅，大多数不超过三层。 实验显示，堆叠多个GCN层将导致过度平滑，也就是说，所有顶点将收敛到相同的值。尽管一些研究人员设法解决了这个问题，但这仍然是GNN的最大局限所在。设计真正的深度GNN对于未来的研究来说是一个令人兴奋的挑战，并将对进一步深入理解GNN做出相当大的贡献。 动态图形另一个具有挑战性的问题是如何处理具有动态结构的图形。静态图总是稳定的，因此对其进行建模是可行的，而动态图引入了变化的结构。当边和节点出现或消失时，GNN不能自适应地做出改变。目前对动态GNN的研究也在积极进行中，我们认为它是一般GNN的具备稳定性和自适应性的重要里程碑。 非结构性场景 我们讨论了GNN在非结构场景中的应用，但我们没有找到从原始数据中生成图的最佳方法。在图像域中，一些研究可以利用CNN获取特征图，然后对其进行上采样，形成超像素作为节点，还有的直接利用一些对象检测算法来获取对象节点。在文本域中，有些研究使用句法树作为句法图，还有的研究采用全连接图。因此，关键是找到图生成的最佳方法，使GNN在更广泛的领域发挥更大的作用。 可扩展性问题 如何将嵌入式算法应用于社交网络或推荐系统这类大规模网络环境，是几乎所有图形嵌入算法面对的一个致命问题，GNN也不例外。对GNN进行扩展是很困难的，因为涉及其中的许多核心流程在大数据环境中都要消耗算力。 这种困难体现在几个方面：首先，图数据并不规则，每个节点都有自己的邻域结构，因此不能批量化处理。其次，当存在的节点和边数量达到数百万时，计算图的拉普拉斯算子也是不可行的。此外，我们需要指出，可扩展性的高低，决定了算法是否能够应用于实际场景。目前已经有一些研究提出了解决这个问题的办法，我们正在密切关注这些新进展。 结论 在过去几年中，GNN已经成为图领域机器学习任务的强大而实用的工具。这一进展有赖于表现力，模型灵活性和训练算法的进步。在本文中，我们对图神经网络进行了全面综述。对于GNN模型，我们引入了按图类型、传播类型和训练类型分类的GNN变体。 此外，我们还总结了几个统一表示不同GNN变体的通用框架。在应用程序分类方面，我们将GNN应用程序分为结构场景、非结构场景和其他18个场景，然后对每个场景中的应用程序进行详细介绍。最后，我们提出了四个开放性问题，指出了图神经网络的主要挑战和未来的研究方向，包括模型深度、可扩展性、动态图处理和对非结构场景的处理能力。 "},"paper/Perceptual_GAN_for_Small_Object_Detection.html":{"url":"paper/Perceptual_GAN_for_Small_Object_Detection.html","title":"Perceptual GAN for Small Object Detection阅读笔记","keywords":"","body":"Perceptual GAN for Small Object Detection阅读笔记 作者：唐青 链接：https://zhuanlan.zhihu.com/p/83659247 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Perceptual Generative Adversarial Networks for Small Object Detectionopenaccess.thecvf.com 1 摘要及介绍概述 基于小目标，提出了Perceptual GAN网络来生成小目标的超分表达(super-resolved representation)，Perceptual GAN利用大小目标的结构相关性来增强小目标的表达(represnetation)，使其与其对应大目标的表达相似。 Perceptual GAN分为两个子网络，生成器(generator network) 和一个感知判别器( perceptual discriminator network)。原始的GAN的判别器用于判别生成器生成的图片为fake or ture，本文主要修改了判别器部分，使判别器还能从生成的超分图形中的检测获益量来计算损失值(perceptual loss)。 使用的数据库为Tsinghua-Tencent 100K(交通标志检测) 和 the Caltech benchmark(行人检测) 。 2 Perceptual GANs Perceptual GANs 对生成器和判别器都进行了修改，使生成器能生成小目标的超分表达，判别器拥有两个loss(dversarial loss and perceptual loss )。 Figure 1.　公式 文中3.1中的公式(本文中figure 1)表示本文目标是要最小化G和最大化D，即使用Generator生成小目标的超分图(与Groundtruth越近越好)，使用Discriminator判别生成的超分图与Groundtruth(使Discriminator约混淆越好)，让Discriminator不能判别生成的超分图是原图true还是生成图fake，从而达到生成图形与原图的无限接近。 Figure 2.　公式 文中使用大目标 作为Groundtruth，希望使用Generator将小目标生成为大目标。基于此，Figure1公式变为Figure2公式。 判别器输出为两个分枝： 对抗分枝：用于区分生成的超分表达与大目标groudtruth图片。训练办法和一般GAN中判别器类似，loss公式为章节3.1公式(3)。 感知分枝：用于justify生成的表达对检测精度的获益量。感知分枝先需提前使用大目标进行训练得到pre-trained模型，使其能产生相对高的检测精度。感知分枝的输出为一个多任务输出(classification+bounding-box regression)。 训练时轮次训练生成器与判别器 把所有的实例平均分为两类：大目标和小目标。首先使用大目标训练判别器的感知分枝，基于已学习过的感知分枝，使用小目标来训练生成器和使用大小目标一起来训练判别器的对抗分枝。交替地执行生成器和判别器网络的对抗分枝的训练过程，直到最终达到一个平衡点，即：可以为小目标生成类似大目标的超分特征，并且具有较高的检测精度。 Figure 3. 整个Perceptual GAN 结构图。 2.1 Conditional Generator Network Architecture生成器网络结构 虚线左边是生成器结构，上面为一般ConvNet用于特征提取，下面加了一组残差block，目的是输出一张图，让其对上面网络输出的小物体特征图进行补充(Eltwise-Sum)，组合生成Super-Resolved Feature。 2.2 Discriminator Network Architecture 判别器网络结构 虚线右边是判别器结构，其上下两部分别为对抗分枝与感知分枝。对抗分枝结构与一般classification(fake/true)网络类似，感知分枝从全连接开始又分为另外两个分枝，分别输出classification和Bounding box。判别器网络的损失为上下两枝的损失各自乘以权重后的和，本文中这两权重都设为1。 Figure 4为判别器对抗损失，最小化 ，即，使 )) 接近1。 Figure 4. 公式 判别器感知损失和一般detection结构类似。 "},"paper/GMMN.html":{"url":"paper/GMMN.html","title":"GAN变体-GMMN 网络","keywords":"","body":"就是不GAN——生成式矩(Moment)匹配网络GMMN 龙腾 在GAN发展热火朝天的当下，生成模型的其他方向显得有些势单力薄。GAN的一个薄弱环节在于Min-max Game问题的不易优化，该问题在WGAN中得到了较好的解决。WGAN核心思想即把判别器的Loss替换为衡量真实数据分布 与生成数据分布 的分布差异——Wasserstein距离。 事实上，这种思路并非首先由WGAN提出，在2015年的ICML上，就有人用优化MMD(maximum mean discrepancy,一种度量两个数据集的差异的度量)的方式提出了不同于GAN的生成式模型——Generative Moment Matching Networks。该模型使用一个(多元均匀分布上的)随机采样Sample作为输入，将经过若干非线性层之后的输出作为生成的样本。 本文的贡献有二：1.提出了基于MMD优化的GMMN，2.针对GMMN可能存在的问题(高维数据难以表现)给出了一个基于Auto-Encoder的改进方案GMMN-AE。其网络结构如下图所示： GMMN的贡献在于：使用了MMD作为损失函数易于优化，避免了难解的Min-max问题。MMD衡量两组样本 和 在某个空间S中的分布差异。如果某个空间S中包含了 和 的分布信息。那么最小化MMD就等价于拟合两个分布。 本文使用MMD的平方作为目标函数，如下式(1)所示： (这里的 是一个再生核变换，将样本从原空间映射到某个空间S，我们选择的函数 决定了目标空间S。再生核变换的理论性质参见视频课程：再生核变换与核技巧) 使用MMD的平方而非MMD是因为我们需要利用核技巧。（核技巧：机器学习中常用的技术，核技巧使得我们我们不用显式地知道变换后的样本 的形式，甚至不需要知道 的形式。因而 具有很高灵活性。使用核技巧只需要知道目标空间中样本的内积 ，而无需知道目标空间的其他性质。） 基于核技巧，(1)式展开为： 考虑 是恒等变换的情况 不难发现(1)式计算的是样本均值。当 是二次变换时，(1)式则计算了 和 的二阶矩。当 包含各次项的时候，(1)式就计算了各阶矩，此时，最小化(1)式就相当于匹配 和 的各阶矩，因而等效于匹配了两个分布。这就是为什么本文被称作矩匹配网络。 本文使用 的 是高斯变换： 高斯函数能够展开为无穷级数，因此可以满足(1)式计算各阶矩的需求。 标记生成样本为 ，数据样本为 ，我们容易计算(1)对生成样本的第 维的梯度 利用链式法则，梯度可以一直沿着网络后向传播。因此网络是可训练的。 GMMN存在一个难点：直接生成高维数据较为困难，因为： 高维的图像实际上存在低维流形。 高维数据不易处理 因此，本文提出GMMN+AE进行改进。该模型将整个训练过程打破为两部分，AE负责生成高维数据的低维流形，即中间表示层representation，然后在此基础上进行图像重构。而GMMN生成的内容是representation，而非图像，这个过程如图1.(b)所示。 实际应用中，GMMN+AE的训练分成三步完成： 1.逐层预训练AE 2.Finetune AE 3.训练GMMN 在后面的实验部分我们可以看到加入AE极大的提升了GMMN的表现。 本文还指出了一些GMMN常见的训练技巧，包括： 对AE的各个Encoder层施加Drop-out。 使用集成方法来集成不同 的高斯函数(实验中MNIST取 [2,5,10,20,40,80]六个核平均，TFD取[5,10,20,40,80,160]六个核平均)： \\3. 使用MMD的平方根来监督训练，以获得自适应学习率的效果(MMD较小时梯度不消失), \\4. Mini-batch训练：核技巧的弱点是：需要构造核矩阵(即目标空间的中样本的两两之间的内积)，因此，核矩阵的规模是n^2。所以不能处理大规模数据集。GMMN使用Mini-batch的方式来规避核矩阵过大的问题。其理由是——一个Batch的数据能够近似数据整体的分布。实际实现中使用的Batch-size为1000。 样本生成过程：在GMMN中，从多元均匀分布采样一个样本作为网络输入，进行一轮前向传播，得到的输出即为生成图像样本。在GMMN-A中，从均匀分布采样一个样本作为网络输入，进行一轮前向传播得到的是图像的representation。该representation再经过AE的解码层生成图像。 实验：生成模型的研究中，常用的衡量标准是：使用生成分布和真实分布的log-likelihood判定生成模型效果，本文对比方法包括: 深度信念网络DBN Contractive stacked auto-encoder Deep Generative Stochastic Network GAN GMMN GMMN+AE 使用的数据集为：MNIST 和 TFD（多伦多人脸） 其定量的结果为： 定性的结果如下图：(a)-(d)生成的结果 (e)-(f)生成数据以及生成的图像在真实数据中的最近邻 生成模型的一个风险是，仅仅记住了数据集而非真正的学会了数据分布，为此作者给出的两点证据： 上图(e)-(h)可以看出生成数据和真实数据集中的最近邻有略微差异。 用均匀分布中的5个点生成的5张图像（下图红框）；再以均匀分布上，这5个点之间的其他位置采样到的点（采样方式为线性插值）作为输入生成的图像。 本文的启示： 本文对Related Work的探讨非常全面，分析了其他生成模型的特点和不足： Boltzman系列的模型，主要问题是MCMC过程难以优化 信念网络系列模型，主要问题是对隐变量顺序有所假设，与真实世界不符 基于Auto Encoder重构概率分布的模型，也需要MCMC优化。 变分网络系列，需要额外信息。 \\2. 本文的写作很值得学习，作者在各个章节都反复强调了MMD对于Mini-max的优点，使得各个Section相对独立，降低了审稿人阅读的难度。 \\3. 本文的末尾探讨了许多基于MMD的可能的改进方向。 \\4. 本文的开源代码和Tensorflow版本 \\4. 本文的不足：Batch_size 1000的时候才能做到一个batch能够近似数据分布，这是对于MNIST和TFD这样的小数据集而言的，对于大规模数据集仍然是一个问题。 "},"paper/Deformable_Kernels.html":{"url":"paper/Deformable_Kernels.html","title":"图像视频去噪中的Deformable Kernels","keywords":"","body":"【论文解读】图像/视频去噪中的Deformable Kernels CV路上一名研究僧 东南大学 电子与通信工程硕士在读 1. 简介 这是一篇源自商汤(SenseTime)的论文，文章题目\"Learning Deformable Kernels for Image and Video Denoising\"。与KPN、MKPN(传送门)相似，也是一种基于核预测的去噪模型，整体上，网络结构也比较相似，不同点在于，预测出的Kernels对于周围像素点的权重，以及周围像素点的选择方式。本文源码已开源至Github，欢迎Start/Fork~ https://github.com/z-bingo/Deformable-Kernels-For-Video-Denoisinggithub.com 从BM3D这类经典的传统图像去噪方式起，不同方法之间的最大区别就在于如何用一种有效的方式选择合适的像素点、以及如何定义这些像素点的权重，进而对这些像素点加权平均可得到近似“干净”的图像，也就达到了图像去噪的目的。 就KPN和MKPN而言，网络的输出是per-pixel的自适应卷积核，KPN仅预测单一尺寸的卷积核；而MKPN可以同时预测多个尺寸的卷积核，进而适应含噪图像中物体的不同尺度，达到既可以较好地保留细节、又可以平滑平坦区域的目的。这篇文章提出的DKPN（暂且称之为DKPN），亦是如此，预测出per-pixel的自适应卷积核，但是对于可变形卷积来说，不仅包含weights和bias两部分，还会包含offsets，用于指示“周围像素”是指哪些像素，打破了传统卷积运算对于方形领域的定义。通过这种方式，可以在采样点有限的情况下，尽可能地增大kernels的感受野，有利于高效地利用图像中的信息。Deformable Kernels和传统的Kernels异同可通过下图体现： ​ Deformable Kernels 2. Deformable Convolution 本节通过尽可能简短、简洁的语言来介绍2D以及3D可变形卷积的原理，以及工作方式。 2.1 2D Deformable Convolution 对于一个已知的图像X，在去噪任务中可认为是含噪声的图像，那么，卷积核在该图像上卷积可表示为 其中， 表示在图像中的坐标， 表示卷积核的个数， 表示卷积核的采样偏移点，对于传统的方形领域，由 构成的集合为（以 的卷积核为例)： 基于传统卷积理论，可变形卷积在采样偏移点上额外叠加了一个可以学习的offsets参数，将规则采样问题变为了一个不规则采样问题，若将offests表示为 ,2D可变性卷积可表示为： 在此，有一个问题需要注意，由于offsets是学习得到的偏移量，因此，其一般不会是整数，而是小数，意味着要采样的点不处于规则的像素上。此时，就要通过双线性插值等插值算法根据规则的像素点进行插值，得到想要坐标点的像素值。 2.2 3D Deformable Convolution 与2D可变形卷积相似，3D可变性卷积就是3D卷积的拓展，在3D卷积的基础上添加三个可学习的offsets。传统3D卷积一般可用于Volume数据的处理中，对应于去噪领域，即多帧图像去噪或视频去噪，此时网络的输入可以看做 ，分别表示batch_size，输入图像的数量或视频的帧数，颜色通道数以及长和宽，3D卷积会作用于颜色通道、长和宽三个维度。那么，对于3D可变性卷积来说，除了长和宽两个维度的offsets之外，第三个offsets就成了帧与帧之间的偏移，这样就会更有利与网络在不同的帧之间提取有用的信息，对于充分利用连续的视频帧之间的冗余信息是非常有效的。3D可变形卷积可表示为： 对于非规则的采样点，在三维空间中通常通过三线性插值来实现。 3. DKPN网络结构 DKPN网络结构图 上图是DKPN网络结构图，其主干网络也是一个基于U-Net的Encoder-Decoder结构，由于可变性卷积多了额外的offsets参数需学习，与KPN不同，U-Net的输出不再是自适应卷积核的权重，可是offsets；offsets经过Sampler采样后，与输入的多帧含噪声图像concat到一起，再经过几个卷积层后可得到自适应卷积核的weights。此时，将刚刚采样得到的图像与weigths相乘便可得到去噪后图像。 需要注意的是，当去噪任务为单帧图像去噪时，每个像素点有两个需要需要的offset，而视频去噪时有三个offsets需要学习。 KPN中，作者采用了退火项作为Loss函数的一部分，单独为每帧输入图像预测去噪后的图像，这是为了防止网络很快收敛至一个局部极小值，使得参考帧之外的图像帧在输出去噪图像中几乎不起作用。相似的思想在DKPN中也得到了使用，对于输出的N个采样点，DKPN会将其分为s个组，每个组相互独立地预测去噪后的干净图像，并作为loss函数的退火项，这样就可以有效防止网络很快收敛至局部最小值。将输入图像序列经过Sampler后的图像分为s组，每组平均包含 个采样点，分别表示为 ，那么，相互独立的去噪后图像可以表示为： loss函数可以表示为： 4. 实验结果 文中部分实验结果如下： 5.一些思考 为了避免网络迅速收敛至一个局部最优解，对于得到的Deformable Kernels采样点，会将其分为s组，每组负责预测一个去噪后的图像尽可能与ground truth相似...同时，这种做法也会增强网络的泛化能力。 那么，如何分组或许也是一个比较重点的地方，博主想到了以下几种方法： (1) 规则分组，对于N个采样点，均匀地划分为s组，这也是最常规的方式无需仔细讨论； (2) 随机分组，对N个采样点随机分为s组，而不采用规则的分组，这样可以使得每个采样点都能发挥近似相等的地位，能否进一步提高泛化能力？ (3) 其实，分组的做法可以衍生至dropout层的原理，为了防止过拟合，按照一定概率断开一些链接。那么，也可以像dropout层一样，而不是采用分组的方式，每次按照概率p随机取一些采样点预测去噪后的图像，这样能够提高泛化能力？ 欢迎大家与我一起讨论~ Reference [1] Learning Deformable Kernels for Image and Video Denoising "},"paper/Isolating_Sources_of_Disentanglement_in_VAEs.html":{"url":"paper/Isolating_Sources_of_Disentanglement_in_VAEs.html","title":"Isolating Sources of Disentanglement in VAEs","keywords":"","body":"Isolating Sources of Disentanglement in VAEs 论文地址:https://arxiv.org/abs/1802.04942 作者:Ricky T. Q. Chen, Xuechen Li, Roger Grosse, David Duvenaud University of Toronto, Vector Institute 我们分解了变分下界，展示了潜变量中的全相关值(Total Correlation)的存在，并设计了β-TCVAE算法。β-TCVAE算法是一个精炼的，可以替代β-VAE来完成解纠缠（Disentanglement）表示算法，在训练过程中不需要额外的参数。我们并进一步提出了一种不需要分类器的解纠缠度量方法，叫做互信息间隔（Mutual Information Gap）。我们展示了大量的和高质量的实验，使用我们的模型在一些限制条件和非线性条件下的设置中，证明了全相关和解纠缠之间重要的关系。 "},"paper/Spectral_Normalization.html":{"url":"paper/Spectral_Normalization.html","title":"Spectral Normalization 谱归一化","keywords":"","body":"Spectral Normalization 谱归一化 本文主要介绍谱归一化这项技术，详细论文参考下面的链接。 Spectral Normalization For GANsarxiv.org 本文主要对论文中的基础知识和遗漏的细节做出补充，以便于更好地理解谱归一化。部分内容参考并整合了如下两篇博客：Spectral Normalization Explainedchristiancosgrove.com GAN 的谱归一化(Spectral Norm)和矩阵的奇异值分解(Singular Value Decompostion)kaiz.xyz Lipschitz continuity 在 GAN 中，假设我们有一个判别器 ，其中 是图像空间。如果判别器是 K-Lipschitz continuous 的，那么对图像空间中的任意 x 和 y，有： 其中 为 norm，如果 K 取到最小值，那么 K 被称为 Lipschitz constant。 直观地来说，Lipschitz 条件限制了函数变化的剧烈程度，即函数的梯度。在一维空间中，很容易看出 y=sin(x) 是 1-Lipschitz的，它的最大斜率是 1。 那么，为什么要使判别器函数具有 Lipschitz continuity 呢？Wasserstein GAN 提出了用 wasserstein 距离取代之前的 KL 散度或者 JS 散度，作为 GAN 判别器的损失函数： 其中 分别为真实数据和生成的数据的分布函数，wasserstein 距离衡量了这两个分布函数的差异性。直观地理解，就是根据这两个分布函数分别生成一堆数据 和另一堆数据 , 然后计算这两堆数据之间的距离。距离的算法是找到一种一一对应的配对方案 ，把 移动到 ，求总移动距离的最小值。由于在 GAN 中， 和 都没有显式的表达式，只能是从里面不停地采样，所以不可能找到这样的 ，无法直接优化公式 (2) 。W-GAN 的做法是根据 Kantorovich-Rubinstein duality，将公式 (2) 转化成公式 (3)，过程详见： Wasserstein GAN and the Kantorovich-Rubinstein Dualityvincentherrmann.github.io 其中 即为判别器函数。只有当判别器函数满足 1-Lipschitz 约束时，(2) 才能转化为 (3)。除此之外，正如上文所说，Lipschitz continuous 的函数的梯度上界被限制，因此函数更平滑，在神经网络的优化过程中，参数变化也会更稳定，不容易出现梯度爆炸，因此Lipschitz continuity 是一个很好的性质。 为了让判别器函数满足 1-Lipschitz continuity，W-GAN 和之后的 W-GAN GP 分别采用了 weight-clipping 和 gradient penalty 来约束判别器参数。这里的谱归一化，则是另一种让函数满足 1-Lipschitz continuity 的方式。 矩阵的 Lipschitz continuity 众所周知，矩阵的乘法是线性映射。对线性映射来说，如果它在零点处是 K-Lipschitz 的，那么它在整个定义域上都是 K-Lipschitz 的。想象一条过零点的直线，它的斜率是固定的，只要它上面任何一点是 K-Lipschitz 的，那么它上面所有点都是 K-Lipschitz 的。因此，对矩阵 来说，它满足 K-Lipschitz 的充要条件是： 对其做如下变换： 其中 表示两个向量的内积。由于矩阵 是半正定矩阵，它的所有特征值均为非负。我们假设它的特征向量构成的基底为 ，对应的特征值为 ，令 。那么，公式 (5) 的左半部分可以转化为： 要使公式 (6) 对任意 恒成立，且 非负，则必有 。若 为最大的特征值，只需要满足 ，这里 即为矩阵 的 spectral norm。 综上所述，映射 满足 K-Lipschitz 连续，K 的最小值为 。那么，要想让矩阵 满足 1-Lipschitz 连续，只需要在A的所有元素上同时除以 即可（观察公式 (4)左侧是线性映射）。 通过上面的讨论，我们得出了这样的结论：矩阵 除以它的 spectral norm（ 最大特征值的开根号 ）可以使其具有 1-Lipschitz continuity。 矩阵的奇异值分解 上文提到的矩阵的 spectral norm 的另一个称呼是矩阵的最大奇异值。回顾矩阵的 SVD 分解： 矩阵 存在这样的一种分解： 其中： U 是一个 的单位正交基矩阵 是一个 的对角阵，对角线上的元素为奇异值，非对角线上的元素为0 V 是一个 的单位正交基矩阵 SVD 分解 （上图来自：奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园） 由于 U 和 V 都是单位正交基，因此可以把矩阵乘以向量分成三步：旋转，拉伸，旋转。一前一后的两步旋转不改变向量的模长，唯一改变向量模长的是中间的拉伸，即与 相乘的那一步。而矩阵的 Lipschitz continuity 关心的正是矩阵对向量模长的改变，不关心旋转。因此，只需要研究中间的 即可。而 又是一个对角矩阵，因此，它对向量的模长拉长的最大值，就是对角线上的元素的最大值。也就是说，矩阵的最大奇异值即为它的 spectral norm。 根据公式 (7) ，以及 SVD 分解中 U V 都是单位正交基，单位正交基的转置乘以它本身为单位矩阵，有： 因此，只需要求出 的最大特征值，再开根号 ，就求出了矩阵的最大奇异值，也就是矩阵的 spectral norm，和上一小节的推导殊途同归。 神经网络的 Spectral Normalization 对于复合函数，我们有这样的定理： 而多层神经网络，正是多个复合函数嵌套的操作。最常见的嵌套是：一层卷积，一层激活函数，再一层卷积，再一层激活函数，这样层层包裹起来。而激活函数通常选取的 ReLU，Leaky ReLU 都是 1-Lipschitz 的，带入到 (9) 中相乘不影响总体的 Lipschitz constant，我们只需要保证卷积的部分是 1-Lipschitz continuous 的，就可以保证整个神经网络都是 1-Lipschitz continuous 的。 而在图像上每个位置的卷积操作，正好可以看成是一个矩阵乘法。因此，我们只需要约束各层卷积核的参数 ，使它是 1-Lipschitz continuous 的，就可以满足整个神经网络的 1-Lipschitz continuity。而我们已经知道，想让矩阵满足 1-Lipschitz continuous，只需要让它所有元素同时除以它的最大奇异值，或者说是它的 spectural norm。因此，下一步的问题是如何计算 的最大奇异值。 对大矩阵做 SVD 分解运算量很大，我们不希望在优化神经网络的过程中，每步都对卷积核矩阵做一次 SVD 分解。一个近似的解决方案是 power iteration 算法。 Power Iteration Power iteration 是用来近似计算矩阵最大的特征值（dominant eigenvalue 主特征值）和其对应的特征向量（主特征向量）的。 假设矩阵 A 是一个 的满秩的方阵，它的单位特征向量为 ，对应的特征值为 。那么对任意向量 ，有： 我们经过 k 次迭代： 由于 \\lambda_2>...>\\lambda_n\" alt=\"图片\"> (不考虑两个特征值相等的情况，因为太少见了)。可知，经过 k 次迭代后 ( )。因此： 也就是说，经过 k 次迭代后，我们将得到矩阵主特征向量的线性放缩，只要把这个向量归一化，就得到了该矩阵的单位主特征向量，进而可以解出矩阵的主特征值。 而我们在神经网络中，想求的是权重矩阵 的最大奇异值，根据上面几节的推导，知道这个奇异值正是 最大特征值的开方 。因此，我们可以采用 power iteration 的方式求解 的单位主特征向量，进而求出最大特征值 。论文中给出的算法是这样的： 如果单纯看分子，我们发现这两步合起来就是 ，反复迭代 (13) 中上下两个式子 ，即可得到矩阵 的单位主特征向量 。只不过 (13) 的每算“半”步都归一化一次。其实这种归一化并不影响向量 的方向收敛到主特征向量的方向，而只影响特征向量前面的系数。而每步归一化一次的好处是，每步都可以得到单位主特征向量的近似解。 那么，知道 的单位主特征向量 后，如何求出最大特征值 呢？ 而将公式 (13) 的第二个式子两边同时左乘 : 最终，(15) 即为论文中提出的权重矩阵 的 spectral norm 公式。 而在具体的代码实现过程中，可以随机初始化一个噪声向量 代入公式 (13) 。由于每次更新参数的 step size 很小，矩阵 的参数变化都很小，矩阵可以长时间维持不变。因此，可以把参数更新的 step 和求矩阵最大奇异值的 step 融合在一起，即每更新一次权重 ，更新一次 和 ，并将矩阵归一化一次（除以公式 (15) 近似算出来的 spectral norm）。 具体代码见： christiancosgrove/pytorch-spectral-normalization-gangithub.com 编辑于 2019-01-23 生成对抗网络（GAN） 计算机视觉 对角占优矩阵 　　对角占优矩阵是计算数学中应用非常广泛的矩阵类，它较多出现于经济价值模型和反网络系统的系数矩阵及解某些确定微分方程的数值解法中，在信息论、系统论、现代经济学、网络、算法和程序设计等众多领域都有着十分重要的应用。　　定义：n阶方阵A，如果其主对角线元素的绝对值大于同行其他元素绝对值之和，则称A是对角占优的。 如果A的每个对角元的绝对值都比所在行的非对角元的绝对值的和要大，即 |a_ii|>sum{j!=i}|a_ij| 对所有的i成立，那么称A是（行）严格对角占优阵。 如果A'是行严格对角占优阵，那么称A是列严格对角占优阵。 习惯上如果不指明哪种类型的话就认为是行对角占优。 严格对角占优矩阵一定正定吗？ 不一定，比如 负三阶单位矩阵 实对称矩阵是高等代数中一个重要的内容, 所谓定型实 对称矩阵是指正定、负定、半正定和半负定矩阵, 我们首先回 顾一下本文将用到的有关实对称矩阵的一些结论: 性质1: 一个实对称矩阵A正定的充要条件是存在可逆方 阵C, 使得A=C′C。 性质2: 一个实对称矩阵A半正定的充要条件是它的所有 主子式都大于等于零。 性质3: 一个实对称矩阵A负定( 半负定) 的充要条件是- A 为正定( 半正定) 。 性质4: n维欧氏空间中, 一组基ε1,ε2, ⋯,εn 的度量矩阵A= (aij), 其中aij=(εi,εj)为实对称矩阵, 而且矩阵A是正定的。 性质5: n维欧氏空间中, 两组基ε1,ε2, ⋯,εn 和η1 ,η2, ⋯,ηn 的度量矩阵分别为A和B, 那么A和B是合同的, 即若(η1,η2 , ⋯,ηn ) =(ε1,ε2, ⋯,εn)C, 则有B=C′AC。 本文要证明的主要定理为: 定理1: A=(aij)为n阶正定矩阵, 则有detA≤ n k=1 ∏akk 关于定型实对称矩阵的行列式的一个结论( 长江师范学院数学系, 重庆408100)杨世显 对称正定矩阵对角线上的元素必须相同吗？ 不必须，例如所有满足对角线元素都是正数的对角矩阵都是对称正定的。 为什么hermite正定矩阵的模最大的元素一定位于主对角线上? 利用正定矩阵的任何主子阵正定。 如果模最大的元素A(i,j)不在对角线上，那么二阶主子阵 A(i,i) A(i,j) A(j,i) A(j,j) 不是正定的。 对称正定矩阵的绝对值最大元为什么是对角元？ 正定矩阵的所有主子式大于0 则 aij=aji =0 ，主对角元是一阶主子式>=0，但其他主子式不一定>=0，故不一定。 有，零矩阵就是半正定的。 正定矩阵对角线的各元素都大于0吗？ 直接用正定的定义就可以了。 取x=(0,0,...,1,...,0)'，即第i个元素为1，其余为0的列向量，那么 x'Ax=a_{ii}>0。 都是非负数，可以有0，但是不能全部是0。 应该是正确的。我们可以利用数学归纳法来证明这个结论。 首先，n＝1时，是显然成立的。 假设，n＝k时成立。 则，当n＝k+1时。则考虑其一个n阶主子式，其也是正定的。其对角元的元素之和全都大于0。再考察另一个n阶主子式，则其对角元的元素也全大于零。综上知，其所有的对角元的元素都大于0。 综上知，命题得证。 正定矩阵对角线的元素aii都大于0吗？ 取x为单位阵的第i列，由x'Ax>0即得。 纯量阵就是A=aE 其中a为常数，E为单位矩阵 正定矩阵的所有的特征值都是大于零的， 而矩阵的迹(即：主对角线元素之和)=所有特征值的和>0 对角线元素均为0的对称矩阵，它是半正定的吗？ 半正定矩阵的对角线元素是非负的，而且零矩阵是一个特殊矩阵。请问对角线元素为0一定能推出是半正定吗？ 线性代数中什么叫纯量？为什么正定矩阵的主对角线上的元素都大于0？ 半正定，等价于所有主子式>=0 ，主对角元是一阶主子式>=0，但其他主子式不一定>=0，故不一定。 有，零矩阵就是半正定的。 对角线元素为0、非对角线元素大于等于0的对称矩阵，它是半正定的吗？ 不一定是，最简单的，二阶矩阵，角线元素为0、非对角线元素为1。而它的行列式为-1。 请问，对角线元素为0、非对角线元素大于等于0的对称矩阵，它是半正定的吗？ 不一定是，最简单的，二阶矩阵，角线元素为0、非对角线元素为1。而它的行列式为-1 为什么说半正定矩阵的行列式大于等于0? 因为半正定矩阵的特征值>=0 特别附加定义，半正定矩阵为对称矩阵 所以可以对角化(定理) A=P*B*P^-1 |A|=|B|>=0 "},"paper/Unbalanced_sample_loss.html":{"url":"paper/Unbalanced_sample_loss.html","title":"不均衡样本loss","keywords":"","body":"本文是谷歌对CVPR ' 19上发表的一篇文章的综述，文章的标题是Class-Balanced Loss Based on Effective Number of Samples。 它为最常用的损耗(softmax-cross-entropy、focal loss等)提出了一个针对每个类别的重新加权方案，能够快速提高精度，特别是在处理高度类不平衡的数据时。 论文的PyTorch实现源码：https://github.com/vandit15/Class-balanced-loss-pytorch 样本的有效数量 在处理长尾数据集(其中大部分样本属于很少的类，而许多其他类的样本非常少)的时候，如何对不同类的损失进行加权可能比较棘手。通常，权重设置为类样本的倒数或类样本的平方根的倒数。 传统的权重调整与这里提出的权重调整 然而，正如上面的图所示，这一过渡是因为随着样本数量的增加，新数据点的带来的好处会减少。新添加的样本极有可能是现有样本的近似副本，特别是在训练神经网络时使用大量数据增强(如重新缩放、随机裁剪、翻转等)的时候，很多都是这样的样本。用有效样本数重新加权可以得到较好的结果。 有效样本数可以想象为n个样本所覆盖的实际体积，其中总体积N由总样本表示。 有效样本数量 我们写成： 有效样本数量 我们还可以写成下面这样： 每个样本的贡献 这意味着第j个样本对有效样本数的贡献为βj-1。 上式的另一个含义是，如果β=0，则En=1。同样，当β→1的时候En→n。后者可以很容易地用洛必达法则证明。这意味着当N很大时，有效样本数与样本数N相同。在这种情况下，唯一原型数N很大，每个样本都是唯一的。然而，如果N=1，这意味着所有数据都可以用一个原型表示。 类别均衡损失 如果没有额外的信息，我们不能为每个类设置单独的Beta值，因此，使用整个数据的时候，我们将把它设置为一个特定的值(通常设置为0.9、0.99、0.999、0.9999中的一个)。 因此，类别均衡损失可表示为： 这里， L(p,y) 可以是任意的损失。 类别均衡Focal Loss 原始版本的focal loss有一个α平衡变量。这里，我们将使用每个类的有效样本数对其重新加权。 类似地，这样一个重新加权的项也可以应用于其他著名的损失(sigmod -cross-entropy, softmax-cross-entropy等)。 实现 在开始实现之前，需要注意的一点是，在使用基于sigmoid的损失进行训练时，使用b=-log(C-1)初始化最后一层的偏差，其中C是类的数量，而不是0。这是因为设置b=0会在训练开始时造成巨大的损失，因为每个类的输出概率接近0.5。因此，我们可以假设先验类是1/C，并相应地设置b的值。 每个类的权值的计算 计算归一化的权值 上面的代码行是获取权重并将其标准化的简单实现。 得到标签的onehot张量 在这里，我们得到权重的独热值，这样它们就可以分别与每个类的损失值相乘。 实验 类平衡提供了显著的收益，特别是当数据集高度不平衡时(不平衡= 200,100)。 结论 利用有效样本数的概念，可以解决数据重叠问题。由于我们没有对数据集本身做任何假设，因此重新加权通常适用于多个数据集和多个损失函数。因此，可以使用更合适的结构来处理类不平衡问题，这一点很重要，因为大多数实际数据集都存在大量的数据不平衡。 "},"paper/NN.html":{"url":"paper/NN.html","title":"论文神经网络示意图","keywords":"","body":"示意图 NN SVG ★★★★ 提供 三种典型 的神经网络绘图风格，个性化参数多 交互式绘图 jettan/tikz_cnn ★☆ 基于tikz的tex的宏指令绘制 绘制脚本繁杂 PlotNeuralNet ★★★☆ 底层基于tikz的tex的宏指令绘制 上层提供基于python的描述框架，绘制脚本简单 可绘制复杂的网络结构 ConvNetDraw ★★ 基于javascript和css绘制 仅支持基本Layer类型 gwding/draw_convnet ★★☆ 简单易用 底层基于matplotlib绘制 ajtulloch/dnngraph （便捷性不好评价，暂不打分） 基于Haskell语言 计算图 lutzroeder/netron ★★★★★ （2019.4.30新增，惭愧） 支持工具: ONNX, Keras, CoreML, Caffe2, MXNet, TensorFlow Lite, Caffe, PyTorch, Torch, CNTK, PaddlePaddle, Darknet, scikit-learn, TensorFlow.js, TensorFlow. 提供运行方式: 浏览器, Python Server; macOS, Linux, Windows draw_net.py ★★☆ caffe自带的画图工具 Netscope ★★★ 非常易用 仅支持网页版 Netscope-dgschwend ★★★☆ 基于Netscope二次开发 给出网络的各种计算操作次数（非常方便！） TFLearn ★★★☆ 其子工具提供了模型可视化的功能 Tensorboard ★★★ 其子工具提供了模型可视化的功能 如果还有遇到，我会继续补充的。 2019年3月24日更新 NN SVG HarisIqbal88/PlotNeuralNet jettan/tikz_cnn https://cbovar.github.io/ConvNetDraw/ 2017.11.04更新 Tensorboard 2017.10.13更新 TFLearn 2016.9.10更新 Netscope Netscope-dgschwend 最早回答 ajtulloch/dnngraph caffe/draw_net.py gwding/draw_convnet "},"super_resolution/":{"url":"super_resolution/","title":"超分辨率","keywords":"","body":" 超分辨率方向综述 超分辨率技术 超分辨率代码数据集合集 超分辨率baseline 超分辨率的损失函数总结 "},"super_resolution/SR_summarize.html":{"url":"super_resolution/SR_summarize.html","title":"超分辨率方向综述","keywords":"","body":"基于深度学习的超分辨率图像技术一览 近年来，使用深度学习技术的图像超分辨率（SR）取得了显着进步。一般可以将现有的SR技术研究大致分为三大类：监督SR，无监督SR和特定领域SR（人脸）。 先说监督SR 如今已经有各种深度学习的超分辨率模型。这些模型依赖于有监督的超分辨率，即用LR图像和相应的基础事实（GT）HR图像训练。虽然这些模型之间的差异非常大，但它们本质上是一组组件的组合，例如模型框架，上采样方法，网络设计和学习策略等。从这个角度来看，研究人员将这些组件组合起来构建一个用于拟合特定任务的集成SR模型。 由于图像超分辨率是一个病态问题，如何进行上采样（即从低分辨率产生高分辨率）是关键问题。基于采用的上采样操作及其在模型中的位置，SR模型可归因于四种模型框架：预先采样SR，后上采样SR，渐进上采样SR和迭代上下采样SR，如图所示。 除了在模型中的位置之外，上采样操作如何实现它们也非常重要。为了克服插值法的缺点，并以端到端的方式学习上采样操作，转置卷积层（Transposed Convolution Layer）和亚像素层（Sub-pixel Layer）可以引入到超分辨率中。 转置卷积层，即反卷积层，基于尺寸类似于卷积层输出的特征图来预测可能的输入。具体地说，它通过插入零值并执行卷积来扩展图像，从而提高了图像分辨率。为了简洁起见，以3×3内核执行2次上采样为例，如图所示。首先，输入扩展到原始大小的两倍，其中新添加的像素值被设置为0（b）。然后应用大小为3×3、步长1和填充1的内核卷积（c）。这样输入特征图实现因子为2的上采样，而感受野最多为2×2。 由于转置卷积层可以以端到端的方式放大图像大小，同时保持与vanilla卷积兼容的连接模式，因此它被广泛用作SR模型的上采样层。然而，它很容易在每个轴上产生“不均匀重叠（uneven overlapping）”，并且在两个轴的乘法进一步产生了特有的不同幅度棋盘状图案，从而损害了SR性能。 亚像素层也是端到端学习的上采样层，通过卷积生成多个通道然后重新整形，如图所示。首先卷积产生具有s2倍通道的输出，其中s是上采样因子（b）。假设输入大小为h×w×c，则输出大小为h×w×s2c。之后，执行整形（shuffle）操作产生大小为sh×sw×c的输出（c）。感受野大小可以达到3×3。 由于端到端的上采样方式，亚像素层也被SR模型广泛使用。与转置卷积层相比，亚像素层的最大优势是具有较大的感知场，提供更多的上下文信息，能帮助生成更准确的细节。然而，亚像素层的感受野的分布是不均匀的，块状区域实际上共享相同的感受野，这可能导致在块边界附近的一些畸变。 各种深度学习的模型已经被用于SR，如图所示。 ResNet学习残差而不是彻底的映射，已被SR模型广泛采用，如上图（a）所示。其中，残差学习策略可以大致分为两种类型，即全局和局部残差学习。 由于超分辨率是图像到图像的转换任务，其中输入图像与目标图像高度相关，全局残差学习仅学习两个图像之间的残差。在这种情况下，它避免学习从完整图像到另一个图像的复杂转换，而只需要学习残差图来恢复丢失的高频细节。由于大多数区域残差接近于零，模型的复杂性和学习难度都大大降低。这种方法在预上采样的SR框架普遍采用。 局部残差学习类似于ResNet的残差学习，用于缓解不断增加的网络深度引起的退化问题并提高学习能力。 实践中，上述方法都是通过快捷连接（通常有小常数因子的缩放）和逐元素加法操作实现的。区别在于，前者直接连接输入图像和输出图像，而后者通常在不同深度的网络中层之间添加多个快捷方式。 • 递归学习 递归学习（以递归方式多次应用相同模块）也被超分辨率采用，如上图 （b）所示。在实践中，递归学习固有地带来了消失（vanishing）或爆涨（exploding）梯度问题，因此残差学习和多信号监督等一些技术通常与递归学习相结合，以减轻这些问题。 • 通道关注 考虑到不同通道之间特征表征的相互依赖和作用，一种“挤压-激发（SAE，squeeze-and-excitation）”模块明确对通道相互依赖性建模，来提高表示能力，如上图（c）所示。其中用全局平均池化将每个输入通道压缩到通道描述子（即一个常数）中，然后将这些描述子馈送到两个全连接层产生通道尺度因子。基于通道乘法，用尺度因子重新缩放输入通道得到最终输出。 • 致密连接 致密连接在视觉任务中变得越来越流行。在致密块的每个层，所有前层的特征图用作输入，并且其自身特征图用作所有后续层的输入，在一个有l层致密块中带来l·（l - 1）/ 2个连接。致密连接，不仅有助于缓解梯度消失问题、增强信号的传播并促进特征重用，而且在连接之后采用小增长率（即致密块的通道数）和通道缩减来大大减少参数量。 为了融合低级和高级特征以提供更丰富的信息来重建高质量的细节，致密连接被引入SR领域，如上图（d）所示。 • 多路径学习 多路径学习指模型存在多个路径传递特征，这些路径执行不同的操作以提供更好的建模功能。具体而言，它可以分为三种类型：全局法、局部法和特定尺度法。 全局多路径学习是指用多个路径提取图像不同方面的特征。这些路径可以在传播中相互交叉，从而大大增强了特征提取的能力。 本地多路径学习用新块进行多尺度特征提取，如上图（e）所示。该块采用不同内核大小的卷积同时提取特征，然后将输出连接起来并再次进行相同的操作。快捷方式通过逐元素添加来连接该块的输出和输入。通过这种局部多路径学习，SR模型可以更好地从多个尺度提取图像特征，进一步提高性能。 特定尺度多路径学习共享模型的主要部分（即特征提取的中间部分），并分别在网络的开头和结尾附加特定尺度的预处理路径和上采样路径，如上图（f）所示。在训练期间，仅启用和更新与所选尺度对应的路径。这样大多数参数在不同尺度上共享。 • 高级卷积 卷积运算是深度神经网络的基础，改进卷积运算可获得更好的性能或更快的速度。这里给出两个方法：扩张卷积（Dilated Convolution）和群卷积（Group Convolution）。众所周知，上下文信息有助于在图像超分辨率生成逼真的细节。扩张卷积能将感受野增加两倍，最终实现更好的性能。群卷积以很少的性能损失可减少大量的参数和操作，如上图（g）所示。 • 像素递归学习 大多数SR模型认为这是一个与像素无关的任务，因此无法正确地确定生成像素之间的相互依赖性。在人注意力转移机制推动下，一种递推网络可依次发现参与的补丁并进行局部增强。以这种方式，模型能够根据每个图像自身特性自适应地个性化最佳搜索路径，从而充分利用图像全局的内依赖性（intra-dependence）。不过，需要长传播路径的递归过程，特别对超分辨率的HR图像，大大增加了计算成本和训练难度。 • 金字塔池化 金字塔池化模块更好地利用全局和局部的上下文信息，如上图（h）所示。具体地，对于尺寸为h×w×c的特征图，每个特征图被划分为M×M个区间，并经历全局平均池化产生M×M×c个输出。然后，执行1×1卷积输出压缩到一个单信道。之后，通过双线性插值将低维特征图上采样到与原始特征图相同的大小。使用不同的M，该模块可以有效地整合全局和局部的上下文信息。 • 小波变换 众所周知，小波变换（WT）是一种高效的图像表示，将图像信号分解为表示纹理细节的高频小波和包含全局拓扑信息的低频小波。将WT与基于深度学习的SR模型相结合，这样插值LR小波的子带作为输入，并预测相应HR子带的残差。WT和逆WT分别用于分解LR输入和重建HR输出。 另外学习策略问题，涉及损失函数的设计（包括像素损失，内容损失，纹理损失，对抗损失和周期连续损失）、批处理归一化（BN）、课程学习（Curriculum Learning）和多信号监督（Multi-supervision）等等。 再说无监督SR 现有的超分辨率工作主要集中在监督学习上，然而难以收集不同分辨率的相同场景的图像，因此通常通过对HR图像预定义退化来获得SR数据集中的LR图像。为了防止预定义退化带来的不利影响，无监督的超分辨率成为选择。在这种情况下，只提供非配对图像（HR或LR）用于训练，实际上得到的模型更可能应对实际场景中的SR问题。 • 零击（zero shot）超分辨率 单个图像内部的统计数据足以提供超分辨率所需的信息，所以零击超分辨率（ZSSR）在测试时训练小图像特定的SR网络进行无监督SR，而不是在大数据集上训练通用模型。具体来说，核估计方法直接从单个测试图像估计退化内核，并在测试图像上执行不同尺度因子的退化来构建小数据集。然后在该数据集上训练超分辨率的小CNN模型用于最终预测。 ZSSR利用图像内部特定信息的跨尺度复现这一特点，对非理想条件下（非bi-cubic退化核获得的图像，受模糊、噪声和压缩畸变等影响）更接近现实世界场景的图像，比以前的方法性能提高一大截，同时在理想条件下（bi-cubic插值构建的图像），和以前方法结果差不多。尽管这样，由于需要在测试期间为每个图像训练单个网络，使得其测试时间远比其他SR模型长。 • 弱监督SR 为了在超分辨率中不引入预退化，弱监督学习的SR模型，即使用不成对的LR-HR图像，是一种方案。一些方法学习HR-LR退化模型并用于构建训练SR模型的数据集，而另外一些方法设计周期循环（cycle-in-cycle）网络同时学习LR-HR和HR-LR映射。 由于预退化是次优的，从未配对的LR-HR数据集中学习退化是可行的。一种方法称为“两步法”： 1）训练HR-LR 的GAN模型，用不成对的LR-HR图像学习退化； 2）基于第一个GAN模型，使用成对的LR-HR图像训练LR- HR 的GAN模型执行SR。 对于HR到LR 的GAN模型，HR图像被馈送到生成器产生LR输出，不仅需要匹配HR图像缩小（平均池化）获得的LR图像，而且还要匹配真实LR图像的分布。训练之后，生成器作为退化模型生成LR-HR图像对。 对于LR到HR 的GAN模型，生成器（即SR模型）将生成的LR图像作为输入并预测HR输出，不仅需要匹配相应的HR图像而且还匹配HR图像的分布 。 在“两步法”中，无监督模型有效地提高了超分辨率真实世界LR图像的质量，比以前方法性能获得了很大改进。 无监督SR的另一种方法是将LR空间和HR空间视为两个域，并使用周期循环结构学习彼此之间的映射。这种情况下，训练目的包括推送映射结果去匹配目标的域分布，并通过来回（round trip）映射使图像恢复。 • 深度图像先验知识 CNN结构在逆问题之前捕获大量的低级图像统计量，所以在执行SR之前可使用随机初始化的CNN作为手工先验知识。具体地讲，定义生成器网络，将随机向量z作为输入并尝试生成目标HR图像I。训练目标是网络找到一个Iˆ y，其下采样Iˆy与LR图像Ix相同。 因为网络随机初始化，从未在数据集上进行过训练，所以唯一的先验知识是CNN结构本身。虽然这种方法的性能仍然比监督方法差很多，但远远超过传统的bicubic上采样。此外，表现出的CNN架构本身合理性，促使将深度学习方法与CNN结构或自相似性等先验知识相结合来提高超分辨率。 特定SR 特定SR领域主要包括深度图、人脸图像、高光谱图像和视频等内容的SR应用。 面部图像超分辨率，即面部幻觉（FH， face hallucination），通常可以帮助其他与面部相关的任务。与通用图像相比，面部图像具有更多与面部相关的结构化信息，因此将面部先验知识（例如，关键点，结构解析图和身份）结合到FH中是非常流行且有希望的方法。利用面部先验知识的最直接的方式是约束所生成的HR图像具有与基础事实（GT）的HR图像相同的面部相关信息。 与全色图像（PAN，panchromatic images），即具有3个波段的RGB图像相比，有数百个波段的高光谱图像（HSI，hyperspectral images）提供了丰富的光谱特征并有助于各种视觉任务。然而，由于硬件限制，收集高质量的HSI比收集PAN更困难，收集的HSI分辨率要低得多。因此，超分辨率被引入该领域，研究人员倾向于将HR PAN和LR HSI结合起来预测HR HSI。 就视频超分辨率而言，多个帧提供更多的场景信息，不仅有帧内空间依赖性而且有帧间时间依赖性（例如，运动、亮度和颜色变化）。大多数方法主要集中在更好地利用时空依赖性，包括显式运动补偿（例如，光流算法、基于学习的方法）和递归方法等。 参考文献 Z Wang, J Chen, S Hoi,“Deep Learning for Image Super-resolution: A Survey”, Arxiv 1902.06068,2019 W Yang et al.,“Deep Learning for Single Image Super-Resolution: A Brief Review”, Archiv 1808.03344, 2018 Z Li et al.,“Feedback Network for Image Super-Resolution”, CVPR 2019 C Chen et al.,“Camera Lens Super-Resolution”, CVPR 2019 K Zhang et al.,“Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels”, CVPR 2019 -完- "},"super_resolution/SR.html":{"url":"super_resolution/SR.html","title":"超分辨率技术","keywords":"","body":"超分辨率技术（Super-Resolution, SR）是指从观测到的低分辨率图像重建出相应的高分辨率图像，在监控设备、卫星图像和医学影像等领域都有重要的应用价值。 本文针对端到端的基于深度学习的单张图像超分辨率方法(Single Image Super-Resolution, SISR)，总结一下从SRCNN到EDSR的发展历程。(排列顺序大致按论文中给出的4倍上采样结果的峰值信噪比(Peak Signal to Noise Ratio, PSNR)从低到高排列) 1. SRCNN (Learning a Deep Convolutional Network for Image Super-Resolution, ECCV2014) SRCNN是深度学习用在超分辨率重建上的开山之作。SRCNN的网络结构非常简单，仅仅用了三个卷积层，网络结构如下图所示。 SRCNN SRCNN首先使用双三次(bicubic)插值将低分辨率图像放大成目标尺寸，接着通过三层卷积网络拟合非线性映射，最后输出高分辨率图像结果。本文中，作者将三层卷积的结构解释成三个步骤：图像块的提取和特征表示，特征非线性映射和最终的重建。 三个卷积层使用的卷积核的大小分为为9x9,，1x1和5x5，前两个的输出特征个数分别为64和32。用Timofte数据集（包含91幅图像）和ImageNet大数据集进行训练。使用均方误差(Mean Squared Error, MSE)作为损失函数，有利于获得较高的PSNR。 code: http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html 2. FSRCNN (Accelerating the Super-Resolution Convolutional Neural Network, ECCV2016) FSRCNN与SRCNN都是香港中文大学Dong Chao， Xiaoou Tang等人的工作。FSRCNN是对之前SRCNN的改进，主要在三个方面：一是在最后使用了一个反卷积层放大尺寸，因此可以直接将原始的低分辨率图像输入到网络中，而不是像之前SRCNN那样需要先通过bicubic方法放大尺寸。二是改变特征维数，使用更小的卷积核和使用更多的映射层。三是可以共享其中的映射层，如果需要训练不同上采样倍率的模型，只需要fine-tuning最后的反卷积层。 由于FSRCNN不需要在网络外部进行放大图片尺寸的操作，同时通过添加收缩层和扩张层，将一个大层用一些小层来代替，因此FSRCNN与SRCNN相比有较大的速度提升。FSRCNN在训练时也可以只fine-tuning最后的反卷积层，因此训练速度也更快。FSRCNN与SCRNN的结构对比如下图所示。 FSRCNN FSRCNN可以分为五个部分。特征提取：SRCNN中针对的是插值后的低分辨率图像，选取的核大小为9×9，这里直接是对原始的低分辨率图像进行操作，因此可以选小一点，设置为5×5。收缩：通过应用1×1的卷积核进行降维，减少网络的参数，降低计算复杂度。非线性映射：感受野大，能够表现的更好。SRCNN中，采用的是5×5的卷积核，但是5×5的卷积核计算量会比较大。用两个串联的3×3的卷积核可以替代一个5×5的卷积核，同时两个串联的小卷积核需要的参数3×3×2=18比一个大卷积核5×5=25的参数要小。FSRCNN网络中通过m个核大小为3×3的卷积层进行串联。扩张：作者发现低维度的特征带来的重建效果不是太好，因此应用1×1的卷积核进行扩维，相当于收缩的逆过程。反卷积层：可以堪称是卷积层的逆操作，如果步长为n，那么尺寸放大n倍，实现了上采样的操作。 FSRCNN中激活函数采用PReLU，损失函数仍然是均方误差。对CNN来说，Set91并不足够去训练大的网络结构，FSRCNN提出general-100 + Set91进行充当训练集。并且进行数据增强，1）缩小尺寸为原来的0.9, 0.8, 0.7和0.6。2）旋转 90°，180°和270°，因此获得了数据量的提升。 code: http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html 3. ESPCN (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network, CVPR2016) 作者在本文中介绍到，像SRCNN那样的方法，由于需要将低分辨率图像通过上采样插值得到与高分辨率图像相同大小的尺寸，再输入到网络中，这意味着要在较高的分辨率上进行卷积操作，从而增加了计算复杂度。本文提出了一种直接在低分辨率图像尺寸上提取特征，计算得到高分辨率图像的高效方法。ESPCN网络结构如下图所示。 ESPCN ESPCN的核心概念是亚像素卷积层(sub-pixel convolutional layer)。网络的输入是原始低分辨率图像，通过三个卷积层以后，得到通道数为 的与输入图像大小一样的特征图像。再将特征图像每个像素的 个通道重新排列成一个 的区域，对应高分辨率图像中一个 大小的子块，从而大小为 的特征图像被重新排列成 的高分辨率图像。我理解的亚像素卷积层包含两个过程，一个普通的卷积层和后面的排列像素的步骤。就是说，最后一层卷积层输出的特征个数需要设置成固定值，即放大倍数r的平方，这样总的像素个数就与要得到的高分辨率图像一致，将像素进行重新排列就能得到高分辨率图。 在ESPCN网络中，图像尺寸放大过程的插值函数被隐含地包含在前面的卷积层中，可以自动学习到。由于卷积运算都是在低分辨率图像尺寸大小上进行，因此效率会较高。 训练时，可以将输入的训练数据标签，预处理成重新排列操作前的格式，比如将21×21的单通道图，预处理成9个通道，7×7的图，这样在训练时，就不需要做重新排列的操作。另外，ESPCN激活函数采用tanh替代了ReLU。损失函数为均方误差。 github(tensorflow): https://github.com/drakelevy/ESPCN-TensorFlow github(pytorch): https://github.com/leftthomas/ESPCN github(caffe): https://github.com/wangxuewen99/Super-Resolution/tree/master/ESPCN 4. VDSR (Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR2016) 在介绍VDSR之前，首先想先提一下何恺明在2015年的时候提出的残差网络ResNet。ResNet的提出，解决了之前网络结构比较深时无法训练的问题，性能也得到了提升，ResNet也获得了CVPR2016的best paper。残差网络结构(residual network)被应用在了大量的工作中。 正如在VDSR论文中作者提到，输入的低分辨率图像和输出的高分辨率图像在很大程度上是相似的，也就是指低分辨率图像携带的低频信息与高分辨率图像的低频信息相近，训练时带上这部分会多花费大量的时间，实际上我们只需要学习高分辨率图像和低分辨率图像之间的高频部分残差即可。残差网络结构的思想特别适合用来解决超分辨率问题，可以说影响了之后的深度学习超分辨率方法。VDSR是最直接明显的学习残差的结构，其网络结构如下图所示。 VDSR VDSR将插值后得到的变成目标尺寸的低分辨率图像作为网络的输入，再将这个图像与网络学到的残差相加得到最终的网络的输出。VDSR主要有4点贡献。1.加深了网络结构(20层)，使得越深的网络层拥有更大的感受野。文章选取3×3的卷积核，深度为D的网络拥有(2D+1)×(2D+1)的感受野。2.采用残差学习，残差图像比较稀疏，大部分值都为0或者比较小，因此收敛速度快。VDSR还应用了自适应梯度裁剪(Adjustable Gradient Clipping)，将梯度限制在某一范围，也能够加快收敛过程。3.VDSR在每次卷积前都对图像进行补0操作，这样保证了所有的特征图和最终的输出图像在尺寸上都保持一致，解决了图像通过逐步卷积会越来越小的问题。文中说实验证明补0操作对边界像素的预测结果也能够得到提升。4.VDSR将不同倍数的图像混合在一起训练，这样训练出来的一个模型就可以解决不同倍数的超分辨率问题。 code: https://cv.snu.ac.kr/research/VDSR/ github(caffe): https://github.com/huangzehao/caffe-vdsr github(tensorflow): https://github.com/Jongchan/tensorflow-vdsr github(pytorch): https://github.com/twtygqyy/pytorch-vdsr 5. DRCN (Deeply-Recursive Convolutional Network for Image Super-Resolution, CVPR2016) DRCN与上面的VDSR都是来自首尔国立大学计算机视觉实验室的工作，两篇论文都发表在CVPR2016上，两种方法的结果非常接近。DRCN第一次将之前已有的递归神经网络(Recursive Neural Network)结构应用在超分辨率处理中。同时，利用残差学习的思想(文中的跳跃连接（Skip-Connection）)，加深了网络结构(16个递归)，增加了网络感受野，提升了性能。DRCN网络结构如下图所示。 DRCN DRCN输入的是插值后的图像，分为三个模块，第一个是Embedding network，相当于特征提取，第二个是Inference network, 相当于特征的非线性映射，第三个是Reconstruction network,即从特征图像恢复最后的重建结果。其中的Inference network是一个递归网络，即数据循环地通过该层多次。将这个循环进行展开，等效于使用同一组参数的多个串联的卷积层，如下图所示。 DRCN 其中的 到 是D个共享参数的卷积层。将这D个卷积层的每一层的结果都通过相同的Reconstruction Net，在Reconstruction Net中与输入的图像相加，得到D个输出重建结果。这些所有的结果在训练时都同时被监督，即所有的递归都被监督，作者称之为递归监督(Recursive-Supervision)，避免了梯度消失/爆炸问题。将D个递归得到的结果再加权平均： 得到一个总输出。每个加权 在训练的过程中也不断地更新。最终的目标函数就需要优化每一个递归层输出的误差和总输出的误差： 表示的是权值衰减(weight decay)。 的初始值设置得比较高以使得训练过程稳定，因为训练开始的阶段递归更容易收敛。随着训练的进行， 逐渐衰减来提升最终输出的性能。 code: https://cv.snu.ac.kr/research/DRCN/ github(tensorflow): https://github.com/jiny2001/deeply-recursive-cnn-tf 6. RED (Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections, NIPS2016) 这篇文章提出了由对称的卷积层-反卷积层构成的网络结构，作为一个编码-解码框架，可以学习由低质图像到原始图像端到端的映射。网络结构如下图所示。 RED RED网络的结构是对称的，每个卷积层都有对应的反卷积层。卷积层用来获取图像的抽象内容，反卷积层用来放大特征尺寸并且恢复图像细节。卷积层将输入图像尺寸减小后，再通过反卷积层上采样变大，使得输入输出的尺寸一样。每一组镜像对应的卷积层和反卷积层有着跳线连接结构，将两部分具有同样尺寸的特征(要输入卷积层的特征和对应的反卷积层输出的特征)做相加操作(ResNet那样的操作)后再输入到下一个反卷积层，操作过程如下图所示。 这样的结构能够让反向传播信号能够直接传递到底层，解决了梯度消失问题，同时能将卷积层的细节传递给反卷积层，能够恢复出更干净的图片。可以看到，网络中有一条线是将输入的图像连接到后面与最后的一层反卷积层的输出相加，也就是VDSR中用到的方式，因此RED中间的卷积层和反卷积层学习的特征是目标图像和低质图像之间的残差。RED的网络深度为30层，损失函数用的均方误差。 7. DRRN (Image Super-Resolution via Deep Recursive Residual Network, CVPR2017) DRRN的作者应该是受到了ResNet、VDSR和DRCN的启发，采用了更深的网络结构来获取性能的提升。作者也在文中用图片示例比较了DRRN与上述三个网络的区别，比较示例图如下所示。 DRRN DRRN中的每个残差单元都共同拥有一个相同的输入，即递归块中的第一个卷积层的输出。每个残差单元都包含2个卷积层。在一个递归块内，每个残差单元内对应位置相同的卷积层参数都共享(图中DRRN的浅绿色块或浅红色块)。作者列出了ResNet、VDSR、DRCN和DRRN四者的主要策略。ResNet是链模式的局部残差学习。VDSR是全局残差学习。DRCN是全局残差学习+单权重的递归学习+多目标优化。DRRN是多路径模式的局部残差学习+全局残差学习+多权重的递归学习。 文章中比较了不同的递归块和残差单元数量的实验结果，最终选用的是1个递归块和25个残差单元，深度为52层的网络结构。总之，DRRN就是通过对之前已有的ResNet等结构进行调整，采取更深的网络结构得到结果的提升。 github(caffe): tyshiwo/DRRN_CVPR17 github(pytorch): https://github.com/jt827859032/DRRN-pytorch 8. LapSRN (Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution, CVPR2017) 论文中作者先总结了之前的方法存在有三点问题。一是有的方法在输入图像进网络前，需要使用预先定义好的上采样操作(例如bicubic)来获得目标的空间尺寸，这样的操作增加了额外的计算开销，同时也会导致可见的重建伪影。而有的方法使用了亚像素卷积层或者反卷积层这样的操作来替换预先定义好的上采样操作，这些方法的网络结构又相对比较简单，性能较差，并不能学好低分辨率图像到高分辨率图像复杂的映射。二是在训练网络时使用 型损失函数时，不可避免地会产生模糊的预测，恢复出的高分辨率图片往往会太过于平滑。三是在重建高分辨率图像时，如果只用一次上采样的操作，在获得大倍数(8倍以上)的上采样因子时就会比较困难。而且在不同的应用时，需要训练不同上采样倍数的模型。针对这三点问题，作者提出了LapSRN，网络结构如下图所示。 LapSRN LapSRN的结构可以看成有多级，每一级完成一次2倍的上采样操作，要实现8倍的上采样就需要有三级。在每一级中，先通过一些级联的卷积层提取特征，接着通过一个反卷积层将提取出的特征的尺寸上采样2倍。反卷积层后连有两个卷积层，一个卷积层的作用是继续提取特征，另外一个卷积层的作用是预测出这一级的残差。输入图像在每一级也经过一个反卷积层使尺寸上采样2倍，再与对应级的残差相加，就能重构出这一级的上采样结果。LapSRN设计损失函数为： 其中， 叫作Charbonnier惩罚函数( 范数的变形)， 大小设置为0.001。x表示低分辨率图像，y表示高分辨率图像，r表示残差，s表示对应的级。N表示训练时batch size的大小，L表示网络一共有多少级。通过将高分辨率图下采样，在每一级都存在有对应的ground truth进行监督，因此每一级都有一个损失，训练的时候就是要把每一级的损失的和降低。 LapSRN通过逐步上采样，一级一级预测残差的方式，在做高倍上采样时，也能得到中间低倍上采样结果的输出。由于尺寸是逐步放大，不是所有的操作都在大尺寸特征上进行，因此速度比较快。LapSRN设计了损失函数来训练网络，对每一级的结果都进行监督，因此取得了不错的结果。 github(matconvnet): https://github.com/phoenix104104/LapSRN github(pytorch): https://github.com/twtygqyy/pytorch-LapSRN github(tensorflow): https://github.com/zjuela/LapSRN-tensorflow 9. SRDenseNet (Image Super-Resolution Using Dense Skip Connections, ICCV2017) DenseNet是CVPR2017的best papaer获奖论文。DenseNet在稠密块(dense block)中将每一层的特征都输入给之后的所有层，使所有层的特征都串联(concatenate)起来，而不是像ResNet那样直接相加。这样的结构给整个网络带来了减轻梯度消失问题、加强特征传播、支持特征复用、减少参数数量的优点。一个稠密块的结构如下图所示。 dense block SRDenseNet将稠密块结构应用到了超分辨率问题上，取得了不错的效果。网络结构如下图所示。 SRDenseNet SRDenseNet可以分成四个部分。首先是用一个卷积层学习低层的特征，接着用多个稠密块学习高层的特征，然后通过几个反卷积层学到上采样滤波器参数，最后通过一个卷积层生成高分辨率输出。 文章中针对用于最后重建的输入内容不同，设计了三种结构并做了比较。一是反卷积层只输入最顶层稠密块的输出。二是添加了一个跳跃连接，将最底层卷积层的输出特征和最顶层稠密块的输出特征串联起来，再输入反卷积层。三是添加了稠密跳跃连接，就是把稠密块看成一个整体，第一个卷积层的输出以及每个稠密块的输出，都输入给在之后的所有稠密块，像是把在反卷积层之前的整个网络也设计成像稠密块那样的结构。由于这样做，所有的特征都串联起来，这样直接输入反卷积层会产生巨大的计算开销，因此添加了一个核大小为1×1的卷积层来减小特征数量，这个卷积层被称为瓶颈层。最后的结果是越复杂的越好，3>2>1。文章中分析的是，受益于低层特征和高层特征的结合，超分辨率重建的性能得到了提升。像第三种结构把所有深度层的特征都串联起来，得到了最佳的结果，说明不同深度层的特征之间包含的信息是互补的。 10. SRGAN(SRResNet) (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, CVPR2017) 在这篇文章中，将生成对抗网络(Generative Adversarial Network, GAN)用在了解决超分辨率问题上。文章提到，训练网络时用均方差作为损失函数，虽然能够获得很高的峰值信噪比，但是恢复出来的图像通常会丢失高频细节，使人不能有好的视觉感受。SRGAN利用感知损失(perceptual loss)和对抗损失(adversarial loss)来提升恢复出的图片的真实感。感知损失是利用卷积神经网络提取出的特征，通过比较生成图片经过卷积神经网络后的特征和目标图片经过卷积神经网络后的特征的差别，使生成图片和目标图片在语义和风格上更相似。一个GAN所要完成的工作，GAN原文举了个例子：生成网络(G)是印假钞的人，判别网络(D)是检测假钞的人。G的工作是让自己印出来的假钞尽量能骗过D，D则要尽可能的分辨自己拿到的钞票是银行中的真票票还是G印出来的假票票。开始的时候呢，G技术不过关，D能指出这个假钞哪里很假。G每次失败之后都认真总结经验，努力提升自己，每次都进步。直到最后，D无法判断钞票的真假……SRGAN的工作就是： G网通过低分辨率的图像生成高分辨率图像，由D网判断拿到的图像是由G网生成的，还是数据库中的原图像。当G网能成功骗过D网的时候，那我们就可以通过这个GAN完成超分辨率了。 文章中，用均方误差优化SRResNet(SRGAN的生成网络部分)，能够得到具有很高的峰值信噪比的结果。在训练好的VGG模型的高层特征上计算感知损失来优化SRGAN，并结合SRGAN的判别网络，能够得到峰值信噪比虽然不是最高，但是具有逼真视觉效果的结果。SRGAN网络结构如下图所示。 在生成网络部分(SRResNet)部分包含多个残差块，每个残差块中包含两个3×3的卷积层，卷积层后接批规范化层(batch normalization, BN)和PReLU作为激活函数，两个2×亚像素卷积层(sub-pixel convolution layers)被用来增大特征尺寸。在判别网络部分包含8个卷积层，随着网络层数加深，特征个数不断增加，特征尺寸不断减小，选取激活函数为LeakyReLU，最终通过两个全连接层和最终的sigmoid激活函数得到预测为自然图像的概率。SRGAN的损失函数为： 其中内容损失可以是基于均方误差的损失的损失函数： 也可以是基于训练好的以ReLU为激活函数的VGG模型的损失函数: i和j表示VGG19网络中第i个最大池化层(maxpooling)后的第j个卷积层得到的特征。对抗损失为： 文章中的实验结果表明，用基于均方误差的损失函数训练的SRResNet，得到了结果具有很高的峰值信噪比，但是会丢失一些高频部分细节，图像比较平滑。而SRGAN得到的结果则有更好的视觉效果。其中，又对内容损失分别设置成基于均方误差、基于VGG模型低层特征和基于VGG模型高层特征三种情况作了比较，在基于均方误差的时候表现最差，基于VGG模型高层特征比基于VGG模型低层特征的内容损失能生成更好的纹理细节。 github(tensorflow): https://github.com/zsdonghao/SRGAN github(tensorflow): https://github.com/buriburisuri/SRGAN github(torch): https://github.com/junhocho/SRGAN github(caffe): https://github.com/ShenghaiRong/caffe_srgan github(tensorflow): https://github.com/brade31919/SRGAN-tensorflow github(keras): https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks github(pytorch): ai-tor/PyTorch-SRGAN 11. EDSR (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW2017) EDSR是NTIRE2017超分辨率挑战赛上获得冠军的方案。如论文中所说，EDSR最有意义的模型性能提升是去除掉了SRResNet多余的模块，从而可以扩大模型的尺寸来提升结果质量。EDSR的网络结构如下图所示。 EDSR 可以看到，EDSR在结构上与SRResNet相比，就是把批规范化处理(batch normalization, BN)操作给去掉了。文章中说，原始的ResNet最一开始是被提出来解决高层的计算机视觉问题，比如分类和检测，直接把ResNet的结构应用到像超分辨率这样的低层计算机视觉问题，显然不是最优的。由于批规范化层消耗了与它前面的卷积层相同大小的内存，在去掉这一步操作后，相同的计算资源下，EDSR就可以堆叠更多的网络层或者使每层提取更多的特征，从而得到更好的性能表现。EDSR用L1范数样式的损失函数来优化网络模型。在训练时先训练低倍数的上采样模型，接着用训练低倍数上采样模型得到的参数来初始化高倍数的上采样模型，这样能减少高倍数上采样模型的训练时间，同时训练结果也更好。 这篇文章还提出了一个能同时不同上采样倍数的网络结构MDSR，如下图。 MDSR MDSR的中间部分还是和EDSR一样，只是在网络前面添加了不同的预训练好的模型来减少不同倍数的输入图片的差异。在网络最后，不同倍数上采样的结构平行排列来获得不同倍数的输出结果。 从文章给出的结果可以看到，EDSR能够得到很好的结果。增大模型参数数量以后，结果又有了进一步的提升。因此如果能够解决训练困难的问题，网络越深，参数越多，对提升结果确实是有帮助吧。 github(torch): https://github.com/LimBee/NTIRE2017 github(tensorflow): https://github.com/jmiller656/EDSR-Tensorflow github(pytorch): https://github.com/thstkdgus35/EDSR-PyTorch 通过以上11篇有关深度学习超分辨率方法的论文，可以看到通过网络结构、损失函数以及训练方式的演变，深度学习超分辨率方法在结果、速度以及应用性上都有了不断的提高。这里再放上一篇深度学习超分辨率方法综述的链接(Super-Resolution via Deep Learning)以及github上一个超分辨率方法的总结(https://github.com/YapengTian/Single-Image-Super-Resolution)。 非常感谢许多知乎和博客上的文章，由于比较多，这里列出参考得比较多的几个资源： https://zhuanlan.zhihu.com/p/25532538?utm_source=tuicool&utm_medium=referral http://blog.csdn.net/u011692048/article/category/7121139 http://blog.csdn.net/wangkun1340378/article/category/7004439 "},"super_resolution/code_dataset.html":{"url":"super_resolution/code_dataset.html","title":"超分辨率代码数据集合集","keywords":"","body":"Awesome-Super-Resolution（in progress） repositories repositories Awesome paper list: 图像超分辨： https://github.com/YapengTian/Single-Image-Super-Resolution 超分辨Benckmark： https://github.com/huangzehao/Super-Resolution.Benckmark 视频超分辨： https://github.com/flyywh/Video-Super-Resolution https://github.com/LoSealL/VideoSuperResolution Awesome paper list: Single-Image-Super-Resolution Super-Resolution.Benckmark Video-Super-Resolution VideoSuperResolution Awesome repos: repo Framework EDSR-PyTorch PyTorch Image-Super-Resolution Keras image-super-resolution Keras Super-Resolution-Zoo MxNet super-resolution Keras neural-enhance Theano srez Tensorflow waifu2x Torch BasicSR PyTorch super-resolution PyTorch VideoSuperResolution Tensorflow video-super-resolution Pytorch Datasets Note this table is referenced from here. Name Usage Link Comments Set5 Test download jbhuang0604 SET14 Test download jbhuang0604 BSD100 Test download jbhuang0604 Urban100 Test download jbhuang0604 Manga109 Test website SunHay80 Test download jbhuang0604 BSD300 Train/Val download BSD500 Train/Val download 91-Image Train download Yang DIV2K2017 Train/Val website NTIRE2017 Real SR Train/Val website NTIRE2019 Waterloo Train website VID4 Test download 4 videos MCL-V Train website 12 videos GOPRO Train/Val website 33 videos, deblur CelebA Train website Human faces Sintel Train/Val website Optical flow FlyingChairs Train website Optical flow Vimeo-90k Train/Test website 90k HQ videos Dataset collections Benckmark and DIV2K: Set5, Set14, B100, Urban100, Manga109, DIV2K2017 include bicubic downsamples with x2,3,4,8 SR_testing_datasets: Test: Set5, Set14, B100, Urban100, Manga109, Historical; Train: T91,General100, BSDS200 paper Non-DL based approach SCSR: TIP2010, Jianchao Yang et al.paper, code ANR: ICCV2013, Radu Timofte et al. paper, code A+: ACCV 2014, Radu Timofte et al. paper, code IA: CVPR2016, Radu Timofte et al. paper SelfExSR: CVPR2015, Jia-Bin Huang et al. paper, code NBSRF: ICCV2015, Jordi Salvador et al. paper RFL: ICCV2015, Samuel Schulter et al paper, code DL based approach Note this table is referenced from here Model Published Code Keywords SRCNN ECCV14 Keras Kaiming RAISR arXiv - Google, Pixel 3 ESPCN CVPR16 Keras Real time/SISR/VideoSR VDSR CVPR16 Matlab Deep, Residual DRCN CVPR16 Matlab Recurrent DRRN CVPR17 Caffe, PyTorch Recurrent LapSRN CVPR17 Matlab Huber loss IRCNN CVPR17 Matlab EDSR CVPR17 PyTorch NTIRE17 Champion BTSRN CVPR17 - NTIRE17 SelNet CVPR17 - NTIRE17 TLSR CVPR17 - NTIRE17 SRGAN CVPR17 Tensorflow 1st proposed GAN VESPCN CVPR17 - VideoSR MemNet ICCV17 Caffe SRDenseNet ICCV17 -, PyTorch Dense SPMC ICCV17 Tensorflow VideoSR EnhanceNet ICCV17 TensorFlow Perceptual Loss PRSR ICCV17 TensorFlow an extension of PixelCNN AffGAN ICLR17 - MS-LapSRN TPAMI18 Matlab Fast LapSRN DCSCN arXiv Tensorflow IDN CVPR18 Caffe Fast DSRN CVPR18 TensorFlow Dual state，Recurrent RDN CVPR18 Torch Deep, BI-BD-DN SRMD CVPR18 Matlab Denoise/Deblur/SR DBPN CVPR18 PyTorch NTIRE18 Champion WDSR CVPR18 PyTorch，TensorFlow NTIRE18 Champion ProSRN CVPR18 PyTorch NTIRE18 ZSSR CVPR18 Tensorflow Zero-shot FRVSR CVPR18 PDF VideoSR DUF CVPR18 Tensorflow VideoSR TDAN arXiv - VideoSR，Deformable Align SFTGAN CVPR18 PyTorch CARN ECCV18 PyTorch Lightweight RCAN ECCV18 PyTorch Deep, BI-BD-DN MSRN ECCV18 PyTorch SRFeat ECCV18 Tensorflow GAN ESRGAN ECCV18 PyTorch PRIM18 region 3 Champion FEQE ECCV18 Tensorflow Fast NLRN NIPS18 Tensorflow Non-local, Recurrent SRCliqueNet NIPS18 - Wavelet CBDNet arXiv Matlab Blind-denoise TecoGAN arXiv Tensorflow VideoSR GAN RBPN CVPR19 PyTorch VideoSR SRFBN CVPR19 PyTorch Feedback MoreMNAS arXiv - Lightweight，NAS FALSR arXiv TensorFlow Lightweight，NAS Meta-SR arXiv Arbitrary Magnification AWSRN arXiv PyTorch Lightweight OISR CVPR19 PyTorch ODE-inspired Network DPSR CVPR19 PyTorch DNI CVPR19 PyTorch MAANet arXiv Multi-view Aware Attention RNAN ICLR19 PyTorch Residual Non-local Attention FSTRN CVPR19 - VideoSR, fast spatio-temporal residual block MsDNN arXiv TensorFlow NTIRE19 real SR 21th place Super Resolution survey： [1] Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang, Jing-Hao Xue. Deep Learning for Single Image Super-Resolution: A Brief Review. arxiv, 2018. paper [2]Saeed Anwar, Salman Khan, Nick Barnes. A Deep Journey into Super-resolution: A survey. arxiv, 2019.paper Super-Resolution.Benckmark A curated list of super-resolution resources and a benchmark for single image super-resolution algorithms. See my implementated super-resolution algorithms: SRGAN VDSR CSCN TODO Build a benckmark like SelfExSR_Code State-of-the-art algorithms Classical Sparse Coding Method ScSR [Web] Image super-resolution as sparse representation of raw image patches (CVPR2008), Jianchao Yang et al. Image super-resolution via sparse representation (TIP2010), Jianchao Yang et al. Coupled dictionary training for image super-resolution (TIP2011), Jianchao Yang et al. Anchored Neighborhood Regression Method ANR [Web] Anchored Neighborhood Regression for Fast Example-Based Super-Resolution (ICCV2013), Radu Timofte et al. A+ [Web] A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution (ACCV2014), Radu Timofte et al. IA [Web] Seven ways to improve example-based single image super resolution (CVPR2016), Radu Timofte et al. Self-Exemplars SelfExSR [Web] Single Image Super-Resolution from Transformed Self-Exemplars (CVPR2015), Jia-Bin Huang et al. Bayes NBSRF [Web] Naive Bayes Super-Resolution Forest (ICCV2015), Jordi Salvador et al. Deep Learning Method SRCNN [Web] [waifu2x by nagadomi] Image Super-Resolution Using Deep Convolutional Networks (ECCV2014), Chao Dong et al. Image Super-Resolution Using Deep Convolutional Networks (TPAMI2015), Chao Dong et al. CSCN [Web] Deep Networks for Image Super-Resolution with Sparse Prior (ICCV2015), Zhaowen Wang et al. Robust Single Image Super-Resolution via Deep Networks with Sparse Prior (TIP2016), Ding Liu et al. VDSR [Web] [Unofficial Implementation in Caffe] Accurate Image Super-Resolution Using Very Deep Convolutional Networks (CVPR2016), Jiwon Kim et al. DRCN [Web] Deeply-Recursive Convolutional Network for Image Super-Resolution (CVPR2016), Jiwon Kim et al. ESPCN [PDF] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (CVPR2016), Wenzhe Shi et al. Is the deconvolution layer the same as a convolutional layer? [PDF] Checkerboard artifact free sub-pixel convolution [PDF] FSRCNN [Web] Acclerating the Super-Resolution Convolutional Neural Network (ECCV2016), Dong Chao et al. LapSRN [Web] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017), Wei-Sheng Lai et al. EDSR [PDF] Enhanced Deep Residual Networks for Single Image Super-Resolution (Winner of NTIRE2017 Super-Resolution Challenge), Bee Lim et al. Perceptual Loss and GAN Perceptual Loss [PDF] Perceptual Losses for Real-Time Style Transfer and Super-Resolution (ECCV2016), Justin Johnson et al. SRGAN [PDF] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (CVPR2017), Christian Ledig et al. AffGAN [PDF] AMORTISED MAP INFERENCE FOR IMAGE SUPER-RESOLUTION (ICLR2017), Casper Kaae Sønderby et al. EnhanceNet [PDF] EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis, Mehdi S. M. Sajjadi et al. neural-enchance [Github] Video SR VESPCN [PDF] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation (CVPR2017), Jose Caballero et al. Dicussion Deconvolution and Sub-Pixel Convolution Deconvolution and Checkerboard Artifacts SubPixel Datasets Test Dataset Image source Set 5 Bevilacqua et al. BMVC 2012 Set 14 Zeyde et al. LNCS 2010 BSD 100 Martin et al. ICCV 2001 Urban 100 Huang et al. CVPR 2015 Train Dataset Image source Yang 91 Yang et al. CVPR 2008 BSD 200 Martin et al. ICCV 2001 General 100 Dong et al. ECCV 2016 ImageNet Olga Russakovsky et al. IJCV 2015 COCO Tsung-Yi Lin et al. ECCV 2014 Quantitative comparisons Results from papers of VDSR, DRCN, CSCN and IA. Note: IA use enchanced prediction trick to improve result. Results on Set 5 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 33.66/0.9929 36.54/0.9544 36.66/0.9542 36.49/0.9537 36.93/0.9552 37.53/0.9587 37.63/0.9588 37.39/ 3x - PSNR/SSIM 30.39/0.8682 32.59/0.9088 32.75/0.9090 32.58/0.9093 33.10/0.9144 33.66/0.9213 33.82/0.9226 33.46/ 4x - PSNR/SSIM 28.42/0.8104 30.28/0.8603 30.48/0.8628 30.31/0.8619 30.86/0.8732 31.35/0.8838 31.53/0.8854 31.10/ Results on Set 14 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 30.24/0.8688 32.28/0.9056 32.42/0.9063 32.22/0.9034 32.56/0.9074 33.03/0.9124 33.04/0.9118 32.87/ 3x - PSNR/SSIM 27.55/0.7742 29.13/0.8188 29.28/0.8209 29.16/0.8196 29.41/0.8238 29.77/0.8314 29.76/0.8311 29.69/ 4x - PSNR/SSIM 26.00/0.7027 27.32/0.7491 27.49/0.7503 27.40/0.7518 27.64/0.7587 28.01/0.7674 28.02/0.7670 27.88/ Results on BSD 100 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 29.56/0.8431 31.21/0.8863 31.36/0.8879 31.18/0.8855 31.40/0.8884 31.90/0.8960 31.85/0.8942 31.79/ 3x - PSNR/SSIM 27.21/0.7385 28.29/0.7835 28.41/0.7863 28.29/0.7840 28.50/0.7885 28.82/0.7976 28.80/0.7963 28.76/ 4x - PSNR/SSIM 25.96/0.6675 26.82/0.7087 "},"super_resolution/baseline.html":{"url":"super_resolution/baseline.html","title":"超分辨率baseline","keywords":"","body":"Super-Resolution.Benckmark A curated list of super-resolution resources and a benchmark for single image super-resolution algorithms. See my implementated super-resolution algorithms: SRGAN VDSR CSCN TODO Build a benckmark like SelfExSR_Code State-of-the-art algorithms Classical Sparse Coding Method ScSR [Web] Image super-resolution as sparse representation of raw image patches (CVPR2008), Jianchao Yang et al. Image super-resolution via sparse representation (TIP2010), Jianchao Yang et al. Coupled dictionary training for image super-resolution (TIP2011), Jianchao Yang et al. Anchored Neighborhood Regression Method ANR [Web] Anchored Neighborhood Regression for Fast Example-Based Super-Resolution (ICCV2013), Radu Timofte et al. A+ [Web] A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution (ACCV2014), Radu Timofte et al. IA [Web] Seven ways to improve example-based single image super resolution (CVPR2016), Radu Timofte et al. Self-Exemplars SelfExSR [Web] Single Image Super-Resolution from Transformed Self-Exemplars (CVPR2015), Jia-Bin Huang et al. Bayes NBSRF [Web] Naive Bayes Super-Resolution Forest (ICCV2015), Jordi Salvador et al. Deep Learning Method SRCNN [Web] [waifu2x by nagadomi] Image Super-Resolution Using Deep Convolutional Networks (ECCV2014), Chao Dong et al. Image Super-Resolution Using Deep Convolutional Networks (TPAMI2015), Chao Dong et al. CSCN [Web] Deep Networks for Image Super-Resolution with Sparse Prior (ICCV2015), Zhaowen Wang et al. Robust Single Image Super-Resolution via Deep Networks with Sparse Prior (TIP2016), Ding Liu et al. VDSR [Web] [Unofficial Implementation in Caffe] Accurate Image Super-Resolution Using Very Deep Convolutional Networks (CVPR2016), Jiwon Kim et al. DRCN [Web] Deeply-Recursive Convolutional Network for Image Super-Resolution (CVPR2016), Jiwon Kim et al. ESPCN [PDF] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (CVPR2016), Wenzhe Shi et al. Is the deconvolution layer the same as a convolutional layer? [PDF] Checkerboard artifact free sub-pixel convolution [PDF] FSRCNN [Web] Acclerating the Super-Resolution Convolutional Neural Network (ECCV2016), Dong Chao et al. LapSRN [Web] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017), Wei-Sheng Lai et al. EDSR [PDF] Enhanced Deep Residual Networks for Single Image Super-Resolution (Winner of NTIRE2017 Super-Resolution Challenge), Bee Lim et al. Perceptual Loss and GAN Perceptual Loss [PDF] Perceptual Losses for Real-Time Style Transfer and Super-Resolution (ECCV2016), Justin Johnson et al. SRGAN [PDF] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (CVPR2017), Christian Ledig et al. AffGAN [PDF] AMORTISED MAP INFERENCE FOR IMAGE SUPER-RESOLUTION (ICLR2017), Casper Kaae Sønderby et al. EnhanceNet [PDF] EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis, Mehdi S. M. Sajjadi et al. neural-enchance [Github] Video SR VESPCN [PDF] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation (CVPR2017), Jose Caballero et al. Dicussion Deconvolution and Sub-Pixel Convolution Deconvolution and Checkerboard Artifacts SubPixel Datasets Test Dataset Image source Set 5 Bevilacqua et al. BMVC 2012 Set 14 Zeyde et al. LNCS 2010 BSD 100 Martin et al. ICCV 2001 Urban 100 Huang et al. CVPR 2015 Train Dataset Image source Yang 91 Yang et al. CVPR 2008 BSD 200 Martin et al. ICCV 2001 General 100 Dong et al. ECCV 2016 ImageNet Olga Russakovsky et al. IJCV 2015 COCO Tsung-Yi Lin et al. ECCV 2014 Quantitative comparisons Results from papers of VDSR, DRCN, CSCN and IA. Note: IA use enchanced prediction trick to improve result. Results on Set 5 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 33.66/0.9929 36.54/0.9544 36.66/0.9542 36.49/0.9537 36.93/0.9552 37.53/0.9587 37.63/0.9588 37.39/ 3x - PSNR/SSIM 30.39/0.8682 32.59/0.9088 32.75/0.9090 32.58/0.9093 33.10/0.9144 33.66/0.9213 33.82/0.9226 33.46/ 4x - PSNR/SSIM 28.42/0.8104 30.28/0.8603 30.48/0.8628 30.31/0.8619 30.86/0.8732 31.35/0.8838 31.53/0.8854 31.10/ Results on Set 14 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 30.24/0.8688 32.28/0.9056 32.42/0.9063 32.22/0.9034 32.56/0.9074 33.03/0.9124 33.04/0.9118 32.87/ 3x - PSNR/SSIM 27.55/0.7742 29.13/0.8188 29.28/0.8209 29.16/0.8196 29.41/0.8238 29.77/0.8314 29.76/0.8311 29.69/ 4x - PSNR/SSIM 26.00/0.7027 27.32/0.7491 27.49/0.7503 27.40/0.7518 27.64/0.7587 28.01/0.7674 28.02/0.7670 27.88/ Results on BSD 100 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 29.56/0.8431 31.21/0.8863 31.36/0.8879 31.18/0.8855 31.40/0.8884 31.90/0.8960 31.85/0.8942 31.79/ 3x - PSNR/SSIM 27.21/0.7385 28.29/0.7835 28.41/0.7863 28.29/0.7840 28.50/0.7885 28.82/0.7976 28.80/0.7963 28.76/ 4x - PSNR/SSIM 25.96/0.6675 26.82/0.7087 26.90/0.7101 26.84/0.7106 27.03/0.7161 27.29/0.7251 27.23/0.7233 27.25/ "},"super_resolution/loss.html":{"url":"super_resolution/loss.html","title":"超分辨率的损失函数总结","keywords":"","body":"超分辨率的损失函数总结 MSE，是图像空间的内容“相似”，而在图像上普遍存在区域，其属于某个类别（老虎皮，草，渔网等），如果出现纹理或者网格，那么优化MSE很容易将这个区域磨平，即平滑。【直接对应评价指标MSE和MAE，以及PSNR，但是PSNR高不见得好，图像可能不自然。见文献2的图：PSNR/SSIM与感知质量(视觉效果)的一致性】 PSNR/SSIM与感知质量(视觉效果)的一致性 L1，可忍受异常值，相较于MSE和L2是没有那么平滑一些的。 Perceptual loss，是特征空间的类别/纹理“相似”。毕竟有学者认为深度卷积网络用于图像分类，利用的是物体的纹理差异。 多尺度(MS)-SSIM，那就是图像空间的结构“相似”。在文献[1]中，就找到了MS-SSIM+L1的混合损失适合于图像复原。 图像SR问题有两个主要方向，见文献[2]的图。其一为了实现更好的图像重建效果，即灰度/RGB复原；其二为了实现更好的视觉质量，即看起来“自然”。有没有方法是兼顾两者的呢？似乎。。。 图像SR的两个方向 [1] Loss Functions for Image Restoration with Neural Networks IEEE TCI 2017 [[paper]] [[code]] [2] 2018 PIRM Challenge on Perceptual Image Super-resolution [[paper]] "},"data/":{"url":"data/","title":"图片和数据处理","keywords":"","body":" 图片处理 图像数据增强 数据增强 imaaug 数据增强大杀器 "},"data/picture.html":{"url":"data/picture.html","title":"图片处理","keywords":"","body":"前言：用CNN进行训练模型的时候，通常需要对图像进行处理，有时候也叫做数据增强，常见的图像处理的Python库：OpenCV、PIL、matplotlib、tensorflow等，这里用TensorFlow介绍图像处理的过程 图片处理 展示一张图片 注意需要对图像进行解码，然后进行展示，用tf.image.decode_png 先定义一个图片展示的函数代码如下： import numpy as np import tensorflow as tf import matplotlib.pyplot as plt def show_image_tensor(image_tensor): #使用交互式回话 image = image_tensor.eval() print(\"图片的大小为：{}\".format(image.shape)) if len(image.shape)==3 and image.shape[2]==1: plt.imshow(image[:,:,0],cmap=\"Greys_r\") plt.show() elif len(image.shape)==3: plt.imshow(image) plt.show() 进行图像的读取和解码，然后调用函数进行展示 #1读取、编码、展示 file_content=tf.read_file(image_path) image_tensor = tf.image.decode_png(file_content,channels=3) show_image_tensor(image_tensor) 结果如下： 图片的大小为：(512, 512, 3) 修改大小，压缩或者放大 用tf.image.resize_images \"\"\" BILINEAR = 0 线性插值，默认 NEAREST_NEIGHBOR = 1 最近邻插值，失真最小 BICUBIC = 2 三次插值 AREA = 3 面积插值 # images: 给定需要进行大小转换的图像对应的tensor对象，格式为：[height, width, num_channels]或者[batch, height, width, num_channels] # API返回值和images格式一样，唯一区别是height和width变化为给定的值 \"\"\" resize_image_tensor = tf.image.resize_images(images=image_tensor,size=(20,20), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) show_image_tensor(resize_image_tensor)#注意前面进行解码的时候一定要用tf.image.decode_png 结果： 图片的大小为：(20, 20, 3) 注意：当放大时候，几乎图像不失真 剪切 或者是填充用tf.image.resize_image_with_crop_or_pad # 图片重置大小，通过图片的剪切或者填充（从中间开始计算新图片的大小） corp_pad_image_tensor = tf.image.resize_image_with_crop_or_pad(image_tensor,300,300) show_image_tensor(corp_pad_image_tensor) 上述为中间位置剪切或者填充，下面介绍任意位置剪切或者填充 # 填充数据（给定位置开始填充） pad_image_tensor = tf.image.pad_to_bounding_box(image=image_tensor, offset_height=200, offset_width=50, target_height=1000,target_width=1000) # show_image_tensor(pad_image_tensor) corp_to_bounding_box_image_tensor=tf.image.crop_to_bounding_box(image=image_tensor, offset_height=20, offset_width=50, target_height=300,target_width=400) show_image_tensor(corp_to_bounding_box_image_tensor) 数据增强 当训练数据有限的时候，可以通过一些变换来从已有的训 练数据集中生成一些新的数据，来扩大训练数据。数据增强的方法有： 镜像，翻转 例如：以垂直平面为对称轴如下： 代码如下： # 上下交换 filp_up_down_image_tensor = tf.image.flip_up_down(image_tensor) # show_image_tensor(filp_up_down_image_tensor) filp_left_right_image_tensor = tf.image.flip_left_right(image_tensor) show_image_tensor(filp_left_right_image_tensor) 以水平面为对称轴如下： 转置，相当于矩阵的转置,90度转换 # 转置 transpose_image_tensor = tf.image.transpose_image(image_tensor) # show_image_tensor(transpose_image_tensor) # 旋转（90度、180度、270度....） # k*90度旋转，逆时针旋转 k_rot90_image_tensor = tf.image.rot90(image_tensor, k=4) # show_image_tensor(k_rot90_image_tensor) 颜色空间转换 注意：颜色空间的转换必须讲image的值转换为float32类型，不能使用unit8类型 图像基本格式： rgb（颜色）0-255,三个255为白色，转化为float32就是把区间变为0-1 hsv（h: 图像的色彩/色度，s:图像的饱和度，v：图像的亮度） grab（灰度） # 颜色空间的转换必须讲image的值转换为float32类型，不能使用unit8类型 float32_image_tensor = tf.image.convert_image_dtype(image_tensor, dtype=tf.float32) # show_image_tensor(float32_image_tensor) # rgb -> hsv（h: 图像的色彩/色度，s:图像的饱和度，v：图像的亮度） hsv_image_tensor= tf.image.rgb_to_hsv(float32_image_tensor) show_image_tensor(hsv_image_tensor) # hsv -> rgb rgb_image_tensor = tf.image.hsv_to_rgb(float32_image_tensor) # show_image_tensor(rgb_image_tensor) # rgb -> gray gray_image_tensor = tf.image.rgb_to_grayscale(rgb_image_tensor) show_image_tensor(gray_image_tensor) 可以从颜色空间中提取图像的轮廓信息(图像的二值化) a = gray_image_tensor b = tf.less_equal(a,0.4) # 0是黑，1是白 # condition?true:false # condition、x、y格式必须一模一样，当condition中的值为true的之后，返回x对应位置的值，否则返回y对应位置的值 # 对于a中所有大于0.4的像素值，设置为0 c = tf.where(condition=b,x=a,y=a-a) # 对于a中所有小于等于0.4的像素值，设置为1 d= tf.where(condition=b,x=c-c+1,y=c) show_image_tensor(d) 这样的方法，可以运用到车牌设别的过程中，对车牌自动进行截取。 图像调整（亮度调整，对比度调整，gammer调整，归一化操作） 亮度调整 image: RGB图像信息，设置为float类型和unit8类型的效果不一样，一般建议设置为float类型 delta: 取值范围(-1,1）之间的float类型的值，表示对于亮度的减弱或者增强的系数值 底层执行：rgb -> hsv -> h,s,v*delta -> rgb 同理还有色调和饱和度 adiust_brightness_image_tensor = tf.image.adjust_brightness(image=image_tensor, delta=-0.8) # show_image_tensor(adiust_brightness_image_tensor) # 色调调整 # image: RGB图像信息，设置为float类型和unit8类型的效果不一样，一般建议设置为float类型 # delta: 取值范围(-1,1）之间的float类型的值，表示对于色调的减弱或者增强的系数值 # 底层执行：rgb -> hsv -> h*delta,s,v -> rgb adjust_hue_image_tensor = tf.image.adjust_hue(image_tensor, delta=-0.8) # show_image_tensor(adjust_hue_image_tensor) # 饱和度调整 # image: RGB图像信息，设置为float类型和unit8类型的效果不一样，一般建议设置为float类型 # saturation_factor: 一个float类型的值，表示对于饱和度的减弱或者增强的系数值，饱和因子 # 底层执行：rgb -> hsv -> h,s*saturation_factor,v -> rgb adjust_saturation_image_tensor = tf.image.adjust_saturation(image_tensor, saturation_factor=20) show_image_tensor(adjust_saturation_image_tensor) # 对比度调整，公式：(x-mean) * contrast_factor + mean(小的更小，大的更大） adiust_contrast_image_tensor=tf.image.adjust_contrast(images=image_tensor, contrast_factor=1000) show_image_tensor(adiust_contrast_image_tensor) # 图像的gamma校正 # images: 要求必须是float类型的数据 # gamma：任意值，Oup = In * Gamma只要不是白色，都加深 adjust_gamma_image_tensor = tf.image.adjust_gamma(float32_image_tensor, gamma=10) # show_image_tensor(adjust_gamma_image_tensor) # 图像的归一化(x-mean)/adjusted_sttdev, adjusted_sttdev=max(stddev, 1.0/sqrt(image.NumElements())) # per_image_standardization_image_tensor = tf.image.per_image_standardization(image_tensor) # show_image_tensor(per_image_standardization_image_tensor) 噪音数据的加入 高斯噪声、模糊处理 # noisy_image_tensor = image_tensor + tf.cast(50 * tf.random_normal(shape=[512, 512, 3], mean=0, stddev=0.1), tf.uint8) noisy_image_tensor = image_tensor + tf.cast( tf.random_uniform(shape=[512, 512, 3], minval=60, maxval=70), tf.uint8) show_image_tensor(noisy_image_tensor) "},"data/picture_enhance.html":{"url":"data/picture_enhance.html","title":"图像数据增强","keywords":"","body":"导读 图像增强是CV领域非常常用的技术，这里找到一个非常好用的图像增强的工具，可以用于Pytorch和Keras，而且功能强大，使用简单，更重要的是可以成对的进行图像增强，简直是实战利器，有了这个，妈妈再也不用担心我的数据不够了。 Augmentor是一个Python的图像增强库。这是一个独立的库，不依赖与某个平台或某个框架，非常的方便，可以进行细粒度的增强控制，而且实现了大部分的增强技术。使用了随机的方法来构建基础的模块，用户可以把这些模块组成pipline使用。 安装 Augmentor是Python写的。还有一个Julia的版本，链接：https://github.com/Evizero/Augmentor.jl 使用pip安装： pip install Augmentor 从源码安装的话，请看编译文档。升级版本的话： pip install Augmentor --upgrade 文档 完整的文档链接: http://augmentor.readthedocs.io 快速指南和使用 Augmentor的目的是进行自动的图像增强（生成人造数据）为了扩展数据集作为机器学习算法的输入，特别是神经网络和深度学习。 这个包通过创建一个增强的pipeline，即定义一系列的操作。这些操作有比如旋转和变换，一个加一个成为一个增强的pipeline，当完成的时候，pipeline可以执行，增强之后的数据也创建成功。 开始时，需要初始化pipeline对象，指向一个文件夹。 import Augmentor p = Augmentor.Pipeline(\"/path/to/images\") 然后可以在pipeline对象中添加操作： p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10) p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5) 每个函数需要制定一个概率，用来决定是否需要对这个图像进行这个操作。 一旦你创建了pipeline，可以从中进行采样，就像这样： p.sample(10000) 这样会产生10000个增强之后的图像。默认会写到指定文件夹中的名为output的目录中，这个指定文件夹就是初始化时指定的那个。 如果你想进行一次图像的增强操作，可以使用process(): p.process() 这个函数在进行数据集缩放的时候会有用。可以创建一个pipeline，其中所有的操作的概率都设置为1，然后使用process()方法。 多线程 Augmentor (version >=0.2.1) 现在使用多线程技术来提高速度。 对于原始图像非常小的图像来说，某些pipeline可能会变慢。如果发现这种情况，可以设置multi_threaded为False。 p.sample(100, multi_threaded=False) 默认的情况下，sample()函数是使用多线程的。这个只在保存到磁盘的时候实现。生成器也会在下个版本使用多线程。 Ground Truth数据 图像可以两个一组的通过pipeline，所以ground truth的图像可以同等的进行增强。 为了并行的对原始数据进行ground truth的增强，可以使用ground_truth()方法增加一个ground truth的文件夹到pipeline中： p = Augmentor.Pipeline(\"/path/to/images\") # Point to a directory containing ground truth data. # Images with the same file names will be added as ground truth data # and augmented in parallel to the original data. p.ground_truth(\"/path/to/ground_truth_images\") # Add operations to the pipeline as normal: p.rotate(probability=1, max_left_rotation=5, max_right_rotation=5) p.flip_left_right(probability=0.5) p.zoom_random(probability=0.5, percentage_area=0.8) p.flip_top_bottom(probability=0.5) p.sample(50) 多掩模/图像增强 使用DataPipeline类 (Augmentor version >= 0.2.3)，可以对有多个相关的掩模的图像进行增强： 任意长度的图像列表都可以成组的通过pipeline，并且使用DataPipeline类同样的进行增强。这个对于ground truth图像有好几个掩模的时候非常有用。举个例子。 下面的例子中，图像和掩模包含在一个images的数据结构中，对应的标签在y中： p = Augmentor.DataPipeline(images, y) p.rotate(1, max_left_rotation=5, max_right_rotation=5) p.flip_top_bottom(0.5) p.zoom_random(1, percentage_area=0.5) augmented_images, labels = p.sample(100) DataPipeline直接返回图像，并不存储在磁盘中，也不从磁盘中读取数据。图像通过初始化直接传到DataPipeline中。images的数据结构的创建细节，可以参考https://github.com/mdbloice/Augmentor/blob/master/notebooks/Multiple-Mask-Augmentation.ipynb。 Keras和Pytorch的生成器 如果你不想将图像存储到硬盘中，可以使用生成器，generator，使用Keras的情况： g = p.keras_generator(batch_size=128) images, labels = next(g) 返回的图像的batchsize是128，还有对应的labels。Generator返回的数据是不确定的，可以用来在线生成增强的数据，用在训练神经网络中。 同样的，你可以使用Pytorch： import torchvision transforms = torchvision.transforms.Compose([ p.torch_transform(), torchvision.transforms.ToTensor(), ]) 主要功能 弹性畸变 使用弹性畸变，一张图像可以生成许多图像。 这个输入图像有一个像素宽的黑边，表明了在进行畸变的时候，没有改变尺寸，也没有在新的图像上进行任何的padding。 具体的功能可以在这里看到: 透视变换 总共有12个不同类型的透视变换。4中最常用的如下： 剩下的8种透视变换: 保持大小的旋转 默认保持原始文件大小的旋转： 对比其他软件的旋转: 保持大小的剪切 剪切的同时也会自动从剪切图像中裁剪正确的区域，所以图像中没有黑的区域或者padding。 对比普通的剪切操作： 裁剪 裁剪同样也使用了一种更加适合机器学习的方法： 随机擦除 随机擦除是一种使模型对遮挡更加鲁棒的技术。这个对使用神经网络训练物体检测的时候非常有用： 看 Pipeline.random_erasing() 文档了解更多的用法。 把操作串成Pipeline 使用几个操作，单个图像可以增强成许多的新图像，对应同样的label： 在上面的例子中，我们使用了3个操作：首先做了畸变操作，然后进行了左右的镜像，概率为0.5，最后以0.5的概率做了上下的翻转。然后从这个pipeline中采样了100次，得到了100个数据。 p.random_distortion(probability=1, grid_width=4, grid_height=4, magnitude=8) p.flip_left_right(probability=0.5) p.flip_top_bottom(probability=0.5) p.sample(100) 指南 使用生成器和Keras集成 Augmentor 可以用来替换Keras中的augmentation功能。Augmentor 可以创建一个生产器来产生增强后的图像，细节可以查看下面的notebook： 从本地文件夹中读取图像进行增强，然后使用生成器将增强的图像流送到卷积神经网络中，参见 https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras.ipynb 增强内存中的图像，使用生成器将新的图像送到Keras的网络中，参见 https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras_Array_Data.ipynb Augmentor 允许每个类定义不同的pipelines，这意味着你可以在分类问题中为不同的类别定义不同的增强策略。 例子在这里：https://github.com/mdbloice/Augmentor/blob/master/notebooks/Per_Class_Augmentation_Strategy.ipynb 完整的例子 我们可以使用一张图像来完成一个增强的任务，演示一下Augmentor的pipeline和一些功能。 首先，导入包，初始化Pipeline对象，指定一个文件夹，这个文件夹里放着你的图像。 import Augmentor p = Augmentor.Pipeline(\"/home/user/augmentor_data_tests\") 然后你可以在pipeline中添加各种操作： p.rotate90(probability=0.5) p.rotate270(probability=0.5) p.flip_left_right(probability=0.8) p.flip_top_bottom(probability=0.3) p.crop_random(probability=1, percentage_area=0.5) p.resize(probability=1.0, width=120, height=120) 操作添加完了之后，可以进行采样： p.sample(100) 其中的几个 增强的图像对边缘检测任务也许很有用 "},"data/data.html":{"url":"data/data.html","title":"数据增强","keywords":"","body":"很多实际的项目，我们都难以有充足的数据来完成任务，要保证完美的完成任务，有两件事情需要做好：(1)寻找更多的数据。(2)充分利用已有的数据进行数据增强，今天就来说说数据增强。 作者 | 言有三 编辑 | 言有三 1 什么是数据增强？ 数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。 比如上图，第1列是原图，后面3列是对第1列作一些随机的裁剪、旋转操作得来。 每张图对于网络来说都是不同的输入，加上原图就将数据扩充到原来的10倍。假如我们输入网络的图片的分辨率大小是256×256，若采用随机裁剪成224×224的方式，那么一张图最多可以产生32×32张不同的图，数据量扩充将近1000倍。虽然许多的图相似度太高，实际的效果并不等价，但仅仅是这样简单的一个操作，效果已经非凡了。 如果再辅助其他的数据增强方法，将获得更好的多样性，这就是数据增强的本质。 数据增强可以分为，有监督的数据增强和无监督的数据增强方法。其中有监督的数据增强又可以分为单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。 2 有监督的数据增强 有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。 2.1. 单样本数据增强 所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。 (1) 几何变换类 几何变换类即对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作，下面展示其中的若干个操作。 水平翻转和垂直翻转 随机旋转 随机裁剪 变形缩放 翻转操作和旋转操作，对于那些对方向不敏感的任务，比如图像分类，都是很常见的操作，在caffe等框架中翻转对应的就是mirror操作。 翻转和旋转不改变图像的大小，而裁剪会改变图像的大小。通常在训练的时候会采用随机裁剪的方法，在测试的时候选择裁剪中间部分或者不裁剪。值得注意的是，在一些竞赛中进行模型测试时，一般都是裁剪输入的多个版本然后将结果进行融合，对预测的改进效果非常明显。 以上操作都不会产生失真，而缩放变形则是失真的。 很多的时候，网络的训练输入大小是固定的，但是数据集中的图像却大小不一，此时就可以选择上面的裁剪成固定大小输入或者缩放到网络的输入大小的方案，后者就会产生失真，通常效果比前者差。 (2) 颜色变换类 上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括噪声、模糊、颜色变换、擦除、填充等等。 基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。 添加Coarse Dropout噪声 颜色变换的另一个重要变换是颜色扰动，就是在某一个颜色空间通过增加或减少某些颜色分量，或者更改颜色通道的顺序。 颜色扰动 还有一些颜色变换，本文就不再详述。 几何变换类，颜色变换类的数据增强方法细致数来还有非常多，推荐给大家一个git项目： https://github.com/aleju/imgaug 预览一下它能完成的数据增强操作吧。 2.2. 多样本数据增强 不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。 (1) SMOTE[1] SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。 类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。 SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为： 第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N； 第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式： 第三步，重复以上的步骤，直到大、小样本数量平衡。 该方法的示意图如下。 在python中，SMOTE算法已经封装到了imbalanced-learn库中，如下图为算法实现的数据增强的实例，左图为原始数据特征空间图，右图为SMOTE算法处理后的特征空间图。 (2) SamplePairing[2] SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。 经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。 尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。 (3) mixup[3] mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。 令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下 λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。 SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。 3 无监督的数据增强 无监督的数据增强方法包括两类： (1) 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN[4]。 (2) 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment[5]。 3.1 GAN 关于GAN(generative adversarial networks)，我们已经说的太多了。它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下： (1) G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。 (2) D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。 GAN的以假乱真能力就不多说了。 3.2 Autoaugmentation[5] AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下： (1) 准备16个常用的数据增强操作。 (2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。 (3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。 (4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。 (5) 经过80~100个epoch后网络开始学习到有效的sub-policies。 (6) 之后串接这5个sub-policies，然后再进行最后的训练。 总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。 而对于ImageNet中的图像分类任务，AutoAugment学习到了不使用剪切，也不完全反转颜色，因为这些变换会导致图像失真。AutoAugment学习到的是侧重于微调颜色和色相分布。 除此之外还有一些数据增强方法，篇幅有限不做过多解读，请持续关注。 4 思考 数据增强的本质是为了增强模型的泛化能力，那它与其他的一些方法比如dropout，权重衰减有什么区别？ (1) 权重衰减，dropout，stochastic depth等方法，是专门设计来限制模型的有效容量的，用于减少过拟合，这一类是显式的正则化方法。研究表明这一类方法可以提高泛化能力，但并非必要，且能力有限，而且参数高度依赖于网络结构等因素。 (2) 数据增强则没有降低网络的容量，也不增加计算复杂度和调参工程量，是隐式的规整化方法。实际应用中更有意义，所以我们常说，数据至上。 我们总是在使用有限的数据来进行模型的训练，因此数据增强操作是不可缺少的一环。从研究人员手工定义数据增强操作，到基于无监督的方法生成数据和学习增强操作的组合，这仍然是一个开放的研究领域，感兴趣的同学可以自行了解更多。 更多实战和细节，可以去我的live中阅读。 计算机视觉中数据增强原理和实践www.zhihu.com 参考文献 [1] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357. [2] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018. [3] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017. [4] Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative Adversarial Networks[J]. Advances in Neural Information Processing Systems, 2014, 3:2672-2680. [5] Cubuk E D, Zoph B, Mane D, et al. AutoAugment: Learning Augmentation Policies from Data.[J]. arXiv: Computer Vision and Pattern Recognition, 2018. 切片（crop）： def crop(image, random_crop, image_size): if image.shape[1]>image_size: sz1 = int(image.shape[1]//2) sz2 = int(image_size//2) if random_crop: diff = sz1-sz2 (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1)) else: (h, v) = (0,0) image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:] return image ############################################################################ # 函数：crop # 描述：随机裁剪图像 # # 输入：图像image, crop_size # 返回：图像image ############################################################################ def crop(image, crop_size, random_crop=True): if random_crop: # 若随机裁剪 if image.shape[1] > crop_size: sz1 = image.shape[1] // 2 sz2 = crop_size // 2 diff = sz1 - sz2 (h, v) = (np.random.randint(0, diff + 1), np.random.randint(0, diff + 1)) image = image[v:(v + crop_size), h:(h + crop_size), :] return image # 左右上下翻转 def flip(image, random_flip=True): if random_flip and np.random.choice([True, False]): image = np.fliplr(image) if random_flip and np.random.choice([True, False]): image = np.flipud(image) return image #图像旋转 def random_rotate_image(image): angle = np.random.uniform(low=-10.0, high=10.0) return misc.imrotate(image, angle, 'bicubic') ############################################################################ # 函数：rotation # 描述：随机旋转图片，增强数据，用图像边缘进行填充。 # # 输入：图像image # 返回：图像image ############################################################################ def rotation(image, random_flip=True): if random_flip and np.random.choice([True, False]): w,h = image.shape[1], image.shape[0] # 0-180随机产生旋转角度。 angle = np.random.randint(0,180) RotateMatrix = cv2.getRotationMatrix2D(center=(image.shape[1]/2, image.shape[0]/2), angle=angle, scale=0.7) # image = cv2.warpAffine(image, RotateMatrix, (w,h), borderValue=(129,137,130)) #image = cv2.warpAffine(image, RotateMatrix, (w,h),borderValue=(129,137,130)) image = cv2.warpAffine(image, RotateMatrix, (w,h),borderMode=cv2.BORDER_REPLICATE) return image 图像归一化处理： def prewhiten(x): mean = np.mean(x) std = np.std(x) std_adj = np.maximum(std, 1.0/np.sqrt(x.size)) y = np.multiply(np.subtract(x, mean), 1/std_adj) return y 图像平移： ############################################################################ # 函数：translation # 描述：随机平移图片，增强数据，用图像边缘进行填充。 # # 输入：图像image # 返回：图像image ############################################################################ def translation(image, random_flip=True): if random_flip and np.random.choice([True, False]): w,h = 1920, 1080 H1 = np.float32([[1,0],[0,1]]) H2 = np.random.uniform(50,500, [2,1]) H = np.hstack([H1, H2]) # H = np.float32([[1,0,408],[0,1,431]]) print (H) image = cv2.warpAffine(image, H, (w,h), borderMode=cv2.BORDER_REPLICATE) return image 调整光照 from skimage import exposure import numpy as np def gen_exposure(image, random_xp=True): if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 1.2) # 调暗 if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 1.5) # 调暗 if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.9) # 调亮 if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.8) # 调亮 if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.7) # 调暗 return image "},"data/imaaug.html":{"url":"data/imaaug.html","title":"imaaug 数据增强大杀器","keywords":"","body":"imgaug This python library helps you with augmenting images for your machine learning projects. It converts a set of input images into a new, much larger set of slightly altered images.   Image Heatmaps Seg. Maps Keypoints Bounding Boxes,Polygons Original Input Gauss. Noise+ Contrast+ Sharpen Affine Crop+ Pad Fliplr+ Perspective More (strong) example augmentations of one input image: Features of the library Supports both common and exotic augmentation techniques. E.g. affine transformations, perspective transformations, contrast changes, gaussian noise, dropout of regions, hue/saturation changes, cropping/padding, blurring, ... Supports augmentation of: Images (full support for uint8, for other dtypes see documentation) Heatmaps (float32) Segmentation maps (integer-based, bool, float-based) Keypoints/Landmarks (int or float coordinates) Bounding Boxes (int or float coordinates) Polygons (int or float coordinates) (Beta) LineStrings (int or float coordinates) (Beta) Can augment all of the above automatically with the same sampled values. E.g. rotate both images and the segmentation maps on them by the same random value sampled from uniform(0°, 30°). Native support for heatmaps and segmentation maps that are smaller than their corresponding images. Define flexible stochastic ranges for each augmentation parameter. E.g. \"rotate each image by a value between -45 and 45 degrees\". E.g. \"rotate each image by ABS(N(0, 20.0))*(1+B(1.0, 1.0))\", where ABS(.) is the absolute function, N(.) the gaussian distribution and B(.) the beta distribution. Offers many helper functions. E.g. for drawing heatmaps, segmentation maps, keypoints, bounding boxes and polygons. E.g. for scaling segmentation maps, average/max pooling of images/maps or for padding images to desired aspect ratios (e.g. to square them). Define your augmentation sequence once at the start of the experiment, then apply it many times. Supports augmentation on multiple CPU cores. Documentation http://imgaug.readthedocs.io/en/latest/source/examples_basics.html - Quick example code on how to use the library. http://imgaug.readthedocs.io/en/latest/source/augmenters.html - Example code for some augmentation techniques. (See also the API, which usually contains examples for each augmenter.) http://imgaug.readthedocs.io/en/latest/source/api.html - API. For tutorial jupyter notebooks, see imgaug-doc/notebooks E.g. Load and Augment an Image, Multicore Support, or working with Keypoints/Landmarks, Bounding Boxes, Polygons, Line Strings, Heatmaps, Segmentation Maps Installation The library supports python 2.7 and 3.4+. To install the library, first install all requirements: pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio Shapely Then install imgaug either via pypi (can lag behind the github version): pip install imgaug or install the latest version directly from github: pip install git+https://github.com/aleju/imgaug Alternatively, you can download the repository via git clone https://github.com/aleju/imgaug and install manually via cd imgaug && python setup.py install. To deinstall the library, just execute pip uninstall imgaug. Recent Changes 0.2.8: Improved performance, dtype support and multicore augmentation. See changelog for more details. Overview of most augmenters The images below show examples for most augmentation techniques (values written in the form (a, b) mean that a value was randomly picked from the range a ): meta Noop ChannelShuffle             arithmetic Add Add(per_channel=True) AdditiveGaussianNoise AdditiveGaussianNoise(per_channel=True) AdditiveLaplaceNoise AdditiveLaplaceNoise(per_channel=True) AdditivePoissonNoise AdditivePoissonNoise(per_channel=True) Multiply Multiply(per_channel=True) Dropout Dropout(per_channel=True) CoarseDropout(p=0.2) CoarseDropout(p=0.2, per_channel=True) ImpulseNoise SaltAndPepper Salt Pepper CoarseSaltAndPepper(p=0.2) CoarseSalt(p=0.2) CoarsePepper(p=0.2) Invert Invert(per_channel=True) JpegCompression     blend Alphawith EdgeDetect(1.0) Alphawith EdgeDetect(1.0)(per_channel=True) SimplexNoiseAlphawith EdgeDetect(1.0) FrequencyNoiseAlphawith EdgeDetect(1.0)     blur GaussianBlur AverageBlur MedianBlur BilateralBlur(sigma_color=250,sigma_space=250) MotionBlur(angle=0) MotionBlur(k=5)                 color AddToHueAndSaturation Grayscale             contrast GammaContrast GammaContrast(per_channel=True) SigmoidContrast(cutoff=0.5) SigmoidContrast(gain=10) SigmoidContrast(per_channel=True) LogContrast LogContrast(per_channel=True) LinearContrast LinearContrast(per_channel=True) AllChannels-HistogramEqualization HistogramEqualization AllChannelsCLAHE AllChannelsCLAHE(per_channel=True) CLAHE     convolutional Sharpen(alpha=1) Emboss(alpha=1) EdgeDetect DirectedEdgeDetect(alpha=1)     flip Fliplr Flipud     geometric Affine Affine: Modes     Affine: cval PiecewiseAffine     PerspectiveTransform ElasticTransformation(sigma=0.2)     ElasticTransformation(sigma=5.0) Rot90     segmentation Superpixels(p_replace=1) Superpixels(n_segments=100)             size CropAndPad Crop     Pad PadToFixedSize(height'=height+32,width'=width+32)     CropToFixedSize(height'=height-32,width'=width-32)             weather FastSnowyLandscape(lightness_multiplier=2.0) Clouds Fog Snowflakes     Code Examples A standard machine learning situation. Train on batches of images and augment each batch via crop, horizontal flip (\"Fliplr\") and gaussian blur: from imgaug import augmenters as iaa seq = iaa.Sequential([ iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen) iaa.Fliplr(0.5), # horizontally flip 50% of the images iaa.GaussianBlur(sigma=(0, 3.0)) # blur images with a sigma of 0 to 3.0 ]) for batch_idx in range(1000): # 'images' should be either a 4D numpy array of shape (N, height, width, channels) # or a list of 3D numpy arrays, each having shape (height, width, channels). # Grayscale images must have shape (height, width, 1) each. # All images must have numpy's dtype uint8. Values are expected to be in # range 0-255. images = load_batch(batch_idx) # you have to implement this function images_aug = seq.augment_images(images) # done by the library train_on_images(images_aug) # you have to implement this function Apply heavy augmentations to images (used to create the image at the very top of this readme): import imgaug as ia from imgaug import augmenters as iaa import numpy as np # random example images images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases, # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image. sometimes = lambda aug: iaa.Sometimes(0.5, aug) # Define our sequence of augmentation steps that will be applied to every image # All augmenters with per_channel=0.5 will sample one value _per image_ # in 50% of all cases. In all other cases they will sample new values # _per channel_. seq = iaa.Sequential( [ # apply the following augmenters to most images iaa.Fliplr(0.5), # horizontally flip 50% of all images iaa.Flipud(0.2), # vertically flip 20% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis) rotate=(-45, 45), # rotate by -45 to +45 degrees shear=(-16, 16), # shear by -16 to +16 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation iaa.OneOf([ iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0 iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7 iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images # search either for all edges or for directed edges, # blend the result with the original image using a blobby mask iaa.SimplexNoiseAlpha(iaa.OneOf([ iaa.EdgeDetect(alpha=(0.5, 1.0)), iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)), ])), iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images iaa.OneOf([ iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2), ]), iaa.Invert(0.05, per_channel=True), # invert color channels iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.5, 1.5), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-4, 0), first=iaa.Multiply((0.5, 1.5), per_channel=True), second=iaa.ContrastNormalization((0.5, 2.0)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast iaa.Grayscale(alpha=(0.0, 1.0)), sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths) sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1))) ], random_order=True ) ], random_order=True ) images_aug = seq.augment_images(images) Quickly show example results of your augmentation sequence: from imgaug import augmenters as iaa import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) seq = iaa.Sequential([iaa.Fliplr(0.5), iaa.GaussianBlur((0, 3.0))]) # show an image with 8*8 augmented versions of image 0 seq.show_grid(images[0], cols=8, rows=8) # Show an image with 8*8 augmented versions of image 0 and 8*8 augmented # versions of image 1. The identical augmentations will be applied to # image 0 and 1. seq.show_grid([images[0], images[1]], cols=8, rows=8) Augment two batches of images in exactly the same way (e.g. horizontally flip 1st, 2nd and 5th images in both batches, but do not alter 3rd and 4th images): from imgaug import augmenters as iaa # Standard scenario: You have N RGB-images and additionally 21 heatmaps per image. # You want to augment each image and its heatmaps identically. images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) heatmaps = np.random.randint(0, 255, (16, 128, 128, 21), dtype=np.uint8) seq = iaa.Sequential([iaa.GaussianBlur((0, 3.0)), iaa.Affine(translate_px={\"x\": (-40, 40)})]) # Convert the stochastic sequence of augmenters to a deterministic one. # The deterministic sequence will always apply the exactly same effects to the images. seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start images_aug = seq_det.augment_images(images) heatmaps_aug = seq_det.augment_images(heatmaps) Augment images and landmarks/keypoints on these images: import imgaug as ia from imgaug import augmenters as iaa import random import numpy as np images = np.random.randint(0, 50, (4, 128, 128, 3), dtype=np.uint8) # Generate random keypoints. # The augmenters expect a list of imgaug.KeypointsOnImage. keypoints_on_images = [] for image in images: height, width = image.shape[0:2] keypoints = [] for _ in range(4): x = random.randint(0, width-1) y = random.randint(0, height-1) keypoints.append(ia.Keypoint(x=x, y=y)) keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape)) seq = iaa.Sequential([iaa.GaussianBlur((0, 3.0)), iaa.Affine(scale=(0.5, 0.7))]) seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start # augment keypoints and images images_aug = seq_det.augment_images(images) keypoints_aug = seq_det.augment_keypoints(keypoints_on_images) # Example code to show each image and print the new keypoints coordinates for img_idx, (image_before, image_after, keypoints_before, keypoints_after) in enumerate(zip(images, images_aug, keypoints_on_images, keypoints_aug)): image_before = keypoints_before.draw_on_image(image_before) image_after = keypoints_after.draw_on_image(image_after) ia.imshow(np.concatenate((image_before, image_after), axis=1)) # before and after for kp_idx, keypoint in enumerate(keypoints_after.keypoints): keypoint_old = keypoints_on_images[img_idx].keypoints[kp_idx] x_old, y_old = keypoint_old.x, keypoint_old.y x_new, y_new = keypoint.x, keypoint.y print(\"[Keypoints for image #%d] before aug: x=%d y=%d | after aug: x=%d y=%d\" % (img_idx, x_old, y_old, x_new, y_new)) Apply single augmentations to images: from imgaug import augmenters as iaa import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) flipper = iaa.Fliplr(1.0) # always horizontally flip each input image images[0] = flipper.augment_image(images[0]) # horizontally flip image 0 vflipper = iaa.Flipud(0.9) # vertically flip each input image with 90% probability images[1] = vflipper.augment_image(images[1]) # probably vertically flip image 1 blurer = iaa.GaussianBlur(3.0) images[2] = blurer.augment_image(images[2]) # blur image 2 by a sigma of 3.0 images[3] = blurer.augment_image(images[3]) # blur image 3 by a sigma of 3.0 too translater = iaa.Affine(translate_px={\"x\": -16}) # move each input image by 16px to the left images[4] = translater.augment_image(images[4]) # move image 4 to the left scaler = iaa.Affine(scale={\"y\": (0.8, 1.2)}) # scale each input image to 80-120% on the y axis images[5] = scaler.augment_image(images[5]) # scale image 5 by 80-120% on the y axis Apply an augmenter to only specific image channels: from imgaug import augmenters as iaa import numpy as np # fake RGB images images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # add a random value from the range (-30, 30) to the first two channels of # input images (e.g. to the R and G channels) aug = iaa.WithChannels( channels=[0, 1], children=iaa.Add((-30, 30)) ) images_aug = aug.augment_images(images) You can use more unusual distributions for the stochastic parameters of each augmenter: from imgaug import augmenters as iaa from imgaug import parameters as iap import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # Blur by a value sigma which is sampled from a uniform distribution # of range 0.1 Images can be augmented in background processes using the method augment_batches(batches, background=True), where batches is expected to be a list of image batches or a list of batches/lists of imgaug.KeypointsOnImage or a list of imgaug.Batch. The following example augments a list of image batches in the background: import imgaug as ia from imgaug import augmenters as iaa import numpy as np from skimage import data # Number of batches and batch size for this example nb_batches = 10 batch_size = 32 # Example augmentation sequence to run in the background augseq = iaa.Sequential([ iaa.Fliplr(0.5), iaa.CoarseDropout(p=0.1, size_percent=0.1) ]) # For simplicity, we use the same image here many times astronaut = data.astronaut() astronaut = ia.imresize_single_image(astronaut, (64, 64)) # Make batches out of the example image (here: 10 batches, each 32 times # the example image) batches = [] for _ in range(nb_batches): batches.append( np.array( [astronaut for _ in range(batch_size)], dtype=np.uint8 ) ) # Show the augmented images. # Note that augment_batches() returns a generator. for images_aug in augseq.augment_batches(batches, background=True): ia.imshow(ia.draw_grid(images_aug, cols=8)) If you need a bit more control over the background augmentation process, you can work with augmenter.pool(), which allows you to define how many CPU cores to use, how often to restart child workers, which random number seed to use and how large the chunks of data transferred to each child worker should be. import numpy as np import imgaug as ia from imgaug import augmenters as iaa # Basic augmentation sequence. PiecewiseAffine is slow and therefore well suited # for augmentation on multiple CPU cores. aug = iaa.Sequential([ iaa.Fliplr(0.5), iaa.PiecewiseAffine((0.0, 0.1)) ]) # generator that yields images def create_image_generator(nb_batches, size): for _ in range(nb_batches): # Add e.g. keypoints=... or bounding_boxes=... here to also augment # keypoints / bounding boxes on these images. yield ia.Batch( images=np.random.randint(0, 255, size=size).astype(np.uint8) ) # 500 batches of images, each containing 10 images of size 128x128x3 my_generator = create_image_generator(500, (10, 128, 128, 3)) # Start a pool to augment on multiple CPU cores. # * processes=-1 means that all CPU cores except one are used for the # augmentation, so one is kept free to move data to the GPU # * maxtasksperchild=20 restarts child workers every 20 tasks -- only use this # if you encounter problems such as memory leaks. Restarting child workers # decreases performance. # * seed=123 makes the result of the whole augmentation process deterministic # between runs of this script, i.e. reproducible results. with aug.pool(processes=-1, maxtasksperchild=20, seed=123) as pool: # Augment on multiple CPU cores. # * The result of imap_batches() is also a generator. # * Use map_batches() if your input is a list. # * chunksize=10 controls how much data to send to each child worker per # transfer, set it higher for better performance. batches_aug_generator = pool.imap_batches(my_generator, chunksize=10) for i, batch_aug in enumerate(batches_aug_generator): # show first augmented image in first batch if i == 0: ia.imshow(batch_aug.images_aug[0]) # do something else with the batch here You can dynamically deactivate augmenters in an already defined sequence: import imgaug as ia from imgaug import augmenters as iaa import numpy as np # images and heatmaps, just arrays filled with value 30 images = np.ones((16, 128, 128, 3), dtype=np.uint8) * 30 heatmaps = np.ones((16, 128, 128, 21), dtype=np.uint8) * 30 # add vertical lines to see the effect of flip images[:, 16:128-16, 120:124, :] = 120 heatmaps[:, 16:128-16, 120:124, :] = 120 seq = iaa.Sequential([ iaa.Fliplr(0.5, name=\"Flipper\"), iaa.GaussianBlur((0, 3.0), name=\"GaussianBlur\"), iaa.Dropout(0.02, name=\"Dropout\"), iaa.AdditiveGaussianNoise(scale=0.01*255, name=\"MyLittleNoise\"), iaa.AdditiveGaussianNoise(loc=32, scale=0.0001*255, name=\"SomeOtherNoise\"), iaa.Affine(translate_px={\"x\": (-40, 40)}, name=\"Affine\") ]) # change the activated augmenters for heatmaps, # we only want to execute horizontal flip, affine transformation and one of # the gaussian noises def activator_heatmaps(images, augmenter, parents, default): if augmenter.name in [\"GaussianBlur\", \"Dropout\", \"MyLittleNoise\"]: return False else: # default value for all other augmenters return default hooks_heatmaps = ia.HooksImages(activator=activator_heatmaps) seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start images_aug = seq_det.augment_images(images) heatmaps_aug = seq_det.augment_images(heatmaps, hooks=hooks_heatmaps) List of augmenters The following is a list of available augmenters. Note that most of the below mentioned variables can be set to ranges, e.g. A=(0.0, 1.0) to sample a random value between 0 and 1.0 per image, or A=[0.0, 0.5, 1.0] to sample randomly either 0.0 or 0.5 or 1.0 per image. arithmetic Augmenter Description Add(V, PCH) Adds value V to each image. If PCH is true, then the sampled values may be different per channel. AddElementwise(V, PCH) Adds value V to each pixel. If PCH is true, then the sampled values may be different per channel (and pixel). AdditiveGaussianNoise(L, S, PCH) Adds white/gaussian noise pixelwise to an image. The noise comes from the normal distribution N(L,S). If PCH is true, then the sampled values may be different per channel (and pixel). AdditiveLaplaceNoise(L, S, PCH) Adds noise sampled from a laplace distribution following Laplace(L, S) to images. If PCH is true, then the sampled values may be different per channel (and pixel). AdditivePoissonNoise(L, PCH) Adds noise sampled from a poisson distribution with L being the lambda exponent. If PCH is true, then the sampled values may be different per channel (and pixel). Multiply(V, PCH) Multiplies each image by value V, leading to darker/brighter images. If PCH is true, then the sampled values may be different per channel. MultiplyElementwise(V, PCH) Multiplies each pixel by value V, leading to darker/brighter pixels. If PCH is true, then the sampled values may be different per channel (and pixel). Dropout(P, PCH) Sets pixels to zero with probability P. If PCH is true, then channels may be treated differently, otherwise whole pixels are set to zero. CoarseDropout(P, SPX, SPC, PCH) Like Dropout, but samples the locations of pixels that are to be set to zero from a coarser/smaller image, which has pixel size SPX or relative size SPC. I.e. if SPC has a small value, the coarse map is small, resulting in large rectangles being dropped. ReplaceElementwise(M, R, PCH) Replaces pixels in an image by replacements R. Replaces the pixels identified by mask M. M can be a probability, e.g. 0.05 to replace 5% of all pixels. If PCH is true, then the mask will be sampled per image, pixel and additionally channel. ImpulseNoise(P) Replaces P percent of all pixels with impulse noise, i.e. very light/dark RGB colors. This is an alias for SaltAndPepper(P, PCH=True). SaltAndPepper(P, PCH) Replaces P percent of all pixels with very white or black colors. If PCH is true, then different pixels will be replaced per channel. CoarseSaltAndPepper(P, SPX, SPC, PCH) Similar to CoarseDropout, but instead of setting regions to zero, they are replaced by very white or black colors. If PCH is true, then the coarse replacement masks are sampled once per image and channel. Salt(P, PCH) Similar to SaltAndPepper, but only replaces with very white colors, i.e. no black colors. CoarseSalt(P, SPX, SPC, PCH) Similar to CoarseSaltAndPepper, but only replaces with very white colors, i.e. no black colors. Pepper(P, PCH) Similar to SaltAndPepper, but only replaces with very black colors, i.e. no white colors. CoarsePepper(P, SPX, SPC, PCH) Similar to CoarseSaltAndPepper, but only replaces with very black colors, i.e. no white colors. Invert(P, PCH) Inverts with probability P all pixels in an image, i.e. sets them to (1-pixel_value). If PCH is true, each channel is treated individually (leading to only some channels being inverted). ContrastNormalization(S, PCH) Changes the contrast in images, by moving pixel values away or closer to 128. The direction and strength is defined by S. If PCH is set to true, the process happens channel-wise with possibly different S. JpegCompression(C) Applies JPEG compression of strength C (value range: 0 to 100) to an image. Higher values of C lead to more visual artifacts. blend Augmenter Description Alpha(A, FG, BG, PCH) Augments images using augmenters FG and BG independently, then blends the result using alpha A. Both FG and BG default to doing nothing if not provided. E.g. use Alpha(0.9, FG) to augment images via FG, then blend the result, keeping 10% of the original image (before FG). If PCH is set to true, the process happens channel-wise with possibly different A (FG and BG are computed once per image). AlphaElementwise(A, FG, BG, PCH) Same as Alpha, but performs the blending pixel-wise using a continuous mask (values 0.0 to 1.0) sampled from A. If PCH is set to true, the process happens both pixel- and channel-wise. SimplexNoiseAlpha(FG, BG, PCH, SM, UP, I, AGG, SIG, SIGT) Similar to Alpha, but uses a mask to blend the results from augmenters FG and BG. The mask is sampled from simplex noise, which tends to be blobby. The mask is gathered in I iterations (default: 1 to 3), each iteration is combined using aggregation method AGG (default max, i.e. maximum value from all iterations per pixel). Each mask is sampled in low resolution space with max resolution SM (default 2 to 16px) and upscaled to image size using method UP (default: linear or cubic or nearest neighbour upsampling). If SIG is true, a sigmoid is applied to the mask with threshold SIGT, which makes the blobs have values closer to 0.0 or 1.0. FrequencyNoiseAlpha(E, FG, BG, PCH, SM, UP, I, AGG, SIG, SIGT) Similar to SimplexNoiseAlpha, but generates noise masks from the frequency domain. Exponent E is used to increase/decrease frequency components. High values for E pronounce high frequency components. Use values in the range -4 to 4, with -2 roughly generated cloud-like patterns. blur Augmenter Description GaussianBlur(S) Blurs images using a gaussian kernel with size S. AverageBlur(K) Blurs images using a simple averaging kernel with size K. MedianBlur(K) Blurs images using a median over neihbourhoods of size K. BilateralBlur(D, SC, SS) Blurs images using a bilateral filter with distance D (like kernel size). SC is a sigma for the (influence) distance in color space, SS a sigma for the spatial distance. MotionBlur(K, A, D, O) Blurs an image using a motion blur kernel with size K. A is the angle of the blur in degrees to the y-axis (value range: 0 to 360, clockwise). D is the blur direction (value range: -1.0 to 1.0, 1.0 is forward from the center). O is the interpolation order (O=0 is fast, O=1 slightly slower but more accurate). color Augmenter Description WithColorspace(T, F, CH) Converts images from colorspace T to F, applies child augmenters CH and then converts back from F to T. AddToHueAndSaturation(V, PCH, F, C) Adds value V to each pixel in HSV space (i.e. modifying hue and saturation). Converts from colorspace F to HSV (default is F=RGB). Selects channels C before augmenting (default is C=[0,1]). If PCH is true, then the sampled values may be different per channel. ChangeColorspace(T, F, A) Converts images from colorspace F to T and mixes with the original image using alpha A. Grayscale remains at three channels. (Fairly untested augmenter, use at own risk.) Grayscale(A, F) Converts images from colorspace F (default: RGB) to grayscale and mixes with the original image using alpha A. contrast Augmenter Description GammaContrast(G, PCH) Applies gamma contrast adjustment following I_ij' = I_ij**G', where G' is a gamma value sampled from G and I_ij a pixel (converted to 0 to 1.0 space). If PCH is true, a different G' is sampled per image and channel. SigmoidContrast(G, C, PCH) Similar to GammaContrast, but applies I_ij' = 1/(1 + exp(G' * (C' - I_ij))), where G' is a gain value sampled from G and C' is a cutoff value sampled from C. LogContrast(G, PCH) Similar to GammaContrast, but applies I_ij = G' * log(1 + I_ij), where G' is a gain value sampled from G. LinearContrast(S, PCH) Similar to GammaContrast, but applies I_ij = 128 + S' * (I_ij - 128), where S' is a strength value sampled from S. This augmenter is identical to ContrastNormalization (which will be deprecated in the future). AllChannelsHistogramEqualization() Applies standard histogram equalization to each channel of each input image. HistogramEqualization(F, T) Similar to AllChannelsHistogramEqualization, but expects images to be in colorspace F, converts to colorspace T and normalizes only an intensity-related channel, e.g. L for T=Lab (default for T) or V for T=HSV. AllChannelsCLAHE(CL, K, Kmin, PCH) Contrast Limited Adaptive Histogram Equalization (histogram equalization in small image patches), applied to each image channel with clipping limit CL and kernel size K (clipped to range [Kmin, inf)). If PCH is true, different values for CL and K are sampled per channel. CLAHE(CL, K, Kmin, F, T) Similar to HistogramEqualization, this applies CLAHE only to intensity-related channels in Lab/HSV/HLS colorspace. (Usually this works significantly better than AllChannelsCLAHE.) convolutional Augmenter Description Convolve(M) Convolves images with matrix M, which can be a lambda function. Sharpen(A, L) Runs a sharpening kernel over each image with lightness L (low values result in dark images). Mixes the result with the original image using alpha A. Emboss(A, S) Runs an emboss kernel over each image with strength S. Mixes the result with the original image using alpha A. EdgeDetect(A) Runs an edge detection kernel over each image. Mixes the result with the original image using alpha A. DirectedEdgeDetect(A, D) Runs a directed edge detection kernel over each image, which detects each from direction D (default: random direction from 0 to 360 degrees, chosen per image). Mixes the result with the original image using alpha A. flip Augmenter Description Fliplr(P) Horizontally flips images with probability P. Flipud(P) Vertically flips images with probability P. geometric Augmenter Description Affine(S, TPX, TPC, R, SH, O, CVAL, FO, M, B) Applies affine transformations to images. Scales them by S (>1=zoom in, TPX pixels or TPC percent, rotates them by R degrees and shears them by SH degrees. Interpolation happens with order O (0 or 1 are good and fast). If FO is true, the output image plane size will be fitted to the distorted image size, i.e. images rotated by 45deg will not be partially outside of the image plane. M controls how to handle pixels in the output image plane that have no correspondence in the input image plane. If M='constant' then CVAL defines a constant value with which to fill these pixels. B allows to set the backend framework (currently cv2 or skimage). AffineCv2(S, TPX, TPC, R, SH, O, CVAL, M, B) Same as Affine, but uses only cv2 as its backend. Currently does not support FO=true. Might be deprecated in the future. PiecewiseAffine(S, R, C, O, M, CVAL) Places a regular grid of points on the image. The grid has R rows and C columns. Then moves the points (and the image areas around them) by amounts that are samples from normal distribution N(0,S), leading to local distortions of varying strengths. O, M and CVAL are defined as in Affine. PerspectiveTransform(S, KS) Applies a random four-point perspective transform to the image (kinda like an advanced form of cropping). Each point has a random distance from the image corner, derived from a normal distribution with sigma S. If KS is set to True (default), each image will be resized back to its original size. ElasticTransformation(S, SM, O, CVAL, M) Moves each pixel individually around based on distortion fields. SM defines the smoothness of the distortion field and S its strength. O is the interpolation order, CVAL a constant fill value for newly created pixels and M the fill mode (see also augmenter Affine). Rot90(K, KS) Rotate images K times clockwise by 90 degrees. (This is faster than Affine.) If KS is true, the resulting image will be resized to have the same size as the original input image. meta Augmenter Description Sequential(C, R) Takes a list of child augmenters C and applies them in that order to images. If R is true (default: false), then the order is random (chosen once per batch). SomeOf(N, C, R) Applies N randomly selected augmenters from a list of augmenters C to each image. The augmenters are chosen per image. R is the same as for Sequential. N can be a range, e.g. (1, 3) in order to pick 1 to 3. OneOf(C) Identical to SomeOf(1, C). Sometimes(P, C, D) Augments images with probability P by using child augmenters C, otherwise uses D. D can be None, then only P percent of all images are augmented via C. WithColorspace(T, F, C) Transforms images from colorspace F (default: RGB) to colorspace T, applies augmenters C and then converts back to F. WithChannels(H, C) Selects from each image channels H (e.g. [0,1] for red and green in RGB images), applies child augmenters C to these channels and merges the result back into the original images. Noop() Does nothing. (Useful for validation/test.) Lambda(I, K) Applies lambda function I to images and K to keypoints. AssertLambda(I, K) Checks images via lambda function I and keypoints via K and raises an error if false is returned by either of them. AssertShape(S) Raises an error if input images are not of shape S. ChannelShuffle(P, C) Permutes the order of the color channels for P percent of all images. Shuffles by default all channels, but may restrict to a subset using C (list of channel indices). segmentation Augmenter Description Superpixels(P, N, M) Generates N superpixels of the image at (max) resolution M and resizes back to the original size. Then P percent of all superpixel areas in the original image are replaced by the superpixel. (1-P) percent remain unaltered. size Augmenter Description Resize(S, I) Resizes images to size S. Common use case would be to use S={\"height\":H, \"width\":W} to resize all images to shape HxW. H and W may be floats (e.g. resize to 50% of original size). Either H or W may be \"keep-aspect-ratio\" to define only one side's new size and resize the other side correspondingly. I is the interpolation to use (default: cubic). CropAndPad(PX, PC, PM, PCV, KS) Crops away or pads PX pixels or PC percent of pixels at top/right/bottom/left of images. Negative values result in cropping, positive in padding. PM defines the pad mode (e.g. use uniform color for all added pixels). PCV controls the color of added pixels if PM=constant. If KS is true (default), the resulting image is resized back to the original size. Pad(PX, PC, PM, PCV, KS) Shortcut for CropAndPad(), which only adds pixels. Only positive values are allowed for PX and PC. Crop(PX, PC, KS) Shortcut for CropAndPad(), which only crops away pixels. Only positive values are allowed for PX and PC (e.g. a value of 5 results in 5 pixels cropped away). PadToFixedSize(W, H, PM, PCV, POS) Pads all images up to height H and width W. PM and PCV are the same as in Pad. POS defines the position around which to pad, e.g. POS=\"center\" pads equally on all sides, POS=\"left-top\" pads only the top and left sides. CropToFixedSize(W, H, POS) Similar to PadToFixedSize, but crops down to height H and width W instead of padding. KeepSizeByResize(CH, I, IH) Applies child augmenters CH (e.g. cropping) and afterwards resizes all images back to their original size. I is the interpolation used for images, IH the interpolation used for heatmaps. weather Augmenter Description FastSnowyLandscape(LT, LM) Converts landscape images to snowy landscapes by increasing in HLS colorspace the lightness L of all pixels with L by a factor of LM. Clouds() Adds clouds of various shapes and densities to images. Can be senseful to be combined with an overlay augmenter, e.g. SimplexNoiseAlpha. Fog() Adds fog-like cloud structures of various shapes and densities to images. Can be senseful to be combined with an overlay augmenter, e.g. SimplexNoiseAlpha. CloudLayer(IM, IFE, ICS, AMIN, AMUL, ASPXM, AFE, S, DMUL) Adds a single layer of clouds to an image. IM is the mean intensity of the clouds, IFE a frequency noise exponent for the intensities (leading to non-uniform colors), ICS controls the variance of a gaussian for intensity sampling, AM is the minimum opacity of the clouds (values >0 are typical of fog), AMUL a multiplier for opacity values, ASPXM controls the minimum grid size at which to sample opacity values, AFE is a frequency noise exponent for opacity values, S controls the sparsity of clouds and DMUL is a cloud density multiplier. This interface is not final and will likely change in the future. Snowflakes(D, DU, FS, FSU, A, S) Adds snowflakes with density D, density uniformity DU, snowflake size FS, snowflake size uniformity FSU, falling angle A and speed S to an image. One to three layers of snowflakes are added, hence the values should be stochastic. SnowflakesLayer(D, DU, FS, FSU, A, S, BSF, BSL) Adds a single layer of snowflakes to an image. See augmenter Snowflakes. BSF and BSL control a gaussian blur applied to the snowflakes. "},"math/":{"url":"math/","title":"数学","keywords":"","body":" 矩阵总结 分布 仿射变换 图的基本概念 "},"math/matrix.html":{"url":"math/matrix.html","title":"矩阵总结","keywords":"","body":"对称矩阵、Hermite矩阵、正交矩阵、酉矩阵、奇异矩阵、正规矩阵、幂等矩阵 看文献的时候，经常见到各种各样矩阵，本篇总结了常见的对称矩阵、Hermite矩阵、正交矩阵、酉矩阵、奇异矩阵、正规矩阵、幂等矩阵七种矩阵的定义，作为概念备忘录吧，忘了可以随时查一下。 1、对称矩阵（文献【1】第40页） 其中上标T表示求矩阵的转置（文献【1】第38-39页） 2、Hermite矩阵（文献【2】第97页） 其中H表示求矩阵的复共轭转置：（文献【2】第96页） Hermite阵是对称阵概念的推广，对称阵针对实矩阵（矩阵元素均为实数），Hermite阵针对复矩阵。 3、正交矩阵（文献【1】第115页） 4、酉矩阵（文献【2】第102页） 类似于Hermite阵相对于对称阵，酉矩阵是正交阵概念的推广。 5、奇异矩阵（文献【1】第43页） 6、正规矩阵（文献【2】第119页） 7、幂等矩阵（文献【2】第106-107页） 参考文献： 【1】同济大学数学系 编. 工程数学线性代数[M]. 5版.高等教育出版社，2007. 【2】史荣昌, 魏丰. 矩阵分析[M]. 3版.北京：北京理工大学出版社, 2010. 作者：jbb0523 来源：CSDN 原文：https://blog.csdn.net/jbb0523/article/details/50596604 版权声明：本文为博主原创文章，转载请附上博文链接！ "},"math/distribution_show.html":{"url":"math/distribution_show.html","title":"分布","keywords":"","body":"distribution-is-all-you-need distribution-is-all-you-need is the basic distribution probability tutorial for most common distribution focused on Deep learning using python library. Overview of distribution probability conjugate means it has relationship of conjugate distributions. In Bayesian probability theory, if the posterior distributions p(θ | x) are in the same probability distribution family as the prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. Conjugate prior, wikipedia Multi-Class means that Random Varivance are more than 2. N Times means that we also consider prior probability P(X). To learn more about probability, I recommend reading [pattern recognition and machine learning, Bishop 2006]. distribution probabilities and features Uniform distribution(continuous) code Uniform distribution has same probaility value on [a, b], easy probability. Bernoulli distribution(discrete) , code - Bernoulli distribution is not considered about prior probability P(X). Therefore, if we optimize to the maximum likelihood, we will be vulnerable to overfitting. - We use **binary cross entropy** to classify binary classification. It has same form like taking a negative log of the bernoulli distribution. Binomial distribution(discrete) , code - Binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments. - Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance. Multi-Bernoulli distribution, Categorical distribution(discrete) , code - Multi-bernoulli called categorical distribution, is a probability expanded more than 2. - **cross entopy** has same form like taking a negative log of the Multi-Bernoulli distribution. Multinomial distribution(discrete) , code - The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial. Beta distribution(continuous) , code - Beta distribution is conjugate to the binomial and Bernoulli distributions. - Using conjucation, we can get the posterior distribution more easily using the prior distribution we know. - Uniform distiribution is same when beta distribution met special case(alpha=1, beta=1). Dirichlet distribution(continuous) , code - Dirichlet distribution is conjugate to the MultiNomial distributions. - If k=2, it will be Beta distribution. Gamma distribution(continuous) , code - Gamma distribution will be beta distribution, if `Gamma(a,1) / Gamma(a,1) + Gamma(b,1)` is same with `Beta(a,b)`. - The exponential distribution and chi-squared distribution are special cases of the gamma distribution. Exponential distribution(continuous) , code - Exponential distribution is special cases of the gamma distribution when alpha is 1. Gaussian distribution(continuous) , code - Gaussian distribution is a very common continuous probability distribution Normal distribution(continuous) , code - Normal distribution is standarzed Gaussian distribution, it has 0 mean and 1 std. Chi-squared distribution(continuous) , code - Chi-square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. - Chi-square distribution is special case of Beta distribution Student-t distribution(continuous) , code - The t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. Author If you would like to see the details about relationship of distribution probability, please refer to this. Tae Hwan Jung @graykode, Kyung Hee Univ CE(Undergraduate). Author Email : nlkey2022@gmail.com If you leave the source, you can use it freely. "},"math/affine_transformation.html":{"url":"math/affine_transformation.html","title":"仿射变换","keywords":"","body":"文章结构如下： \\1. 仿射变换（affine transformation） \\2. Jacobian 矩阵与行列式 2.1 Jacobian 矩阵 2.2 Jacobian 行列式 2.2.1 线性变换将面积伸缩 2.2.2 Jacobian 行列式意义 2.2.3 Jacobian 行列式用途 \\3. Jacobian 与 Hessian 1. 仿射变换（affine transformation） 设 为一 阶实矩阵， 是 维向量，定义于几何空间 的仿射变换具有下列形式： 也就是说，仿射变换由一线性变换加上一平移量构成。因为 。除非平移量 为零，仿射变换才是线性变换。仿射变换有两个特殊的性质：共线(collinearity)不变性和比例不变性，意思是 的任一直线经仿射变换的像(image)仍是一直线，而且直线上各点之间的距离比例维持不变。 证明： 设 为 中任一直线，其表达式为 ，向量 代表直线指向， 是直线上一点。令 为直线 上的三个相异点，也就是 ，此三点经仿射变换 后的新位置分别为： 令 ，新的位置可以表示成 ，所以三个点经过仿射变换后仍在同一条直线 。下面计算三点之间的距离比例： ∥p2′−p1′∥∥p3′−p2′∥=∥(t2−t1)v′∥∥(t3−t2)v′∥=∥(t2−t1)v∥∥(t3−t2)v∥=∥p2−p1∥∥p3−p2∥ \\frac{\\left\\|\\mathbf{p}_{2}^{\\prime}-\\mathbf{p}_{1}^{\\prime}\\right\\|}{\\left\\|\\mathbf{p}_{3}^{\\prime}-\\mathbf{p}_{2}^{\\prime}\\right\\|}=\\frac{\\left\\|\\left(t_{2}-t_{1}\\right) \\mathbf{v}^{\\prime}\\right\\|}{\\left\\|\\left(t_{3}-t_{2}\\right) \\mathbf{v}^{\\prime}\\right\\|}=\\frac{\\left\\|\\left(t_{2}-t_{1}\\right) \\mathbf{v}\\right\\|}{\\left\\|\\left(t_{3}-t_{2}\\right) \\mathbf{v}\\right\\|}=\\frac{\\left\\|\\mathbf{p}_{2}-\\mathbf{p}_{1}\\right\\|}{\\left\\|\\mathbf{p}_{3}-\\mathbf{p}_{2}\\right\\|} ​∥p​3​′​​−p​2​′​​∥​​∥p​2​′​​−p​1​′​​∥​​=​∥(t​3​​−t​2​​)v​′​​∥​​∥(t​2​​−t​1​​)v​′​​∥​​=​∥(t​3​​−t​2​​)v∥​​∥(t​2​​−t​1​​)v∥​​=​∥p​3​​−p​2​​∥​​∥p​2​​−p​1​​∥​​ 证毕。 2. Jacobian 矩阵与行列式 假设 是这样的一个函数，对于 维实向量 具有下列形式： 其中 是 的定义域。如果向量函数 的数学形式相当的复杂，一般可以用线性化进行简化。对于单变量函数 ，在 附近我们可以用直线 ，近似 。推理至多变量，令 为一个仿射变换，表示如下： 其中 为一个 阶实矩阵， 。下面解释如何以仿射变化 近似向量函数 ，由此衍生出 的导数矩阵，即Jacobian矩阵。 2.1 Jacobian 矩阵 我们的目标是在 的附近范围使用形式简单的仿射变换 近似 ，因此在 点，我们有 。因为 ，又有 或 ，所以 接着要找出 使得仿射变换 在 点附近最近似于 。自然地，当 ，符合最近似条件的 应使误差 更快的趋于 （零向量）。若存在一个 阶实矩阵 使得 我们说 在 可导(differentiable)。为了让 从任意方向逼近 ，我们另要求 是定义域 的一个内点(interior point)，意思是 的附近点都属于 。 下面证明若向量函数 在 点可导，则 由 唯一决定。考虑 的标准基底 ，其中 第 元素为1，其余为0。设 为一极小数使得 ，属于定义域 。如果 在 可导，就有 因为 ，可知： 上式等号左边即为偏导数 等号右边等于矩阵 的第 列，故： 这个 阶实举证称为向量函数 在 的Jacobian矩阵或导数矩阵(derivative matrix)，记为 。因此，可导函数 在 的最佳仿射近似是 设 ，则 的泰勒展开式为： 因为 ， 上式指出仿射变换的改变量 是自变量的改变量 的线性函数。Jacobian矩阵 即为线性变换矩阵。直白的说，当我们将非线性函数给予线性转换时，Jacobian矩阵就是描述该线性关系的矩阵。 举个例子：极坐标与卡氏坐标的转换： 其中 。 Jacobian矩阵计算如下： 若 是极坐标平面上的一条曲线， 是卡式坐标平面的映射曲线，使用链式法则(chain rule)， 和 具有下列关系： 从几何上说，Jacobian矩阵将极坐标平面的切向量映至卡式坐标平面的切向量。 2.2 Jacobian 行列式 2.2.1 线性变换将面积伸缩 考虑从 映射至 的线性变换： 设区域 ，且 代表 中所有点经 映射后的集合，求 的面积。 因为单位正方形可直接用两边 和 表示，即 设 为线性变换 参考标准基底的表示矩阵，就有 ，其中 ，上例中 。算出 经过 映射的像如下： 也就有： 这表明 其实就是 的列向量 和 所表示的平行四边形。二阶矩阵 的行列式绝对值为 和 所形成的平行四边形的面积，故 ，这里 表示 的面积。 我们推广一下，考虑 为二维向量 和 所形成的平行四边形，以同样的方式可得： 因此 其实为向量 和 所表示的平行四边形。设 ， 等于矩阵相乘 的两边所形成的平行四边形，可得： 其中 等于平行四边形 的面积，可得到： 这个结果表明平行四边形 经线性变换 （参考标准基底的矩阵表示为 ），面积伸缩了 倍。 如果平行四边形 在平面上移动，前述的结果依然成立吗？考虑 其中 代表平移向量。计算 各点经过映射的像： 因此可以得到 ，平移量 并不会改变面积的大小，故任然有 。 2.2.2 Jacobian 行列式意义 若 是可导函数，则Jacobian是一个 阶矩阵，因此可计算行列式。为方便说明，设 ，向量函数 将 映射至 ，则 的Jacobian行列式为： 令 表示 和 表示长方形，其中 和 都是微小的量。若 和 足够接近0，则 近似如下面向量所形成的平行四边形： 令 代表平行四边形 的面积。因为二阶行列式的行向量所形成的平行四边形面积等于行列式的绝对值。 所以，微小区域 经向量函数 映射至 ，其面积伸缩了 倍。 2.2.3 Jacobian 行列式用途 Jacobian行列式最主要的应用在多重积分的换元积分法(integration by substitution)。令 为一个连续实函数，且 和 是一对一可导函数。根据上述面积变化关系，可得到下面的变换积分公式： 3. Jacobian 与 Hessian 设 为二次可导函数， 阶实对称矩阵 称为 的Hessian。定义入下： 又 关于 的梯度(gradient) 定义为一个 维向量，其中第 个元素是 对 的一次偏导数，即： 梯度 是一个向量函数， 的Jacobian矩阵如下： 因此证明梯度 的Jacobian即为Hessian矩阵。 4. 参考 周志成：《线性代数》，国立交通大学出版社 "},"math/graph.html":{"url":"math/graph.html","title":"图的基本概念","keywords":"","body":"图的基本概念 于 2016 年 06 月 20 日 图的基本概念 有向图与无向图 完全图、稀疏图、稠密图 顶点与顶点、顶点与边的关系 顶点的度数及度序列 与顶点度数有关的概念 度序列与Havel-Hakimi定理 二部图与完全二部图 图的同构 子图与生成树 路径 连通性 权值、有向网与无向网 参考文档 有向图与无向图 图（Graph）是由顶点集合和顶点间的二元关系集合（即边的集合或弧的集合）组成的数据结构，通常可以用G（V，E）来表示。其中顶点集合（Vertext Set）和边的集合（Edge Set）分别用V（G）和E（G）表示。V（G）中的元素称为顶点（Vertex），用u、v等符号表示；顶点个数称为图的阶（Order），通常用n表示。E（G）中的元素称为边（Edge），用e等符号表示；边的个数称为图的边数（Size），通常用m表示。 例如，图1（a）所示的图可以表示为G1（V，E）。其中，顶点集合V（G1）＝｛1，2，3，4，5，6｝，集合中的元素为顶点（用序号代表，在其他图中，顶点集合中的元素也可以是其他标识顶点的符号，如字母A、B、C等）；边的集合为： E（G1）＝｛（1，2），（1，3），（2，3），（2，4），（2，5），（2，6），（3，4），（3，5），（4，5）｝ 在上述边的集合中，每个元素（u，v）为一对顶点构成的无序对（用圆括号括起来），表示与顶点u和v相关联的一条无向边（Undirected Edge），这条边没有特定的方向，因此（u，v）与（v，u）是同一条的边。如果图中所有的边都没有方向性，这种图称为无向图（Undirected Graph）。 图1（b）所示的图可以表示为G2（V，E），其中顶点集合V（G2）＝｛1，2，3，4，5，6，7｝，集合中的元素也为顶点的序号；边的集合为： E（G2）＝｛＜1，2＞，＜2，3＞，＜2，5＞，＜2，6＞，＜3，5＞，＜4，3＞，＜5，2＞，＜5，4＞，＜6，7＞｝ 在上述边的集合中，每个元素＜u，v＞为一对顶点构成的有序对（用尖括号括起来），表示从顶点u到顶点v的有向边（Directed Edge），其中u是这条有向边的起始顶点（Start Vertex），简称起点，v是这条有向边的终止顶点（End Vertex），简称终点，这条边有特定的方向，由u指向v，因此＜u，v＞与＜v，u＞是两条不同的边。例如，在图1（b）有向图G2中，＜2，5＞和＜5，2＞是两条不同的边。如果图中所有的边都是有方向性的，这种图称为有向图（Directed Graph）。有向图中的边也可以称为弧（Arc）。有向图也可以表示成D（V，A），其中A为弧的集合。 有向图的基图（Ground Graph）：忽略有向图所有边的方向，得到的无向图称为该有向图的基图。例如，图1（c）所示为图1（b）中有向图G2的基图。 说明：如果一个图中某些边具有方向性，而其他边没有方向性，这种图可以称为混合图（Mixed Graph）； 完全图、稀疏图、稠密图 许多图论算法的复杂度都与图中顶点个数n或边的数目m有关，甚至m与n×（n－1）之间的相对关系也会影响图论算法的选择。下面介绍几个与顶点个数、边的数目相关的概念。 完全图（Complete Graph）：如果无向图中任何一对顶点之间都有一条边，这种无向图称为完全图。在完全图中，阶数和边数存在关系式：m＝n×（n－1）／2。例如，图2（a）所示的无向图就是完全图。阶为n的完全图用Kn表示。例如，图2（a）所示的完全图为4阶完全图K4。 有向完全图（Directed Complete Graph）：如果有向图中任何一对顶点u和v，都存在＜u，v＞和＜v，u＞两条有向边，这种有向图称为有向完全图。在有向完全图中，阶数和边数存在关系式：m＝n×（n－1）。例如，图2（b）所示的有向图就是有向完全图。 稀疏图（Sparse Graph）：边或弧的数目相对较少（远小于n×（n－1））的图称为稀疏图。有的文献认为，边或弧的数目m＜nlog（n）的无向图或有向图，称为稀疏图。例如，图3（a）所示的无向图可以称为稀疏图。 稠密图（Dense Graph）：边或弧的数目相对较多的图（接近于完全图或有向完全图）称为稠密图。例如，图3（b）所示的无向图可以称为稠密图。 平凡图（Trivial Graph）：只有一个顶点的图，即阶n＝1的图称为平凡图。相反，阶n＞1的图称为非平凡图（Nontrivial Graph）。 零图（Null Graph）：边的集合E（G）为空的图，称为零图。 顶点与顶点、顶点与边的关系 在无向图和有向图中，顶点与顶点之间的关系，以及顶点与边的关系是通过“邻接（Adjacency）”这个概念来表示的。 在无向图G（V，E）中，如果（u，v）是E（G）中的元素，即（u，v）是图中的一条无向边，则称顶点u与顶点v互为邻接顶点（Adjacent Vertex），边（u，v）依附于（Attach To）顶点u和v，或称边（u，v）与顶点u和v相关联（Incident）。此外，称有一个共同顶点的两条不同边为邻接边（Adjacent Edge）。 例如，在图1（a）所示的无向图G1中，与顶点2相邻接的顶点有1，3，4，5，6，而依附于顶点2的边有（2，1），（2，3），（2，4），（2，5）和（2，6）。 在有向图G（V，E）中，如果＜u，v＞是E（G）中的元素，即＜u，v＞是图中的一条有向边，则称顶点u邻接到（Adjacent To）顶点v，顶点v邻接自（Adjacent From）顶点u，边＜u，v＞与顶点u和v相关联。 例如，在图1（b）所示的有向图G2中，顶点2分别邻接到顶点3，5，6，邻接自顶点1和5；有向边＜2，6＞的顶点2邻接到顶点6，顶点6邻接自顶点2；顶点2分别与边＜2，3＞，＜2，5＞，＜2，6＞，＜1，2＞和＜5，2＞相关联等。 顶点的度数及度序列 与顶点度数有关的概念 顶点的度数（Degree）：一个顶点u的度数是与它相关联的边的数目，记作deg（u）。例如，在图1（a）所示的无向图G1中，顶点2的度数为5，顶点5的度数为3等。 在有向图中，顶点的度数等于该顶点的出度与入度之和。其中，顶点u的出度（Outdegree）是以u为起始顶点的有向边（即从顶点u出发的有向边）的数目，记作od（u）；顶点u的入度（Indegree）是以u为终点的有向边（即进入到顶点u的有向边）的数目，记作id（u）。顶点u的度数：deg（u）＝od（u）＋id（u）。例如，在图1（b）所示的有向图G2中，顶点2的出度为3，入度为2，度数为：3＋2＝5。 在无向图和有向图中，边数m和所有顶点度数总和都存在如下关系。 定理1　在无向图和有向图中，所有顶点度数总和，等于边数的两倍，即： 这是因为，不管是有向图还是无向图，在统计所有顶点度数总和时，每条边都统计了两次。 偶点与奇点：为方便起见，把度数为偶数的顶点称为偶点（Even Vertex），把度数为奇数的顶点称为奇点（Odd Vertex）。 推论1　每个图都有偶数个奇点。 孤立顶点（Isolated Vertex）：度数为0的顶点，称为孤立顶点。孤立顶点不与其他任何顶点邻接。 叶（Leaf）：度数为1的顶点，称为叶顶点，也称叶顶点（Leaf Vertex）或端点（End Vertex）。其他顶点称为非叶顶点。 图G的最小度（Minimum Degree）：图G所有顶点的最小的度数，记为δ（G）。 图G的最大度（Maximum Degree）：图G所有顶点的最大的度数，记为∆（G）。 例如，图1（a）所示的无向图没有孤立顶点，顶点6为叶顶点；δ（G）＝1，∆（G）＝4；等。 度序列与Havel-Hakimi定理 度序列（Degree Sequence）：若把图G所有顶点的度数排成一个序列s，则称s为图G的度序列。例如，图1（a）所示的无向图G1的度序列为 s：2，5，4，3，3，1；或s′：1，2，3，3，4，5；或s″：5，4，3，3，2，1。 其中序列s是按顶点序号排序的，序列s′是按度数非减顺序排列的，序列s″是按度数非增顺序排列的。给定一个图，确定它的度序列很简单，但是其逆问题并不容易，即给定一个由非负整数组成的有限序列s，判断s是否是某个图的度序列。 序列是可图的（Graphic）：一个非负整数组成的有限序列如果是某个无向图的度序列，则称该序列是可图的。判定一个序列是否是可图的，有以下Havel-Hakimi定理。 定理2（Havel-Hakimi定理）　由非负整数组成的非增序列s：d1，d2，…，dn（n≥2，d1≥1）是可图的，当且仅当序列 是可图的。序列s1中有n－1个非负整数，s序列中d1后的前d1个度数（即d2～dd1＋1）减1后构成s1中的前d1个数。 例如，判断序列s：7，7，4，3，3，3，2，1是否是可图的。删除序列s的首项7，对其后的7项每项减1，得到：6，3，2，2，2，1，0。继续删除序列的首项6，对其后的6项每项减1，得到：2，1，1，1，0，-1，到这一步出现了负数。由于图中不可能存在负度数的顶点，因此该序列不是可图的。 再举一个例子，判断序列s：5，4，3，3，2，2，2，1，1，1是否是可图的。删除序列s的首项5，对其后的5项每项减1，得到：3，2，2，1，1，2，1，1，1，重新排序后为：3，2，2，2，1，1，1，1，1。继续删除序列的首项3，对其后的3项每项减1，得到：1，1，1，1，1，1，1，1。如此再陆续得到序列：1，1，1，1，1，1，0；1，1，1，1，0，0；1，1，0，0，0；0，0，0，0。由此可判定该序列是可图的。 Havel-Hakimi定理实际上给出了根据一个序列s构造图（或判定s不是可图的）的方法：把序列s按照非增顺序排序以后，其顺序为d1，d2，…，dn；度数最大的顶点设为v1，将它与度数次大的前d1个顶点之间连边，然后这个顶点就可以不管了，即在序列中删除首项d1，并把后面的d1个度数减1；再把剩下的序列重新按非增顺序排序，按照上述过程连边；……；直到建出完整的图，或出现负度数等明显不合理的情况为止。 例如，对序列s：3，3，2，2，1，1构造图，设度数从大到小的6个顶点为v1～v6。首先v1与v2、v3、v4连一条边，如图4（a）所示；剩下的序列为2，1，1，1，1。如果后面4个1对应顶点v3、v4、v5、v6，则应该在v2与v3、v2与v4之间连边，最后在v5与v6之间连边，如图4（b）所示。如果后面4个1对应顶点v5、v6、v3、v4，则应该在v2与v5、v2与v6之间连边，最后在v3与v4之间连边，如图4（c）所示。可见，由同一个可图的序列构造出来的图不一定是唯一的。 二部图与完全二部图 二部图（Bipartite Graph）：设无向图为G（V，E），它的顶点集合V包含两个没有公共元素的子集：X＝｛x1，x2，…，xs｝和Y＝｛y1，y2，…，yt｝，元素个数分别为s和t；并且xi与xj之间（1≤i，j≤s）、yl与yr之间（1≤l，r≤t）没有边连接，则称G为二部图，有的文献也称为二分图。 例如，图5（a）所示的无向图就是一个二部图。 完全二部图（Complete Bipartite Graph）：在二部图G中，如果顶点集合X中每个顶点xi与顶点集合Y中每个顶点yl都有边相连，则称G为完全二部图，记为Ks，t，s和t分别为集合X和集合Y中的顶点个数。在完全二部图Ks，t中一共有s×t条边。 例如，如图5（b）所示的K2，3和图5（c）所示的K3，3都是完全二部图。 观察图6（a）和图6（c）所示两个图，表面上看起来这两个图都不是二部图。但仔细观察，发现图6（a）中3个黑色顶点互不相邻，3个白色顶点也互不相邻，每个黑色顶点都与3个白色顶点相邻，因此图6（a）实际上也是K3，3，如图6（b）所示。同样，图6（c）中4个黑色顶点互不相邻，4个白色顶点也互不相邻，对这8个顶点进行编号后，重新画成图6（d）所示的图，发现图6（c）实际上也是一个二部图。 一个图是否为二部图，可由下面的定理判别。 定理3　一个无向图G是二部图当且仅当G中无奇数长度的回路。 图的同构 从图6可知，有些图之间看起来差别很大，比如图6（a）和图6（b）、图6（c）和图6（d），但经过改画后，它们实际上是同一个图。 又如，图7（a）和图7（b）两个图表面上看差别也很大，但是对图7（b）按照图中的顺序给每个顶点编号后发现，这两个图实际上也是同一个图。 图的同构（Isomorphism）：设有两个图G1和G2，如果这两个图区别仅在于图的画法与（或）顶点的标号方式，则称它们是同构的。意思就是说这两个图是同一个图。关于同构的严格定义，请参考其他教材。 子图与生成树 设有两个图G（V，E）和G′（V′，E′），如果V′⊆V，且E′⊆E，则称图G′是图G的子图（Subgraph）。例如，图8（a）、图8（b）所示的无向图都是图1（a）所示的无向图G1的子图，而图8（c）、图8（d）所示的有向图都是图1（b）所示的有向图G2的子图。 生成树（Spanning Tree）：一个无向连通图的生成树是它的包含所有顶点的极小连通子图，这里所谓的极小就是边的数目极小。如果图中有n个顶点，则生成树有n－1条边。一个无向连通图可能有多个生成树。 例如，图1（a）所示的无向图G1的一个生成树如图9（a）所示。为了更形象地表示这个生成树，在图9（b）中把它画成了以顶点1为根结点的树，在图9（c）中把它画成了以顶点3为根结点的树。 观察图10，其中图10（b）和图10（c）都是图10（a）的子图，这两个子图的顶点集相同，为V′＝｛2，3，4，5｝，但边集不相同。图10（b）保留了原图中V′内各顶点间的边，而在图10（c）中，原图的边（3，5）和（3，2）被去掉了。因此有必要进一步讨论子图。 设图G′（V′，E′）是图G（V，E）的子图，且对于V′中的任意两个顶点u和v，只要（u，v）是G中的边，则一定是G′中的边，此时称图G′为由顶点集合V′诱导的G的子图（Subgraph of G Induced By V′），简称为顶点诱导子图（Vertex-Induced Subgraph），记为G［V′］。根据定义，在图10中，图10（b）是由V′＝｛2，3，4，5｝诱导的子图，图10（c）和图10（d）都不是顶点诱导子图。 类似地，对于图G的一个非空的边集合E′，由边集合E′诱导的G的子图是以E′作为边集，以至少与E′中一条边关联的那些顶点构成顶点集V′，这个子图G′（V′，E′）称为是G的一个边诱导子图（Edge-Induced Subgraph），记为G［E′］。根据定义，在图10中，图10（b）、图10（c）和图10（d）都是边诱导子图。 说明：由于边必须依附于顶点而存在，所以对于“某条边属于子图，但该边某个顶点不属于子图”的情形，是没有意义的。 路径 路径是图论中一个很重要的概念。在图G（V，E）中，若从顶点vi出发，沿着一些边经过一些顶点vp1，vp2，…，vpm，到达顶点vj，则称顶点序列（vi，vp1，vp2，…，vpm，vj）为从顶点vi到顶点vj的一条路径（Path），或称为通路，其中（vi，vp1），（vp1，vp2），…，（vpm，vj）为图G中的边。如果G是有向图，则＜vi，vp1＞，＜vp1，vp2＞，…，＜vpm，vj＞为图G中的有向边。 路径长度（Length）：路径中边的数目通常称为路径的长度。 例如，在图1（a）所示的无向图G1中，顶点序列（1，2，5，4）是从顶点1到顶点4的路径，路径长度为3，其中（1，2），（2，5），（5，4）都是图G1中的边；另外，顶点序列（1，3，4）也是从顶点1到顶点4的路径，路径长度为2。 在图1（b）所示的有向图G2中，顶点序列（3，5，2，6）是从顶点3到顶点6的路径，路径长度为3，其中＜3，5＞，＜5，2＞，＜2，6＞都是图G2中的有向边；而从顶点7到顶点1没有路径。 简单路径（Simple Path）：若路径上各顶点vi，vp1，vp2，…，vpm，vj均互相不重复，则这样的路径称为简单路径。例如，在图1（a）所示的无向图G1中，路径（1，2，5，4）就是一条简单路径。 回路（Circuit）：若路径上第一个顶点vi与最后一个顶点vj重合，则称这样的路径为回路。例如，在图1中，图G1中的路径（2，3，4，5，2）和图G2中的路径（5，4，3，5）都是回路。回路也称为环（Loop）。 简单回路（Simple Circuit）：除第一个和最后一个顶点外，没有顶点重复的回路称为简单回路。简单回路也称为圈（Cycle）。长度为奇数的圈称为奇圈（Odd Cycle），长度为偶数的圈称为偶圈（Even Cycle）。 连通性 连通性也是图论中一个很重要的概念。在无向图中，若从顶点u到v有路径，则称顶点u和v是连通（Connected）的。如果无向图中任意一对顶点都是连通的，则称此图是连通图（Connected Graph）；相反，如果一个无向图不是连通图，则称为非连通图（Disconnected Graph）。 如果一个无向图不是连通的，则其极大连通子图称为连通分量（Connected Component），这里所谓的极大是指子图中包含的顶点个数极大。 例如，图1（a）所示的无向图G1就是一个连通图。在图G1中，如果去掉边（2，6），则剩下的图就是非连通的，且包含两个连通分量，一个是由顶点1、2、3、4、5组成的连通分量，另一个是由顶点6构成的连通分量。 又如，图11所示的无向图也是非连通图。其中顶点1、2、3和5构成一个连通分量，顶点4、6、7和8构成另一个连通分量。 在有向图中，若对每一对顶点u和v，既存在从u到v的路径，也存在从v到u的路径，则称此有向图为强连通图（Strongly Connected Digraph）。例如，图12（a）和图12（b）所示的有向图就是强连通图。 对于非强连通图，其极大强连通子图称为其强连通分量（Strongly Connected Component）。例如，图13（a）所示的有向图G2就是非强连通图，它包含3个强连通分量，如图13（b）所示。其中，顶点2、3、4、5构成一个强连通分量，在这个子图中，每一对顶点u和v，既存在从u到v的路径，也存在从v到u的路径；顶点1、6、8也构成一个强连通分量，顶点7自成一个强连通分量。 权值、有向网与无向网 权值（Weight）：某些图的边具有与它相关的数，称为权值。这些权值可以表示从一个顶点到另一个顶点的距离、花费的代价、所需的时间等。如果一个图，其所有边都具有权值，则称为加权图（Weighted Graph），或者称为网络（Net）。根据网络中的边是否具有方向性，又可以分为有向网（Directed Net）和无向网（Undirected Net）。网络也可以用G（V，E）表示，其中边的集合E中每个元素包含3个分量：边的两个顶点和权值。 例如，图14（a）所示的无向网可表示为G1（V，E），其中顶点集合V（G1）＝｛1，2，3，4，5，6，7｝；边的集合为： E（G1）＝｛（1，2，28），（1，6，10），（2，3，16），（2，7，14），（3，4，12），（4，5，22），（4，7，18），（5，6，25），（5，7，24）｝ 在边的集合中，每个元素的第3个分量表示该边的权值。 如图14（b）所示的有向网可以表示为G2（V，E），其中顶点集合V（G1）＝｛1，2，3，4，5，6，7｝；边的集合为： E（G2）＝｛＜1，2，12＞，＜2，4，85＞，＜3，2，43＞，＜4，3，65＞，＜5，1，58＞，＜5，2，90＞，＜5，6，19＞，＜5，7，70＞，＜6，4，24＞，＜7，6，50＞｝ 同样在边的集合中，每个元素的第3个分量也表示该边的权值。 参考文档 图论算法理论、实现及应用 第一章 图的基本概念及图的存储 Name: Email: Site: © 2013 On the Road | designed by veganshe | on FarBox to Top "},"other/":{"url":"other/","title":"其他","keywords":"","body":" 判别器训练 网络FLOPS计算 "},"other/discriminator_train.html":{"url":"other/discriminator_train.html","title":"判别器训练","keywords":"","body":"生成对抗网络GAN如果只训练一个网络会有效果么？ 作者：Chuang 链接：https://www.zhihu.com/question/276070438/answer/825816888 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 这其实是一个很有趣的问题。在实践过程中，如果把判别器（Discriminator）训练得太好了，看似能够在对抗中更加有效的拒绝生成器（Generator）生成的假样本，但是其实一样会产生诸多问题。 判别器最主要的作用就是为生成器提供下降梯度。如果判别器太差，则无法提供有效的梯度，同时判别器越好，生成器梯度消失越严重。 回顾一下，原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例： （公式1） 其中是真实样本分布，是由生成器产生的样本分布。 假设我们如果要学习得到一个最优网络，必然上式求导等于0，则会得到： (公式2) 从公式上就不难看出，此时最优的判别器，就是判断为真实图像概率对于判断为真和假的概率和的比值。当且，最优判别器就应该非常自信地给出概率0；如果说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。 在极端情况——判别器最优时，生成器的损失函数变成什么。给损失函数加上一个不依赖于生成器的项，使之变成 （公式3） 注意，它刚好是判别器损失函数的反。代入最优判别器即公式2，再进行简单的变换可以得到 (公式4) 变换成这个样子是为了引入Kullback–Leibler divergence（简称KL散度）和Jensen-Shannon divergence（简称JS散度）这两个重要的相似度衡量指标。 KL散度和JS散度： (公式5：KL散度) (公式6：JS散度) 于是公式4就可以继续写成 (公式7) 根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布与生成分布之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化和之间的JS散度。 问题就出在这个JS散度上。我们会希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将“拉向”，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略，它们的JS散度是多少呢？ 答案是，因为对于任意一个x只有四种可能： 且 且 且 且 第一种对计算JS散度无贡献，第二种情况由于重叠部分可忽略所以贡献也为0，第三种情况对公式7右边第一个项的贡献是，第四种情况与之类似，所以最终：。 换句话说，无论跟是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数，而这对于梯度下降方法意味着——梯度为0！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。 但是与不重叠或重叠部分可忽略的可能性有多大？不严谨的答案是：非常大。比较严谨的答案是：当与的支撑集（support）是高维空间中的低维流形（manifold）时，与重叠部分测度（measure）为0的概率为1。 所以其实在实践过程中，用一个很优的判别器去训练好一个网络的概率其实很小。 不然还有什么对抗的意义呢，对吧。 ヾ(≧∇≦*)ゝ 参考： 郑华滨：令人拍案叫绝的Wasserstein GANzhuanlan.zhihu.com "},"other/FLOPS.html":{"url":"other/FLOPS.html","title":"网络FLOPS计算","keywords":"","body":"torchprof A minimal dependency library for layer-by-layer profiling of Pytorch models. All metrics are derived using the PyTorch autograd profiler. Quickstart pip install torchprof import torch import torchvision import torchprof model = torchvision.models.alexnet(pretrained=False).cuda() x = torch.rand([1, 3, 224, 224]).cuda() with torchprof.Profile(model, use_cuda=True) as prof: model(x) print(prof.display(show_events=False)) # equivalent to `print(prof)` and `print(prof.display())` Module | Self CPU total | CPU total | CUDA total ---------------|----------------|-----------|----------- AlexNet | | | ├── features | | | │├── 0 | 1.956ms | 7.714ms | 7.787ms │├── 1 | 68.880us | 68.880us | 69.632us │├── 2 | 85.639us | 155.948us | 155.648us │├── 3 | 253.419us | 970.386us | 1.747ms │├── 4 | 18.919us | 18.919us | 19.584us │├── 5 | 30.910us | 54.900us | 55.296us │├── 6 | 132.839us | 492.367us | 652.192us │├── 7 | 17.990us | 17.990us | 18.432us │├── 8 | 87.219us | 310.776us | 552.544us │├── 9 | 17.620us | 17.620us | 17.536us │├── 10 | 85.690us | 303.120us | 437.248us │├── 11 | 17.910us | 17.910us | 18.400us │└── 12 | 29.239us | 51.488us | 52.288us ├── avgpool | 49.230us | 85.740us | 88.960us └── classifier | | | ├── 0 | 626.236us | 1.239ms | 1.362ms ├── 1 | 235.669us | 235.669us | 635.008us ├── 2 | 17.990us | 17.990us | 18.432us ├── 3 | 31.890us | 56.770us | 57.344us ├── 4 | 39.280us | 39.280us | 212.128us ├── 5 | 16.800us | 16.800us | 17.600us └── 6 | 38.459us | 38.459us | 79.872us To see the low level operations that occur within each layer, print the contents of prof.display(show_events=True). Module | Self CPU total | CPU total | CUDA total ------------------------------|----------------|-----------|----------- AlexNet | | | ├── features | | | │├── 0 | | | ││├── conv2d | 15.740us | 1.956ms | 1.972ms ││├── convolution | 12.000us | 1.940ms | 1.957ms ││├── _convolution | 36.590us | 1.928ms | 1.946ms ││├── contiguous | 6.600us | 6.600us | 6.464us ││└── cudnn_convolution | 1.885ms | 1.885ms | 1.906ms │├── 1 | | | ││└── relu_ | 68.880us | 68.880us | 69.632us │├── 2 | | | ││├── max_pool2d | 15.330us | 85.639us | 84.992us ││└── max_pool2d_with_indices | 70.309us | 70.309us | 70.656us │├── 3 | | | ... The original Pytorch EventList can be returned by calling raw() on the profile instance. trace, event_lists_dict = prof.raw() print(trace[2]) # Trace(path=('AlexNet', 'features', '0'), leaf=True, module=Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))) print(event_lists_dict[trace[2].path][0]) --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- Name Self CPU total % Self CPU total CPU total % CPU total CPU time avg CUDA total % CUDA total CUDA time avg Number of Calls --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- conv2d 0.80% 15.740us 100.00% 1.956ms 1.956ms 25.32% 1.972ms 1.972ms 1 convolution 0.61% 12.000us 99.20% 1.940ms 1.940ms 25.14% 1.957ms 1.957ms 1 _convolution 1.87% 36.590us 98.58% 1.928ms 1.928ms 24.99% 1.946ms 1.946ms 1 contiguous 0.34% 6.600us 0.34% 6.600us 6.600us 0.08% 6.464us 6.464us 1 cudnn_convolution 96.37% 1.885ms 96.37% 1.885ms 1.885ms 24.47% 1.906ms 1.906ms 1 --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- Self CPU time total: 1.956ms CUDA time total: 7.787ms Layers can be selected for individually using the optional paths kwarg. Profiling is ignored for all other layers. model = torchvision.models.alexnet(pretrained=False) x = torch.rand([1, 3, 224, 224]) # Layer does not have to be a leaf layer paths = [(\"AlexNet\", \"features\", \"3\"), (\"AlexNet\", \"classifier\")] with torchprof.Profile(model, paths=paths) as prof: model(x) print(prof) Module | Self CPU total | CPU total | CUDA total ---------------|----------------|-----------|----------- AlexNet | | | ├── features | | | │├── 0 | | | │├── 1 | | | │├── 2 | | | │├── 3 | 2.846ms | 11.368ms | 0.000us │├── 4 | | | │├── 5 | | | │├── 6 | | | │├── 7 | | | │├── 8 | | | │├── 9 | | | │├── 10 | | | │├── 11 | | | │└── 12 | | | ├── avgpool | | | └── classifier | 12.016ms | 12.206ms | 0.000us ├── 0 | | | ├── 1 | | | ├── 2 | | | ├── 3 | | | ├── 4 | | | ├── 5 | | | └── 6 | | | Self CPU Time vs CPU Time "}}