{"./":{"url":"./","title":"Introduction","keywords":"","body":"ä»£ç æŠ€å·§æ±‡æ€» "},"introduction/0.html":{"url":"introduction/0.html","title":"é€ç»™ç ”ä¸€å…¥å­¦çš„ä½ ä»¬â€”ç‚¼ä¸¹å¸ˆå…¥é—¨æ‰‹å†Œ","keywords":"","body":"é€ç»™å³å°†å…¥å­¦çš„ä½ ä»¬â€”ç‚¼ä¸¹å¸ˆä¿®ç‚¼æ‰‹å†Œ æ ‡ç­¾ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼š é™ˆæ‰¬ [TOC] å‰æ®µæ—¶é—´æˆ‘ä¸€ç›´åœ¨å¤–é¢æ¯”èµ›ï¼Œå¥½ä¹…æ²¡æœ‰è®¤è®¤çœŸçœŸçš„ååœ¨ç”µè„‘å‰å†™ä¸€å†™ä¸œè¥¿äº†ã€‚è¿™ä¸€å¹´ä»¥æ¥ï¼Œéšç€æˆ‘ä¸ªäººå¯¹æ·±åº¦ç¥ç»ç½‘ç»œçš„å­¦ä¹ é€æ¸æ·±å…¥ï¼Œä¹Ÿå¼€å§‹çœ‹åˆ°æœ‰æ‰€æ”¶è·ã€‚æ­£å¥½è¿™æ®µæ—¶é—´æˆ‘ä»¬å®éªŒå®¤æ–°æ¥äº†8ä½ç ”ä¸€çš„å°ä¼™ä¼´ï¼Œæˆ‘ä¹Ÿå€Ÿæ­¤åˆ†äº«ä¸€äº›æˆ‘ä¸ªäººçš„å­¦ä¹ ç»éªŒå’Œå¿ƒå¾—ç»™å¤§å®¶å§ã€‚ å…³äºæ·±åº¦å­¦ä¹ æˆ‘ä¸€ç‚¹ä¸ªäººè§è§£ æˆ‘æƒ³ï¼Œä½ ä»¬åœ¨é€‰æ‹©äº†è¿›å…¥ç»„é‡Œä¹‹å‰æƒ³å¿…æ˜¯åº”è¯¥æˆ–å¤šæˆ–å°‘æ˜¯å¬è¿‡ç°åœ¨ä»ç„¶å¾ˆç«çš„äººå·¥æ™ºèƒ½ï¼Œä¹Ÿè®¸ä½ ä»¬ä¸€æ—¶åŠä¼šå¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œè¿™æ ·çš„åè¯è¿˜è§‰å¾—å¾ˆé«˜å¤§ä¸Šï¼Œä¹Ÿè®¸ä½ è¿˜åœ¨æ€è€ƒç€ SVM å’Œ MLP åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œæˆ–è®¸æ˜¯ä¸€çŸ¥åŠè§£çš„åœ¨ç½‘ä¸Šæœç€ NLPã€CV ç­‰ç­‰ç­‰ã€‚ å…¶å®åœ¨æˆ‘çœ‹æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œå¹¶ä¸æ˜¯ä»€ä¹ˆå¾ˆå¤æ‚çš„ä¸œè¥¿ï¼Œä»–åœ¨æœ¬è´¨ä¸Šå’Œä¸€ä¸ªäºŒå…ƒå‡½æ•°å¹¶æ²¡æœ‰ä»€ä¹ˆå¤ªå¤§çš„åŒºåˆ«ï¼Œå…¶å¾ˆå¤šçš„ç®—æ³•ä¹Ÿæ¥è‡ªäºæ•°å€¼è®¡ç®—ï¼Œæ¯”å¦‚éšæœºæ¢¯åº¦ä¸‹é™æ³•SGDç­‰ç­‰ã€‚è€Œäº‹å®ä¸Šï¼Œä½ ä»¬å°†æ¥è¦åšçš„å·¥ä½œä¹Ÿè®¸ä¼šæ¯”è§£ä¸€ä¸ªäºŒå…ƒå‡½æ•°å¤æ‚å¾ˆå¤šï¼Œä½†æ˜¯å…¶æœ¬è´¨ä»ç„¶æ²¡æœ‰æ”¹å˜ï¼Œåªä¸è¿‡ä¹Ÿè®¸åœ¨ä½ ä»¬æœªæ¥ç ”ç©¶çš„å·¥ä½œä¸­å¯èƒ½é‡åˆ°çš„ç¬¬ä¸€ä¸ªé—®é¢˜å°±æ˜¯â€”æˆ‘ä¸çŸ¥é“æˆ‘çš„ç›®æ ‡åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ å†è¯´ä¸€å¥é¢˜å¤–è¯ æˆ‘è§‰å¾—åœ¨é•¿ç¯‡å¤§è®ºè¯´èµ· DeepLearning ä¹‹å‰ï¼Œç”±äºæˆ‘ä¹Ÿä¸æ˜¯ç‰¹åˆ«æ¸…æ¥šä½ ä»¬å¤§å­¦é‡Œå­¦çš„åˆ°åº•å’‹æ ·ï¼Œæˆ‘å†å•°å—¦ä¸€äº›åŸºç¡€æŠ€èƒ½ï¼Œå¤§ç¥è¿™ä¸€æ®µå°±è·³è¿‡å§â€¦â€¦ ä»£ç  pythonè¯­æ³• å¯¹äºä¸€ååˆæ ¼çš„ç‚¼ä¸¹å¸ˆæ¥è¯´ï¼Œæ‹¥æœ‰è‰¯å¥½çš„ä»£ç èƒ½åŠ›æ˜¯ç‚¼ä¸¹æˆåŠŸçš„ç¬¬ä¸€æ­¥ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä¹Ÿè®¸ä½ ä»¬å¹¶æ²¡æœ‰æ¥è§¦è¿‡ pythonï¼Œå¤§éƒ¨åˆ†å­¦æ ¡åº”è¯¥æ˜¯å­¦çš„ c è‰¹æˆ–è€… javaï¼Œè€Œåœ¨ç‚¼ä¸¹ç•Œï¼Œæˆ‘ä»¬æœ€å¸¸ä½¿ç”¨çš„è¿™æ˜¯ pythonã€‚ å¦‚ä½•å­¦ä¹  pythonï¼Ÿ åŸºæœ¬è¯­æ³•ï¼šhttp://funhacks.net/explore-python/File-Directory/text_file_io.html python çš„åŸºæœ¬è¯­æ³•ç›¸å¯¹æ¥è¯´å¾ˆç®€å•ï¼Œæˆ‘è¿™é‡Œæ¨èçœ‹FunHacks å¤§ç¥å†™çš„åŸºç¡€å…¥é—¨æ•™ç¨‹ï¼Œæˆ‘ä¸ªäººè§‰å¾—ç›¸æ¯”èœé¸Ÿå’Œå»–é›ªå³°è€å¸ˆçš„æ¥è¯´æ›´ä¸ºå®ç”¨ç®€å•ã€‚ å­¦ä¼šäº†åŸºç¡€çš„è¯­æ³•åï¼Œæˆ‘æ¨èä½ ä»¬å» letcode æˆ–è€…ç‰›å®¢ä¸Šåˆ·å‡ åé¢˜ï¼Œå®æˆ˜æ”¶æ‚‰ä»£ç è¯­æ³•ã€‚ Letcodeï¼šhttps://leetcode-cn.com/problemset/all/ ç‰›å®¢ï¼šhttps://www.nowcoder.com/ å®‰è£… python python ä¹‹æ‰€ä»¥æ·±å—ç‚¼ä¸¹å¸ˆä»¬çš„çƒ­çˆ±ï¼Œé™¤äº†å…¶ç®€æ´æ˜“æ‡‚çš„è¯­æ³•å¤–ï¼Œæ›´ç¦»ä¸å¼€å…¶å¼ºå¤§çš„æ‰©å±•æ€§å’Œå¤šå¦‚ç¹æ˜Ÿçš„ç¬¬ä¸‰æ–¹å¼€æºåº“ï¼Œä½†æ˜¯ä½œä¸ºä¸€ååˆæ¥ä¹åˆ°çš„æ–°äººï¼Œè¿™ä¹Ÿè®¸å¯¹ä½ ä»¬æ¥è¯´ç¬¬ä¸€é“åæ¥äº†â€”å¦‚ä½•é…ç½®å¥½ python ç¯å¢ƒï¼Ÿ è¿™é‡Œæˆ‘æ¨èAnacondaï¼šhttps://mirror.tuna.tsinghua.edu.cn/help/anaconda/ æ¸…åå¤§å­¦çš„æºéå¸¸å¿«ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€é”®å¼é…ç½®ç§‘å­¦æŠ€æœ¯æ¡†æ¶ï¼Œæ³¨æ„è¦æŒ‰å®˜æ–¹è¦æ±‚æ¢tunaçš„æºã€‚ IDE ç°åœ¨æœ‰äº†åŸºç¡€çš„è¯­æ³•ï¼Œæœ‰äº†åŸºç¡€çš„ python åŒ…ï¼Œæƒ³å¿…æ˜¯æŒ‰æºä¸ä½æƒ³è¦åŠ¨æ‰‹å†™ä¸€ä¸‹ä»£ç äº†å§ï¼Ÿ å¸‚é¢ä¸Šä¹Ÿæœ‰åœ¨éå¸¸çš„çš„ IDEï¼Œæˆ‘è¿™é‡Œåˆ†äº«ä¸€äº›æˆ‘ä¸ªäººç”¨è¿‡è§‰å¾—æ¯”è¾ƒå¥½çš„ IDEã€‚ MAC ç³»ç»Ÿï¼šCodeRunnerï¼š CodeRunnerï¼šhttps://coderunnerapp.com/ å³å¼€å³ç”¨ï¼Œåº”è¯¥æ˜¯ä¸ç”¨å†™æ•™ç¨‹å°±èƒ½çœ‹å¾—æ‡‚å§â€¦â€¦ windows&Linux ä¸‹ï¼šsublime sublimeï¼šhttps://www.sublimetext.com/ éœ€è¦å®‰è£…æ’ä»¶åŒ…ï¼šhttps://www.4spaces.org/how-to-install-package-control-on-sublime-text/ å¥½ç”µè„‘ï¼špycharm pycharmï¼šhttps://www.jetbrains.com/pycharm/ åº”è¯¥æ˜¯æœ€å¥½ç”¨çš„ python IDEäº†å§ï¼Œä¸è¿‡ä¹Ÿæœ‰å¾ˆå¤§çš„ç¼ºç‚¹ï¼Œé‚£å°±æ˜¯å¯¹ç”µè„‘é…ç½®è¦æ±‚è¾ƒé«˜ï¼Œå¸¦çš„åŠ¨çš„ç”µè„‘åŸºæœ¬ä¸Šå°±æ˜¯æ— è„‘æ¨è pycharm äº†ã€‚ ä¸€å®šè¦ä¸‹è½½PRO ç‰ˆï¼Œå› ä¸ºä½ å°†æ¥è¿œç¨‹ç”¨æœåŠ¡å™¨è·‘ç¨‹åºï¼Œç¤¾åŒºç‰ˆæ²¡æœ‰è¿œç¨‹åŠŸèƒ½ã€‚ æœ€èˆ’æœçš„ IDE--jupyter jupyterï¼šhttps://jupyter.org/ ä½†å‡¡æ˜¯å†™ pythonçš„ï¼Œjupyterå‡ ä¹å¯ä»¥è¯´æ˜¯ä»å…¥é—¨åˆ°ç²¾é€šéƒ½ä¸€ç›´ä¼´éšç€ä½ çš„ IDE äº†ï¼Œå…¶åŠŸèƒ½æ‰©å±•æ€§æé«˜ï¼Œå¯è¯»æ€§æå¼ºï¼Œç½‘ä¸Šéå¸¸å¤šçš„ä»£ç æ•™ç¨‹éƒ½æ˜¯ç›´æ¥ç”¨ jupyter å†™çš„ï¼Œæ²¡æœ‰ jupyterï¼Œä½ çš„ä¸–ç•Œå¯ä»¥è¯´å°‘äº†ä¸€åŠå…‰æ˜ã€‚ å†™ä½œ å‡ºå…¥ç§‘ç ”é¢†åŸŸçš„ä½ ä»¬ï¼Œä¹Ÿè®¸è¦å’Œä½ ä»¬ç†Ÿæ‚‰çš„ office ä¸‰èŠ‚è¯¾è¯´æ‹œæ‹œäº†ã€‚å†™ä½œåœ¨ç§‘ç ”åœ¨æ˜¯éå¸¸é‡è¦çš„ä¸€ç¯ï¼Œä¸€ç¯‡æ¼‚äº®çš„è®ºæ–‡é™¤äº†è¦æœ‰å¥½çš„ ideaï¼ŒåŒæ ·ç¦»ä¸å¼€ç²¾ç¾çš„æ’ç‰ˆï¼Œæœ€é‡è¦çš„æ˜¯ä¼šè®®å’ŒæœŸåˆŠéƒ½æœ‰ç€ä¸¥æ ¼çš„æ ¼å¼è¦æ±‚ã€‚ LATEXå°†æ¥æ›¿ä½ ä»¬ç†Ÿæ‚‰çš„ wordï¼Œæ‰¿æ‹…èµ·å†™ä½œå·¥å…·çš„å¤§ä»»ã€‚ å­¦ä¹ ç½‘ç«™ï¼šhttps://www.latexstudio.net/ å½“ç„¶äº†ï¼Œé™¤äº†è®ºæ–‡çš„å†™ä½œï¼Œæ—¥å¸¸å­¦ä¹ ç”Ÿæ´»ä¸­çŸ¥è¯†çš„ç§¯ç´¯åŒæ ·éå¸¸é‡è¦ï¼Œä¸€èˆ¬æˆ‘ä»¬åœ¨åšè®ºæ–‡ç¬”è®°çš„æ—¶å€™ï¼Œé¦–é€‰æ¨èçš„æ˜¯Markdownï¼› Markdown çš„è¯­æ³•åŠå…¶ç®€å•ï¼Œç®€å•åˆ°ä½ çœ‹ä¸€éå°±èƒ½å­¦ä¼šï¼šhttps://markdown-zh.readthedocs.io/en/latest/ é™¤æ­¤ä¹‹å¤–ï¼Œä½ è¿˜éœ€è¦ä¸€æ¬¾å¥½çš„ Markdown è½¯ä»¶ï¼Œæˆ‘å½“ç„¶æ˜¯æ¨èTyporaï¼šhttps://www.typora.io/ è®ºæ–‡ å¼€å§‹äº†ç‚¼ä¸¹ç”Ÿæ¶¯æ¨¡å¼ï¼Œé‚£ paper è‡ªç„¶å°±å°‘ä¸äº†ï¼Œç‰¹åˆ«æ˜¯åƒæ·±åº¦å­¦ä¹ è¿™ç§å‘å±•éå¸¸å¿«çš„é¢†åŸŸï¼Œæ¯å¤©ç”Ÿäº§çš„è®ºæ–‡å¯è°“æ˜¯å¤šä¸èƒœæ•°(å½“ç„¶ç»å¤§éƒ¨åˆ†éƒ½æ˜¯å­¦æœ¯åƒåœ¾) æ‰€ä»¥è¯´ï¼Œåˆæ¥ä¹åˆ°çš„ä½ ä»¬ï¼Œå¼€å§‹çš„æ—¶å€™éƒ½ä¼šå¾ˆå›°æƒ‘â€”æˆ‘è¯¥è¯»ä»€ä¹ˆï¼Ÿ æˆ‘ä¸ªäººæ¨èä¸‰ä¸ªé€”å¾„ï¼š ä½ çš„å¯¼å¸ˆ--æ‘¸çˆ¬æ»šæ‰“äº†å‡ åå¹´ï¼Œå§¿åŠ¿ä¸çŸ¥é“æ¯”æˆ‘ä»¬é«˜åˆ°å“ªé‡Œå»äº†ï¼Œä½†æ˜¯å¯æƒœçš„æ˜¯è€æ¿ä»¬çš„æ—¶é—´ä¸€èˆ¬æ¥è¯´éƒ½å¾ˆæœ‰é™ï¼Œä¸æ˜¯å«¡ä¼ å¼Ÿå­ï¼Œå¾ˆéš¾è¯´èƒ½æ•™ä½ å¤šå°‘ã€‚ ç»„é‡Œçš„å¸ˆå“¥å¸ˆå§ä»¬--ä½ çš„å¸ˆå“¥å¸ˆå§ä»¬å¥½æ­¹ä¹Ÿæ˜¯æ¯”ä½ æ‰¾æ¥å‡ å¹´ï¼ŒåŸºæœ¬ä¸Šä½ å¯èƒ½é‡åˆ°çš„å‘ä»–ä»¬éƒ½è¸©è¿‡äº†ï¼Œè€Œä¸”å¦‚æœæ˜¯ä½ ä»¬å°†æ¥èƒ½æ¥æ‰‹ä»–ä»¬çš„å·¥ä½œï¼Œå¯¹ä½ ä»¬æ¥è¯´å¥½å¤„å®åœ¨ä¸è¦å¤ªå¤š(å½“ç„¶äº†ï¼Œä¹Ÿå¯èƒ½æ¥æ‰‹é‚£ç§ç¥–ä¼ çš„å‘â€¦â€¦)ï¼Œä¸è¿‡åƒæˆ‘ä»¬ç»„é‡Œï¼Œå¸ˆå“¥å¸ˆå§ä»¬ç®€ç›´æ˜¯äººå¤ªå¥½äº†ã€‚ æˆ‘æ¥ä¸‹æ¥æ¨èå‡ ä¸ªå’Œè®ºæ–‡æœ‰å…³çš„ç½‘ç«™ï¼Œé€ä¸€ä»‹ç»æ€ä¹ˆç”¨ã€‚ Browse state-of-the-art ç½‘ç«™ï¼šhttps://paperswithcode.com/sota è¿™æ˜¯ Reddit çš„ä¸€ä¸ªç”¨æˆ· rstoj åšçš„ä¸€ä¸ªç½‘ç«™ï¼Œå°† ArXiv ä¸Šçš„æœ€æ–°æœºå™¨å­¦ä¹ è®ºæ–‡ä¸ Github ä¸Šçš„ä»£ç ï¼ˆTensorFlow/PyTorch/MXNet/ç­‰ï¼‰å¯¹åº”èµ·æ¥ã€‚ç›¸æ¯”ä¹‹å‰æ¨èçš„é˜…è¯» ArXiv çš„ç½‘ç«™ï¼Œè¿™ä½ç”¨æˆ·åšå‡ºäº†æ»¡è¶³æ›´å¤šç ”ç©¶è€…çš„æœ€å¤§éœ€æ±‚-- å¯»æ‰¾è®ºæ–‡ç®—æ³•å®ç°çš„ä»£ç ï¼ å¯»æ‰¾è®ºæ–‡ç®—æ³•å®ç°çš„ä»£ç ï¼ å¯»æ‰¾è®ºæ–‡ç®—æ³•å®ç°çš„ä»£ç ï¼ è¿™ä¸ªé¡¹ç›®ç´¢å¼•äº†å¤§çº¦ 5 ä¸‡ç¯‡è®ºæ–‡ï¼ˆæœ€è¿‘ 5 å¹´å‘å¸ƒåœ¨ arxiv ä¸Šçš„è®ºæ–‡ï¼‰å’Œ 1 ä¸‡ä¸ª Github åº“ã€‚ ä½ å¯ä»¥æŒ‰æ ‡é¢˜å…³é”®è¯æŸ¥è¯¢ï¼Œæˆ–è€…ç ”ç©¶é¢†åŸŸå…³é”®è¯ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ç­‰æœç´¢ï¼Œä¹Ÿå¯ä»¥æŒ‰æµè¡Œç¨‹åº¦ã€æœ€æ–°è®ºæ–‡ä»¥åŠ Github ä¸Š Star æ•°é‡æœ€å¤šæ¥æ’åˆ—ã€‚è¿™ä¸ªç½‘ç«™èƒ½è®©ä½ è·Ÿä¸Šæœºå™¨å­¦ä¹ ç¤¾åŒºæµè¡Œçš„æœ€æ–°åŠ¨æ€ã€‚ Papers with Code githubï¼šhttps://github.com/zziz/pwc/blob/master/README.md#---- GitHub ä¸Šéå¸¸çƒ­é—¨çš„ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ€»ç»“è¿™è¿™å‡ å¹´é¡¶ä¼šé‡Œä¼˜ç§€çš„è®ºæ–‡å¹¶ä¸”é™„å¸¦ä»£ç å®ç°ï¼Œé«˜çº§ç‚¼ä¸¹å¸ˆçš„ç»å¯¹ç¦åˆ©ï¼ Model ZOO ç½‘ç«™ï¼šhttps://modelzoo.co/ Model Zoo æ›´åŠ æ³¨é‡æ·±åº¦å­¦ä¹ ç®—æ³•çš„åº”ç”¨ä»·å€¼ï¼Œé‡Œé¢æ¨èçš„é¡¹ç›®å¾ˆå¤šéƒ½æ˜¯ååº”ç”¨çš„é¡¹ç›®ï¼Œå¦‚æœè¯´ä½ ç ”ç©¶ä¾§é‡åœ¨åº”ç”¨ä¸Šï¼Œé‚£ Model Zoo ä¸€å®šæ˜¯ä½ çš„é¦–é€‰ã€‚ å…¶å®æˆ‘å¤§æ¦‚ä¹Ÿåªæ˜¯æŠ›ç –å¼•ç‰çš„ä»‹ç»å‡ ä¸ªæˆ‘è§‰å¾—æ—¥å¸¸æ¯”è¾ƒå®ç”¨çš„ã€‚ GitHub ä¸ºä»€ä¹ˆæˆ‘è¦å•ç‹¬æŠŠ github ä»ä»£ç é‡Œæ‘˜å‡ºæ¥è®²å‘¢ï¼Ÿå› ä¸º GitHub å®åœ¨æ˜¯å¤ªé‡è¦äº†ï¼Œå¯¹ä¸€ä¸ªç°ä»£ç‚¼ä¸¹å¸ˆæ¥è¯´ï¼Œä½ å‡ ä¹æ¯ä¸€å¤©éƒ½ç¦»ä¸å¼€ GitHubã€‚ å¼€æºæ˜¯ä¸€ä»¶å¾ˆä¼Ÿå¤§çš„äº‹æƒ…ï¼Œè¿™ä¸ªé—®é¢˜ä¸Šå‡åˆ°å“²å­¦å±‚é¢ï¼Œæˆ‘ä¸å±•å¼€è®²ã€‚ æˆ‘åœ¨è¿™é‡Œä¹Ÿå¹¶ä¸æƒ³å¯¹äºâ€œé€å»â€å†è¯´ä»€ä¹ˆï¼Œå¦åˆ™å¤ªä¸â€œæ‘©ç™»â€äº†ã€‚æˆ‘åªæƒ³é¼“å¹æˆ‘ä»¬å†åå•¬ä¸€ç‚¹ï¼Œâ€œé€å»â€ä¹‹å¤–ï¼Œè¿˜å¾—â€œæ‹¿æ¥â€ï¼Œæ˜¯ä¸ºâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚ â€”é²è¿…ã€Šæ‹¿æ¥ä¸»ä¹‰ã€‹ å½“ç„¶äº†è¿˜æœ‰å°±æ˜¯å€Ÿæœºæ¨å¹¿ä¸€ä¸‹è‡ªå·±å˜›(çš®ä¸€ä¸‹2333)ï¼Œæˆ‘ç›®å‰ä¸€ç›´åœ¨ç»´æŠ¤æˆ‘ä»¬å®éªŒå®¤çš„ GitHub è´¦å· OUCMLï¼šhttps://github.com/OUCMachineLearning/OUCMLï¼Œæˆ‘ä¸ªäººæ€»ç»“çš„å­¦ä¹ èµ„æ–™åŸºæœ¬ä¸Šéƒ½åœ¨ä¸Šé¢ã€‚(å°å°å£°ï¼šçŸ¥ä¹è´¦å·ï¼šé©¬å¡æ–¯Â·æ‰¬) Gitçš„å¥‡æŠ€æ·«å·§ Gitå¸¸ç”¨å‘½ä»¤é›†åˆï¼ŒForkäºtipsé¡¹ç›® Gitæ˜¯ä¸€ä¸ª â€œåˆ†å¸ƒå¼ç‰ˆæœ¬ç®¡ç†å·¥å…·â€ï¼Œç®€å•çš„ç†è§£ç‰ˆæœ¬ç®¡ç†å·¥å…·ï¼šå¤§å®¶åœ¨å†™ä¸œè¥¿çš„æ—¶å€™éƒ½ç”¨è¿‡ â€œå›æ’¤â€ è¿™ä¸ªåŠŸèƒ½ï¼Œä½†æ˜¯å›æ’¤åªèƒ½å›æ’¤å‡ æ­¥ï¼Œå‡å¦‚æƒ³è¦æ‰¾å›æˆ‘ä¸‰å¤©ä¹‹å‰çš„ä¿®æ”¹ï¼Œå…‰ç”¨ â€œå›æ’¤â€ æ˜¯æ‰¾ä¸å›æ¥çš„ã€‚è€Œ â€œç‰ˆæœ¬ç®¡ç†å·¥å…·â€ èƒ½è®°å½•æ¯æ¬¡çš„ä¿®æ”¹ï¼Œåªè¦æäº¤åˆ°ç‰ˆæœ¬ä»“åº“ï¼Œä½ å°±å¯ä»¥æ‰¾åˆ°ä¹‹å‰ä»»ä½•æ—¶åˆ»çš„çŠ¶æ€ï¼ˆæ–‡æœ¬çŠ¶æ€ï¼‰ã€‚ å…³äºå¦‚ä½•ä½¿ç”¨GitHubï¼Œçœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼šhttps://github.com/521xueweihan/git-tips æ·±åº¦å­¦ä¹ å…¥é—¨ å‰é¢ä¹±ä¸ƒå…«ç³ŸèŠäº†è¿™ä¹ˆå¤šï¼Œæˆ‘ç”¨å“²å­¦ä¸‰é—®æ­¥å…¥æ­£é¢˜ã€‚ æˆ‘æ˜¯è° WIKI: æ·±åº¦å­¦ä¹ ï¼ˆè‹±èªï¼šdeep learningï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„åˆ†æ”¯ï¼Œæ˜¯ä¸€ç¨®ä»¥äººå·¥ç¥ç¶“ç¶²è·¯ç‚ºæ¶æ§‹ï¼Œå°è³‡æ–™é€²è¡Œè¡¨å¾µå­¸ç¿’çš„ç®—æ³•ã€‚[1][2][3][4][5] æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ç§åŸºäºå¯¹æ•°æ®è¿›è¡Œè¡¨å¾å­¦ä¹ çš„ç®—æ³•ã€‚è§‚æµ‹å€¼ï¼ˆä¾‹å¦‚ä¸€å¹…å›¾åƒï¼‰å¯ä»¥ä½¿ç”¨å¤šç§æ–¹å¼æ¥è¡¨ç¤ºï¼Œå¦‚æ¯ä¸ªåƒç´ å¼ºåº¦å€¼çš„å‘é‡ï¼Œæˆ–è€…æ›´æŠ½è±¡åœ°è¡¨ç¤ºæˆä¸€ç³»åˆ—è¾¹ã€ç‰¹å®šå½¢çŠ¶çš„åŒºåŸŸç­‰ã€‚è€Œä½¿ç”¨æŸäº›ç‰¹å®šçš„è¡¨ç¤ºæ–¹æ³•æ›´å®¹æ˜“ä»å®ä¾‹ä¸­å­¦ä¹ ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œäººè„¸è¯†åˆ«æˆ–é¢éƒ¨è¡¨æƒ…è¯†åˆ«[6]ï¼‰ã€‚æ·±åº¦å­¦ä¹ çš„å¥½å¤„æ˜¯ç”¨éç›‘ç£å¼æˆ–åŠç›‘ç£å¼çš„ç‰¹å¾å­¦ä¹ å’Œåˆ†å±‚ç‰¹å¾æå–é«˜æ•ˆç®—æ³•æ¥æ›¿ä»£æ‰‹å·¥è·å–ç‰¹å¾&action=edit&redlink=1)ã€‚[7] è¡¨å¾å­¦ä¹ çš„ç›®æ ‡æ˜¯å¯»æ±‚æ›´å¥½çš„è¡¨ç¤ºæ–¹æ³•å¹¶å»ºç«‹æ›´å¥½çš„æ¨¡å‹æ¥ä»å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®ä¸­å­¦ä¹ è¿™äº›è¡¨ç¤ºæ–¹æ³•ã€‚è¡¨ç¤ºæ–¹æ³•æ¥è‡ªç¥ç»ç§‘å­¦ï¼Œå¹¶æ¾æ•£åœ°å»ºç«‹åœ¨é¡ä¼¼ç¥ç»ç³»ç»Ÿä¸­çš„ä¿¡æ¯å¤„ç†å’Œå¯¹é€šä¿¡æ¨¡å¼çš„ç†è§£ä¸Šï¼Œå¦‚ç¥ç»ç¼–ç ï¼Œè¯•å›¾å®šä¹‰æ‹‰å‹•ç¥ç»å…ƒçš„ååº”ä¹‹é—´çš„å…³ç³»ä»¥åŠå¤§è„‘ä¸­çš„ç¥ç»å…ƒçš„ç”µæ´»åŠ¨ä¹‹é—´çš„å…³ç³»ã€‚[8] è‡³ä»Šå·²æœ‰æ•¸ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚æ·±åº¦ç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œå’Œæ·±åº¦ç½®ä¿¡ç½‘ç»œå’Œé€’å½’ç¥ç»ç½‘ç»œå·²è¢«åº”ç”¨åœ¨è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€éŸ³é¢‘è¯†åˆ«ä¸ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸå¹¶å–å¾—äº†æå¥½çš„æ•ˆæœã€‚ å¦å¤–ï¼Œã€Œæ·±åº¦å­¦ä¹ ã€å·²æˆç‚ºé¡ä¼¼è¡“èªï¼Œæˆ–è€…è¯´æ˜¯ç¥ç»ç½‘ç»œçš„å“ç‰Œé‡å¡‘ã€‚[9][10] Emmmm,è¿˜æ˜¯ç®€å•ç‚¹è¯´å§ï¼ŒæŠŠæ•°æ®ä½œä¸º Xï¼Œæ ‡ç­¾ä½œä¸º Yï¼Œæˆ‘ä»¬æ·±åº¦å­¦ä¹ å°±æ˜¯æ‰¾åˆ°ä¸€ä¸ª F(Â·)ï¼Œä½¿å¾—yâ‰ˆf(x)â€‹ã€‚ è¿™ä¸ª F()å¾ˆå¤æ‚ï¼Œæˆ‘å¾—ä»ï¼ˆå®‡å®™å¤§çˆ†ç‚¸å¼€å§‹è®²èµ·ï¼‰æœºå™¨å­¦ä¹ è®²èµ·ã€‚ æˆ‘ä»å“ªé‡Œæ¥ 1943å¹´ ç”±ç¥ç»ç§‘å­¦å®¶éº¦å¡æ´›å…‹(W.S.McCilloch) å’Œæ•°å­¦å®¶çš®å…¹ï¼ˆW.Pittsï¼‰åœ¨ã€Šæ•°å­¦ç”Ÿç‰©ç‰©ç†å­¦å…¬å‘Šã€‹ä¸Šå‘è¡¨è®ºæ–‡ã€Šç¥ç»æ´»åŠ¨ä¸­å†…åœ¨æ€æƒ³çš„é€»è¾‘æ¼”ç®—ã€‹ï¼ˆA Logical Calculus of the Ideas Immanent in Nervous Activityï¼‰ã€‚å»ºç«‹äº†ç¥ç»ç½‘ç»œå’Œæ•°å­¦æ¨¡å‹ï¼Œç§°ä¸ºMCPæ¨¡å‹ã€‚æ‰€è°“MCPæ¨¡å‹ï¼Œå…¶å®æ˜¯æŒ‰ç…§ç”Ÿç‰©ç¥ç»å…ƒçš„ç»“æ„å’Œå·¥ä½œåŸç†æ„é€ å‡ºæ¥çš„ä¸€ä¸ªæŠ½è±¡å’Œç®€åŒ–äº†çš„æ¨¡å‹ï¼Œä¹Ÿå°±è¯ç”Ÿäº†æ‰€è°“çš„â€œæ¨¡æ‹Ÿå¤§è„‘â€ï¼Œäººå·¥ç¥ç»ç½‘ç»œçš„å¤§é—¨ç”±æ­¤å¼€å¯ã€‚1 1958å¹´ è®¡ç®—æœºç§‘å­¦å®¶ç½—æ£®å¸ƒæ‹‰ç‰¹ï¼ˆ Rosenblattï¼‰æå‡ºäº†ä¸¤å±‚ç¥ç»å…ƒç»„æˆçš„ç¥ç»ç½‘ç»œï¼Œç§°ä¹‹ä¸ºâ€œæ„ŸçŸ¥å™¨â€(Perceptrons)ã€‚ç¬¬ä¸€æ¬¡å°†MCPç”¨äºæœºå™¨å­¦ä¹ ï¼ˆmachine learningï¼‰åˆ†ç±»(classification)ã€‚â€œæ„ŸçŸ¥å™¨â€ç®—æ³•ç®—æ³•ä½¿ç”¨MCPæ¨¡å‹å¯¹è¾“å…¥çš„å¤šç»´æ•°æ®è¿›è¡ŒäºŒåˆ†ç±»ï¼Œä¸”èƒ½å¤Ÿä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»è®­ç»ƒæ ·æœ¬ä¸­è‡ªåŠ¨å­¦ä¹ æ›´æ–°æƒå€¼ã€‚1962å¹´,è¯¥æ–¹æ³•è¢«è¯æ˜ä¸ºèƒ½å¤Ÿæ”¶æ•›ï¼Œç†è®ºä¸å®è·µæ•ˆæœå¼•èµ·ç¬¬ä¸€æ¬¡ç¥ç»ç½‘ç»œçš„æµªæ½®ã€‚ æˆ‘åˆ°å“ªé‡Œå» è¿™ä¸ªé—®é¢˜è¯´å®è¯ï¼Œæˆ‘ç­”ä¸ä¸Šæ¥ã€‚ ä½†æ˜¯ï¼Œæˆ‘å¯ä»¥ç®€è¦ä»‹ç»ä¸€ä¸‹ç›®å‰æ·±åº¦å­¦ä¹ æ¯”è¾ƒç«çš„å‡ ä¸ªé¢†åŸŸï¼š CV è®¡ç®—æœºè§†è§‰ï¼ˆComputational Visionï¼‰æ˜¯ç”±ç›¸æœºæ‹æ‘„å›¾åƒï¼Œ é€šè¿‡ç”µè„‘å¯¹å›¾åƒä¸­çš„ç›®æ ‡è¿›è¡Œè¯†åˆ«å’Œæ£€æµ‹ã€‚å¯ä»¥è¯´æ˜¯æœºå™¨å­¦ä¹ åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼Œæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚ emmmï¼Œè¿™ä¸ªé—¨æ§›ä½ï¼Œåº”ç”¨å¹¿ï¼Œè¿™å‡ å¹´éƒ½å¿«æŒ¤çˆ†äº†ã€‚ NLP è‡ªç„¶èªè¨€è™•ç†ï¼ˆè‹±èªï¼šNatural Language Processingï¼Œç¼©å†™ä½œ NLPï¼‰æ˜¯äººå·¥æ™ºæ…§å’Œèªè¨€å­¸é ˜åŸŸçš„åˆ†æ”¯å­¸ç§‘ã€‚æ­¤é ˜åŸŸæ¢è¨å¦‚ä½•è™•ç†åŠé‹ç”¨è‡ªç„¶èªè¨€ï¼›è‡ªç„¶èªè¨€è™•ç†åŒ…æ‹¬å¤šæ–¹é¢å’Œæ­¥éª¤ï¼ŒåŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚ NLP ç›¸å¯¹æ¥è¯´å„æ–­æ€§è´¨è¾ƒé«˜ï¼Œä¹Ÿå°±æ˜¯å…¶å®å¤§å®¶ç®—æ³•éƒ½å·®ä¸å¤šï¼Œè°æ•°æ®å¤šè° NBã€‚ åŒ»å­¦ æ²¡åšè¿‡ï¼Œä¸çŸ¥é“ã€‚ã€‚ã€‚ã€‚ å¼ºåŒ–å­¦ä¹  ä¸å¾—ä¸æå·¨å‘ AutoML Google çˆ¸çˆ¸ä¸€ç›´åœ¨åŠªåŠ›æ¨è¿› AutoMLï¼Œè¯´ç™½äº†å°±æ˜¯è®©ç®—æ³•è‡ªåŠ¨è®¾è®¡ç¥ç»ç½‘ç»œã€‚ æ¨è 18 å¹´çš„ä¸€ç¯‡ç»¼è¿°ï¼šhttps://arxiv.org/pdf/1810.13306 å›¾å·ç§¯ è¿™ä¸¤å¹´çˆ†ç«çš„ï¼Œè¯´å®è¯æˆ‘ä¹Ÿä¸å¤ªæ‡‚ã€‚ã€‚ã€‚ã€‚ ä»–æ¯”æˆ‘æ‡‚ğŸ‘‰å›¾å·ç§¯ç¥ç»ç½‘ç»œ(GCN)è¯¦è§£:åŒ…æ‹¬äº†æ•°å­¦åŸºç¡€(å‚…é‡Œå¶ï¼Œæ‹‰æ™®æ‹‰æ–¯) è¿ç§»å­¦ä¹  æŠŠä»æ•°æ®é›† A ä¸Šå­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°æ•°æ®é›† B çš„ä»»åŠ¡ä¸Šã€‚ æ·±åº¦å­¦ä¹ æ¡†æ¶ æ‰€è°“å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œä¹‹å‰æˆ‘ä»¬æåˆ°è¿‡ python æœ‰ç€å¾ˆå¤šçš„ç¬¬ä¸‰æ–¹åº“ï¼Œé‚£æˆ‘æ¥ä¸‹æ¥å°†è¦è§£é‡Šä¸€ä¸‹å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼š TensorFlow Google æ”¯æŒçš„ TensorFlow æ¡†æ¶ï¼Œåº”è¯¥æ˜¯æœ€å‡ºåçš„æ²¡æœ‰ä¹‹ä¸€äº†å§ã€‚ä¼˜ç‚¹æ˜¯å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯ TensorFlow å†™çš„ï¼Œä»£ç å¤ç°å¿«ï¼Œç¼ºç‚¹æ˜¯ç‰ˆæœ¬è¿­ä»£äº§ç”Ÿçš„ bug å¤šå¦‚ç¹æ˜Ÿã€‚ https://www.tensorflow.org/overview Keras æ–°æ‰‹å…¥é—¨ç¥å™¨ï¼Œç›¸å½“äºåœ¨ TensorFlow çš„åŸºç¡€ä¸ŠåˆåŒ…äº†ä¸€å±‚çš®ï¼ŒAPI æ¥å£ç¨³å®šï¼Œä»£ç å®ç°ç®€æ˜“ï¼Œç¼ºç‚¹æ˜¯è‡ªç”±åº¦å¤ªä½äº†ã€‚ https://keras.io/ pytorch å¥½ç”¨çš„ä¸€åŒ¹(æˆ‘åœ¨ç”¨)ï¼ŒåŠ¨æ€è®¡ç®—å›¾è°ç”¨è°çŸ¥é“ï¼Œç¼ºç‚¹ä¹‹ä¸€æ˜¯ï¼šè€è€å®å®è¯»æºä»£ç å§ã€‚ PyTorch 1.0é‡æ„å’Œç»Ÿä¸€äº†Caffe2å’ŒPyTorch 0.4æ¡†æ¶çš„ä»£ç åº“ï¼Œåˆ é™¤äº†é‡å¤çš„ç»„ä»¶å¹¶å…±äº«ä¸Šå±‚æŠ½è±¡ï¼Œå¾—åˆ°äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ”¯æŒé«˜æ•ˆçš„å›¾æ¨¡å¼æ‰§è¡Œã€ç§»åŠ¨éƒ¨ç½²å’Œå¹¿æ³›çš„ä¾›åº”å•†é›†æˆç­‰ã€‚è¿™è®©å¼€å‘äººå‘˜å¯ä»¥åŒæ—¶æ‹¥æœ‰PyTorchå’ŒCaffe2çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶åšåˆ°å¿«é€ŸéªŒè¯å’Œä¼˜åŒ–æ€§èƒ½ã€‚PyTorchçš„å‘½ä»¤å¼å‰ç«¯é€šè¿‡å…¶çµæ´»è€Œé«˜æ•ˆçš„ç¼–ç¨‹æ¨¡å‹å®ç°äº†æ›´å¿«é€Ÿçš„åŸå‹è®¾è®¡å’Œå®éªŒï¼Œåˆå¸å–äº†Caffe2å’ŒONNXçš„æ¨¡å—åŒ–ä»¥åŠé¢å‘ç”Ÿäº§çš„ç‰¹ç‚¹ï¼Œä½¿å¾—æ·±åº¦å­¦ä¹ é¡¹ç›®èƒ½ä»ç ”ç©¶åŸå‹å¿«é€Ÿæ— ç¼è¡”æ¥åˆ°ç”Ÿäº§éƒ¨ç½²ï¼Œåœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ç»Ÿä¸€å®éªŒç ”ç©¶å’Œç”Ÿäº§èƒ½åŠ›ã€‚ Theano ä¸æƒ³è´´å›¾ï¼Œä¸¤ä¸ªå­—--æœ‰æ¯’ Caffe æ®è¯´åšç½‘ç»œç»“æ„å‹ç¼©é‚£ä¸€å—çš„å°ä¼™ä¼´ç”¨çš„æ¯”è¾ƒå¤šï¼Œå› ä¸ºå¯ä»¥æ–¹ä¾¿çš„æ”¹åº•å±‚çš„å®ç°ã€‚ Caffe2 ç°åœ¨å« pytorch1.0 MXnet è¿™æœ¬ä¹¦å°±æ˜¯ææ²å¤§ç¥ç”¨ MXnet å†™çš„ï¼Œå¬è¯´æŒºå¥½ç”¨çš„ï¼Œå°±æ˜¯æ²¡å•¥é’±åšå®£ä¼ æ¨å¹¿ï¼Œé‚£æˆ‘å¸®ä»–æ¨å¹¿ä¸€ä¸‹ï¼šhttps://zh.gluon.ai/chapter_introduction/deep-learning-intro.html ä»Šæ™šå…¶å®å†™è¿™ä¸ªå†™äº†æŒºä¹…çš„ï¼Œç°åœ¨å·²ç»å‡Œæ™¨ 1 ç‚¹å¤šäº†ï¼Œå…¶å®æˆ‘çœŸçš„æŒºå–œæ¬¢æ·±åº¦å­¦ä¹ è¿™ä¸ªç ”ç©¶é¢†åŸŸçš„ï¼Œè¯´å®è¯æ€»æ„Ÿè§‰è¿˜æœ‰å¥½å¤šè¯æƒ³è¯´å‡ºæ¥ï¼Œæƒ³å’Œå¤§å®¶èŠèŠåŠ¨æ€è®¡ç®—å›¾ï¼Œè‡ªåŠ¨å¾®åˆ†åº“ï¼Œç½‘ç»œç»“æ„å¯è§†åŒ–ï¼ŒLipschitzæ¡ä»¶ç­‰ç­‰ç­‰ç­‰â€¦â€¦ æ¥ä¸‹æ¥çš„å¤§ä¸‰äº†ï¼Œä¹Ÿè®¸è¯¾ç¨‹å‹åŠ›æŒºå¤§çš„ï¼Œå¸Œæœ›å–œæ¬¢ä¸€ä»¶äº‹æƒ…å°±èƒ½åšæŒä¸‹æ¥å§(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§ è¦æ˜¯è§‰å¾—æœ‰ç”¨éº»çƒ¦å…³æ³¨ç‚¹èµè½¬å‘ä¸‰è¿å•¦ å‚è€ƒæ–‡çŒ®ï¼š [2]ï¼šhttps://zhuanlan.zhihu.com/p/59086302 "},"introduction/AI_system.html":{"url":"introduction/AI_system.html","title":"ä¸ºä»€ä¹ˆè¦ä½¿å¾—AI Systemå…·å¤‡å¯è§£é‡Šæ€§å‘¢ï¼Ÿ","keywords":"","body":"ä¸ºä»€ä¹ˆè¦ä½¿å¾—AI Systemå…·å¤‡å¯è§£é‡Šæ€§å‘¢ï¼Ÿ å¦‚åœ¨AIåŒ»ç–—é¢†åŸŸï¼Œå¦‚æœæ— æ³•ç†è§£åŠéªŒè¯AI Systemåšå†³ç­–çš„æµç¨‹æœºç†ï¼Œé‚£ä¹ˆä»¥é»˜è®¤ç›¸ä¿¡AIåˆ¤æ–­çš„æ–¹å¼æ˜¯ä¸è´Ÿè´£ä»»çš„ï¼ˆSelf Driving etc.ï¼‰ï¼Œæ›´å¤šè®¨è®ºå¯ä»¥çœ‹ä»¥ä¸‹è®ºæ–‡ã€‚ Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models é‡è¦çš„ç½‘ç«™Heatmapping è¿™æ˜¯ä¸€ä¸ªä¸“é—¨æ•´ç†Explainable AIçš„ç›¸å…³æƒ³æ³•ï¼Œå°¤å…¶æ˜¯å»è§£é‡Šé‚£äº›State of the artæ¨¡å‹ï¼Œæˆ‘å…ˆå±•ç¤ºä¸€ä¸ªå¯è§†åŒ–çš„Demoï¼šLRP Demosé“¾æ¥ æ•°å­—è¾¨è¯†åŠé‡è¦çš„åƒç´ çƒ­åŠ›å›¾æ˜¾ç¤º çŒ«å’ªçš„åˆ†ç±»ä¼šæ¯”è¾ƒç›´è§‚ä¸€ä¸‹ï¼š çœ‹æ ·å­æ˜¯å­¦ä¹ åˆ°äº†çŒ«å’ªçš„è½®å»“ ä»¥ä¸‹å†…å®¹æ¥è‡ªHeatmappingï¼ å…³äºLRPçš„ä»‹ç»ï¼š A Tutorial on Implementing LRP A Quick Introduction to Deep Taylor Decomposition å…³äºLRPçš„è½¯ä»¶ï¼š Keras Explanation Toolbox (LRP and other Methods) GitHub project page for the LRP Toolbox TensorFlow LRP Wrapper LRP Code for LSTM å…³äºDeep Modelå¯è§£é‡Šæ€§çš„talkï¼š CVPR18: Tutorial: Part 1: Interpreting and Explaining Deep Models in Computer Vision EMBC 2019 Tutorial (Website) Explainable ML, Medical Applications Northern Lights Deep Learning Workshop Keynote(Website | Slides) Explainable ML, Applications 2018 Int. Explainable AI Symposium Keynote(Website | Slides) Explainable ML, Applications ICIP 2018 Tutorial (Website | Slides: 1-Intro, 2-Methods, 3-Evaluation, 4-Applications) Explainable ML, Applications MICCAI 2018 Tutorial (Website | Slides) Explainable ML, Medical Applications Talk at Int. Workshop ML & AI 2018 (Slides) Deep Taylor Decomposition, Validating Explanations WCCI 2018 Keynote (Slides) Explainable ML, LRP, Applications GCPR 2017 Tutorial (Slides) ICASSP 2017 Tutorial (Slides 1-Intro, 2-Methods, 3-Applications) Hightlight Papers S Lapuschkin, S WÃ¤ldchen, A Binder, G Montavon, W Samek, KR MÃ¼ller. Unmasking Clever Hans Predictors and Assessing What Machines Really Learn Nature Communications, 10:1096, 2019 [preprint | bibtex] å…¥é—¨çº§ä»‹ç»ï¼š G Montavon, W Samek, KR MÃ¼ller. Methods for Interpreting and Understanding Deep Neural Networks Digital Signal Processing, 73:1-15, 2018 [preprint | bibtex] W Samek, T Wiegand, KR MÃ¼ller. Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models ITU Journal: ICT Discoveries - Special Issue 1 - The Impact of AI on Communication Networks and Services, 1(1):39-48, 2018 [preprint, bibtex] æ–¹æ³•ç›¸å…³ï¼š S Bach, A Binder, G Montavon, F Klauschen, KR MÃ¼ller, W Samek. On Pixel-wise Explanations for Non-Linear Classifier Decisions by Layer-wise Relevance Propagation PLOS ONE, 10(7):e0130140, 2015 [preprint, bibtex] G Montavon, S Lapuschkin, A Binder, W Samek, KR MÃ¼ller. Explaining NonLinear Classification Decisions with Deep Taylor Decomposition Pattern Recognition, 65:211â€“222, 2017 [preprint, bibtex] L Arras, G Montavon, KR MÃ¼ller, W Samek. Explaining Recurrent Neural Network Predictions in Sentiment Analysis EMNLP Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, 159-168, 2017 [preprint, bibtex] A Binder, G Montavon, S Lapuschkin, KR MÃ¼ller, W Samek. Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers Artificial Neural Networks and Machine Learning â€“ ICANN 2016, Part II, Lecture Notes in Computer Science, Springer-Verlag, 9887:63-71, 2016 [preprint, bibtex] PJ Kindermans, KT SchÃ¼tt, M Alber, KR MÃ¼ller, D Erhan, B Kim, S DÃ¤hne. Learning how to explain neural networks: PatternNet and PatternAttribution International Conference on Learning Representations (ICLR), 2018 L Rieger, P Chormai, G Montavon, LK Hansen, KR MÃ¼ller. Structuring Neural Networks for More Explainable Predictions in Explainable and Interpretable Models in Computer Vision and Machine Learning, 115-131, Springer SSCML, 2018 J Kauffmann, KR MÃ¼ller, G Montavon. Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models arXiv:1805.06230, 2018 è¯„ä¼°è§£é‡Š Evaluation of Explanation L Arras, A Osman, KR MÃ¼ller, W Samek. Evaluating Recurrent Neural Network Explanations arXiv:1904.11829, 2019 W Samek, A Binder, G Montavon, S Bach, KR MÃ¼ller. Evaluating the Visualization of What a Deep Neural Network has Learned IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660-2673, 2017 [preprint, bibtex] å…³äºè½¯ä»¶çš„Papers: M Alber, S Lapuschkin, P Seegerer, M HÃ¤gele, KT SchÃ¼tt, G Montavon, W Samek, KR MÃ¼ller, S DÃ¤hne, PJ Kindermans iNNvestigate neural networks!. arXiv:1808.04260, 2018 S Lapuschkin, A Binder, G Montavon, KR MÃ¼ller, W Samek The Layer-wise Relevance Propagation Toolbox for Artificial Neural Networks Journal of Machine Learning Research, 17(114):1âˆ’5, 2016 [preprint, bibtex] Application of Science: I Sturm, S Bach, W Samek, KR MÃ¼ller. Interpretable Deep Neural Networks for Single-Trial EEG Classification Journal of Neuroscience Methods, 274:141â€“145, 2016 [preprint, bibtex] A Binder, M Bockmayr, M HÃ¤gele, S Wienert, D Heim, K Hellweg, A Stenzinger, L Parlow, J Budczies, B Goeppert, D Treue, M Kotani, M Ishii, M Dietel, A Hocke, C Denkert, KR MÃ¼ller, F Klauschen. Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles arXiv:1805.11178, 2018 F Horst, S Lapuschkin, W Samek, KR MÃ¼ller, WI SchÃ¶llhorn. Explaining the Unique Nature of Individual Gait Patterns with Deep Learning Scientific Reports, 9:2391, 2019 [preprint, bibtex] AW Thomas, HR Heekeren, KR MÃ¼ller, W Samek. Analyzing Neuroimaging Data Through Recurrent Deep Learning Models arXiv:1810.09945, 2018 æ–‡æœ¬ä¸Šçš„åº”ç”¨ï¼š L Arras, F Horn, G Montavon, KR MÃ¼ller, W Samek. \"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach PLOS ONE, 12(8):e0181142, 2017 [preprint, bibtex] L Arras, G Montavon, KR MÃ¼ller, W Samek. Explaining Recurrent Neural Network Predictions in Sentiment Analysis EMNLP Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, 159-168, 2017 [preprint, bibtex] L Arras, F Horn, G Montavon, KR MÃ¼ller, W Samek. Explaining Predictions of Non-Linear Classifiers in NLP ACL Workshop on Representation Learning for NLP, 1-7, 2016 [preprint, bibtex] F Horn, L Arras, G Montavon, KR MÃ¼ller, W Samek. Exploring text datasets by visualizing relevant words arXiv:1707.05261, 2017 å›¾åƒåŠè„¸éƒ¨è¯†åˆ«çš„åº”ç”¨ï¼š S Lapuschkin, A Binder, G Montavon, KR MÃ¼ller, W Samek. Analyzing Classifiers: Fisher Vectors and Deep Neural Networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2912-2920, 2016 [preprint, bibtex] S Lapuschkin, A Binder, KR MÃ¼ller, W Samek. Understanding and Comparing Deep Neural Networks for Age and Gender Classification IEEE International Conference on Computer Vision Workshops (ICCVW), 1629-1638, 2017 [preprint, bibtex] C Seibold, W Samek, A Hilsmann, P Eisert. Accurate and Robust Neural Networks for Security Related Applications Exampled by Face Morphing Attacks arXiv:1806.04265, 2018 S Bach, A Binder, KR MÃ¼ller, W Samek. Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth Proceedings of the IEEE International Conference on Image Processing (ICIP), 2271-2275, 2016 [preprint, bibtex] A Binder, S Bach, G Montavon, KR MÃ¼ller, W Samek. Layer-wise Relevance Propagation for Deep Neural Network Architectures Proceedings of the 7th International Conference on Information Science and Applications (ICISA), 6679:913-922, Springer Singapore, 2016 [preprint, bibtex] F Arbabzadah, G Montavon, KR MÃ¼ller, W Samek. Identifying Individual Facial Expressions by Deconstructing a Neural Network Pattern Recognition - 38th German Conference, GCPR 2016, Lecture Notes in Computer Science, 9796:344-354, 2016 [preprint, bibtex] è§†é¢‘çš„åº”ç”¨ï¼š C Anders, G Montavon, W Samek, KR MÃ¼ller. Understanding Patch-Based Learning by Explaining Predictions arXiv:1806.06926, 2018 V Srinivasan, S Lapuschkin, C Hellge, KR MÃ¼ller, W Samek. Interpretable human action recognition in compressed domain Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1692-1696, 2017 [preprint, bibtex] è¯­éŸ³çš„åº”ç”¨ï¼š S Becker, M Ackermann, S Lapuschkin, KR MÃ¼ller, W Samek. Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals arXiv:1807.03418, 2018 çŸ­Paper W Samek, G Montavon, A Binder, S Lapuschkin, and KR MÃ¼ller. Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation NIPS Workshop on Interpretable ML for Complex Systems, 1-5, 2016 [preprint, bibtex] G Montavon, S Bach, A Binder, W Samek, KR MÃ¼ller. Deep Taylor Decomposition of Neural Networks ICML Workshop on Visualization for Deep Learning, 1-3, 2016 [preprint, bibtex] A Binder, W Samek, G Montavon, S Bach, KR MÃ¼ller. Analyzing and Validating Neural Networks Predictions ICML Workshop on Visualization for Deep Learning, 1-4, 2016 [preprint, bibtex] æœ€åï¼šBVLC Model Zoo Contributions Pascal VOC 2012 Multilabel Model (see paper): [caffemodel] [prototxt] Age and Gender Classification Models (see paper): [data and models] ç›¸å…³èµ„æ–™ï¼š [1]Samek, Wojciech, Thomas Wiegand, and Klaus-Robert MÃ¼ller. \"Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models.\"arXiv preprint arXiv:1708.08296(2017). "},"code_technique/":{"url":"code_technique/","title":"ç¼–ç¨‹æŠ€å·§","keywords":"","body":" pythonç¼–ç¨‹æŠ€å·§ pythonå¸¸è§æ’åº opencv-pythonæé€Ÿå…¥é—¨ pytorchå¸¸ç”¨ä»£ç  pytorchå¸¸ç”¨ä»£ç æ®µåˆé›† pytorchè®­ç»ƒæŠ€å·§ pytorchè§£å†» pytorchç½‘ç»œå¯è§†åŒ– PSNR&&SSIM SPP Tensor to img && imge to tensor "},"code_technique/python/python_technique.html":{"url":"code_technique/python/python_technique.html","title":"pythonç¼–ç¨‹æŠ€å·§","keywords":"","body":"Python 3 æ–°ç‰¹æ€§ï¼šç±»å‹æ³¨è§£ å‰å‡ å¤©æœ‰åŒå­¦é—®åˆ°ï¼Œè¿™ä¸ªå†™æ³•æ˜¯ä»€ä¹ˆæ„æ€ï¼š def add(x:int, y:int) -> int: return x + y æˆ‘ä»¬çŸ¥é“ Python æ˜¯ä¸€ç§åŠ¨æ€è¯­è¨€ï¼Œå˜é‡ä»¥åŠå‡½æ•°çš„å‚æ•°æ˜¯ä¸åŒºåˆ†ç±»å‹ã€‚å› æ­¤æˆ‘ä»¬å®šä¹‰å‡½æ•°åªéœ€è¦è¿™æ ·å†™å°±å¯ä»¥äº†ï¼š def add(x, y): return x + y è¿™æ ·çš„å¥½å¤„æ˜¯æœ‰æå¤§çš„çµæ´»æ€§ï¼Œä½†åå¤„å°±æ˜¯å¯¹äºåˆ«äººä»£ç ï¼Œæ— æ³•ä¸€çœ¼åˆ¤æ–­å‡ºå‚æ•°çš„ç±»å‹ï¼ŒIDE ä¹Ÿæ— æ³•ç»™å‡ºæ­£ç¡®çš„æç¤ºã€‚ äºæ˜¯ Python 3 æä¾›äº†ä¸€ä¸ªæ–°çš„ç‰¹æ€§ï¼š å‡½æ•°æ³¨è§£ ä¹Ÿå°±æ˜¯æ–‡ç« å¼€å¤´çš„è¿™ä¸ªä¾‹å­ï¼š def add(x:int, y:int) -> int: return x + y ç”¨ : ç±»å‹ çš„å½¢å¼æŒ‡å®šå‡½æ•°çš„å‚æ•°ç±»å‹ï¼Œç”¨ -> ç±»å‹ çš„å½¢å¼æŒ‡å®šå‡½æ•°çš„è¿”å›å€¼ç±»å‹ã€‚ ç„¶åç‰¹åˆ«è¦å¼ºè°ƒçš„æ˜¯ï¼ŒPython è§£é‡Šå™¨å¹¶ä¸ä¼šå› ä¸ºè¿™äº›æ³¨è§£è€Œæä¾›é¢å¤–çš„æ ¡éªŒï¼Œæ²¡æœ‰ä»»ä½•çš„ç±»å‹æ£€æŸ¥å·¥ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›ç±»å‹æ³¨è§£åŠ ä¸åŠ ï¼Œå¯¹ä½ çš„ä»£ç æ¥è¯´æ²¡æœ‰ä»»ä½•å½±å“ï¼š è¾“å‡ºï¼š ä½†è¿™ä¹ˆåšçš„å¥½å¤„æ˜¯ï¼š è®©åˆ«çš„ç¨‹åºå‘˜çœ‹å¾—æ›´æ˜ç™½ è®© IDE äº†è§£ç±»å‹ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„ä»£ç æç¤ºã€è¡¥å…¨å’Œè¯­æ³•æ£€æŸ¥ï¼ˆåŒ…æ‹¬ç±»å‹æ£€æŸ¥ï¼Œå¯ä»¥çœ‹åˆ° str å’Œ float ç±»å‹çš„å‚æ•°è¢«é«˜äº®æç¤ºï¼‰ åœ¨å‡½æ•°çš„ __annotations__ å±æ€§ä¸­ä¼šæœ‰ä½ è®¾å®šçš„æ³¨è§£ï¼š è¾“å‡ºï¼š åœ¨ Python 3.6 ä¸­ï¼Œåˆå¼•å…¥äº†å¯¹å˜é‡ç±»å‹è¿›è¡Œæ³¨è§£çš„æ–¹æ³•ï¼š a: int = 123 b: str = 'hello' æ›´è¿›ä¸€æ­¥ï¼Œå¦‚æœä½ éœ€è¦æŒ‡æ˜ä¸€ä¸ªå…¨éƒ¨ç”±æ•´æ•°ç»„æˆçš„åˆ—è¡¨ï¼š from typing import List l: List[int] = [1, 2, 3] ä½†åŒæ ·ï¼Œè¿™äº›ä»…ä»…æ˜¯â€œæ³¨è§£â€ï¼Œä¸ä¼šå¯¹ä»£ç äº§ç”Ÿä»»ä½•å½±å“ã€‚ ä¸è¿‡ï¼Œä½ å¯ä»¥é€šè¿‡ mypy åº“æ¥æ£€éªŒæœ€ç»ˆä»£ç æ˜¯å¦ç¬¦åˆæ³¨è§£ã€‚ å®‰è£… mypyï¼š pip install mypy æ‰§è¡Œä»£ç ï¼š mypy test.py å¦‚æœç±»å‹éƒ½ç¬¦åˆï¼Œåˆ™ä¸ä¼šæœ‰ä»»ä½•è¾“å‡ºï¼Œå¦åˆ™å°±ä¼šç»™å‡ºç±»ä¼¼è¾“å‡ºï¼š è¿™äº›æ–°ç‰¹æ€§ä¹Ÿè®¸ä½ å¹¶ä¸ä¼šåœ¨ä»£ç ä¸­ä½¿ç”¨ï¼Œä¸è¿‡å½“ä½ åœ¨åˆ«äººçš„ä»£ç ä¸­çœ‹åˆ°æ—¶ï¼Œè¯·æŒ‰ç…§å¯¹æ–¹çš„çº¦å®šè¿›è¡Œèµ‹å€¼æˆ–è°ƒç”¨ã€‚ å½“ç„¶ï¼Œä¹Ÿä¸æ’é™¤ Python ä»¥åçš„ç‰ˆæœ¬æŠŠç±»å‹æ£€æŸ¥åšåˆ°è§£é‡Šå™¨é‡Œï¼Œè°çŸ¥é“å‘¢ã€‚ "},"code_technique/python/sort.html":{"url":"code_technique/python/sort.html","title":"pythonå¸¸è§æ’åº","keywords":"","body":"æ’åºç®—æ³•ç›¸å…³æ¦‚å¿µ ç¨³å®šï¼šå¦‚æœaåŸæœ¬åœ¨bå‰é¢ï¼Œè€Œa=bï¼Œæ’åºä¹‹åaä»ç„¶åœ¨bçš„å‰é¢ï¼› ä¸ç¨³å®šï¼šå¦‚æœaåŸæœ¬åœ¨bçš„å‰é¢ï¼Œè€Œa=bï¼Œæ’åºä¹‹åaå¯èƒ½ä¼šå‡ºç°åœ¨bçš„åé¢ï¼› å†…æ’åºï¼šæ‰€æœ‰æ’åºæ“ä½œéƒ½åœ¨å†…å­˜ä¸­å®Œæˆï¼› å¤–æ’åºï¼šç”±äºæ•°æ®å¤ªå¤§ï¼Œå› æ­¤æŠŠæ•°æ®æ”¾åœ¨ç£ç›˜ä¸­ï¼Œè€Œæ’åºé€šè¿‡ç£ç›˜å’Œå†…å­˜çš„æ•°æ®ä¼ è¾“æ‰èƒ½è¿›è¡Œï¼› å¸¸è§æ’åº å†’æ³¡æ’åº å†’æ³¡æ’åºé‡å¤åœ°èµ°è®¿è¿‡è¦æ’åºçš„æ•°åˆ—ï¼Œä¸€æ¬¡æ¯”è¾ƒä¸¤ä¸ªå…ƒç´ ï¼Œå¦‚æœä»–ä»¬çš„é¡ºåºé”™è¯¯å°±æŠŠä»–ä»¬äº¤æ¢è¿‡æ¥ã€‚èµ°è®¿æ•°åˆ—çš„å·¥ä½œæ˜¯é‡å¤åœ°è¿›è¡Œç›´åˆ°æ²¡æœ‰å†éœ€è¦äº¤æ¢ã€‚å› ä¸ºæ¯æ¬¡éå†ï¼Œæœ€å¤§çš„å…ƒç´ éƒ½ä¼šè¢«é€åˆ°æœ€å³ç«¯ï¼Œæ•…åå†’æ³¡æ’åºã€‚ æ­¥éª¤ï¼š æ¯”è¾ƒç›¸é‚»çš„å…ƒç´ ã€‚å¦‚æœç¬¬ä¸€ä¸ªæ¯”ç¬¬äºŒä¸ªå¤§ï¼Œå°±äº¤æ¢ä»–ä»¬ä¸¤ä¸ªã€‚ å¯¹æ¯ä¸€å¯¹ç›¸é‚»å…ƒç´ ä½œåŒæ ·çš„å·¥ä½œï¼Œä»å¼€å§‹ç¬¬ä¸€å¯¹åˆ°ç»“å°¾çš„æœ€åä¸€å¯¹ã€‚åœ¨è¿™ä¸€ç‚¹ï¼Œæœ€åçš„å…ƒç´ åº”è¯¥ä¼šæ˜¯æœ€å¤§çš„æ•°ã€‚ é’ˆå¯¹æ‰€æœ‰çš„å…ƒç´ é‡å¤ä»¥ä¸Šçš„æ­¥éª¤ï¼Œé™¤äº†æœ€åä¸€ä¸ªã€‚ æŒç»­æ¯æ¬¡å¯¹è¶Šæ¥è¶Šå°‘çš„å…ƒç´ é‡å¤ä¸Šé¢çš„æ­¥éª¤ï¼Œç›´åˆ°æ²¡æœ‰ä»»ä½•ä¸€å¯¹æ•°å­—éœ€è¦æ¯”è¾ƒã€‚ ä»£ç å®ç°ï¼š def bubble_sort(nums): size = len(nums) for i in range(size): for j in range(size-i-1): if nums[j] > nums[j+1]: nums[j+1], nums[j] = nums[j], nums[j+1] return nums æˆ‘ä»¬å¯ä»¥è€ƒè™‘è®¾ç½®ä¸€æ ‡å¿—æ€§å˜é‡posï¼Œç”¨äºè®°å½•æ¯è¶Ÿæ’åºä¸­æœ€åä¸€æ¬¡è¿›è¡Œäº¤æ¢çš„ä½ç½®ã€‚ç”±äºposä½ç½®ä¹‹åçš„è®°å½•å‡å·²äº¤æ¢åˆ°ä½ï¼Œæ•…åœ¨è¿›è¡Œä¸‹ä¸€è¶Ÿæ’åºæ—¶åªè¦æ‰«æåˆ°posä½ç½®å³å¯ã€‚ æ”¹è¿›åçš„å†’æ³¡æ’åºï¼š def bubble_sort2(nums): size = len(nums) i = size - 1 while i > 0: pos = 0 for j in range(i): if nums[j] > nums[j+1]: pos = j nums[j], nums[j+1] = nums[j+1], nums[j] i = pos return nums å†’æ³¡æ’åºåŠ¨å›¾æ¼”ç¤ºï¼š é€‰æ‹©æ’åº é€‰æ‹©æ’åº(Selection-sort)æ˜¯ä¸€ç§ç®€å•ç›´è§‚çš„æ’åºç®—æ³•ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯ï¼šé¦–å…ˆåœ¨æœªæ’åºåºåˆ—ä¸­æ‰¾åˆ°æœ€å°ï¼ˆå¤§ï¼‰å…ƒç´ ï¼Œå­˜æ”¾åˆ°æ’åºåºåˆ—çš„èµ·å§‹ä½ç½®ï¼Œç„¶åï¼Œå†ä»å‰©ä½™æœªæ’åºå…ƒç´ ä¸­ç»§ç»­å¯»æ‰¾æœ€å°ï¼ˆå¤§ï¼‰å…ƒç´ ï¼Œç„¶åæ”¾åˆ°å·²æ’åºåºåˆ—çš„æœ«å°¾ã€‚ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰å…ƒç´ å‡æ’åºå®Œæ¯•ã€‚ ä»£ç å®ç°å¦‚ä¸‹ï¼š def select_sort(nums): size = len(nums) for i in range(size-1): # æ‰¾å‡ºæœ€å°çš„æ•° min_index = i for j in range(i+1, size): if nums[j] é€‰æ‹©æ’åºåŠ¨å›¾æ¼”ç¤ºå¦‚ä¸‹ï¼š æ’å…¥æ’åº æ’å…¥æ’åºï¼ˆInsertion-Sortï¼‰çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡æ„å»ºæœ‰åºåºåˆ—ï¼Œå¯¹äºæœªæ’åºæ•°æ®ï¼Œåœ¨å·²æ’åºåºåˆ—ä¸­ä»åå‘å‰æ‰«æï¼Œæ‰¾åˆ°ç›¸åº”ä½ç½®å¹¶æ’å…¥ã€‚ å…·ä½“æ­¥éª¤ï¼š ä»ç¬¬ä¸€ä¸ªå…ƒç´ å¼€å§‹ï¼Œè¯¥å…ƒç´ å¯ä»¥è®¤ä¸ºå·²ç»è¢«æ’åºï¼› å–å‡ºä¸‹ä¸€ä¸ªå…ƒç´ ï¼Œåœ¨å·²ç»æ’åºçš„å…ƒç´ åºåˆ—ä¸­ä»åå‘å‰æ‰«æï¼› å¦‚æœè¯¥å…ƒç´ ï¼ˆå·²æ’åºï¼‰å¤§äºæ–°å…ƒç´ ï¼Œå°†è¯¥å…ƒç´ ç§»åˆ°ä¸‹ä¸€ä½ç½®ï¼› é‡å¤æ­¥éª¤3ï¼Œç›´åˆ°æ‰¾åˆ°å·²æ’åºçš„å…ƒç´ å°äºæˆ–è€…ç­‰äºæ–°å…ƒç´ çš„ä½ç½®ï¼› å°†æ–°å…ƒç´ æ’å…¥åˆ°è¯¥ä½ç½®åï¼› é‡å¤æ­¥éª¤2~5ã€‚ ä»£ç å®ç°ï¼š def insertion_sort(nums): size = len(nums) for i in range(1, size): cur_val = nums[i] j = i - 1 while j >= 0 and nums[j] > cur_val: nums[j+1] = nums[j] j -= 1 nums[j+1] = cur_val # æ‰¾åˆ°ä½ç½®è¿›è¡Œæ’å…¥ return nums å¯ä»¥è€ƒè™‘ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ¥å¯»æ‰¾æ’å…¥çš„ä½ç½®ï¼š def insertion_sort2(nums): size = len(nums) for i in range(1, size): val = nums[i] left, right = 0, i-1 while left æ’å…¥æ’åºåŠ¨å›¾æ¼”ç¤ºå¦‚ä¸‹ï¼š å½’å¹¶æ’åº å½’å¹¶æ’åºæ˜¯å»ºç«‹åœ¨å½’å¹¶æ“ä½œä¸Šçš„ä¸€ç§æœ‰æ•ˆçš„æ’åºç®—æ³•ã€‚è¯¥ç®—æ³•æ˜¯é‡‡ç”¨åˆ†æ²»æ³•ï¼ˆDivide and Conquerï¼‰çš„ä¸€ä¸ªéå¸¸å…¸å‹çš„åº”ç”¨ã€‚å½’å¹¶æ’åºæ˜¯ä¸€ç§ç¨³å®šçš„æ’åºæ–¹æ³•ã€‚å°†å·²æœ‰åºçš„å­åºåˆ—åˆå¹¶ï¼Œå¾—åˆ°å®Œå…¨æœ‰åºçš„åºåˆ—ï¼›å³å…ˆä½¿æ¯ä¸ªå­åºåˆ—æœ‰åºï¼Œå†ä½¿å­åºåˆ—æ®µé—´æœ‰åºã€‚è‹¥å°†ä¸¤ä¸ªæœ‰åºè¡¨åˆå¹¶æˆä¸€ä¸ªæœ‰åºè¡¨ï¼Œç§°ä¸º2-è·¯å½’å¹¶ã€‚ ä»£ç å®ç°å¦‚ä¸‹ï¼š def merge_sort(nums): size = len(nums) if size å…¶åŠ¨å›¾æ¼”ç¤ºå¦‚ä¸‹ï¼š å¿«é€Ÿæ’åº å¿«é€Ÿæ’åºæ˜¯å¤„ç†å¤§æ•°æ®æœ€å¿«çš„æ’åºç®—æ³•ä¹‹ä¸€ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œé€šè¿‡ä¸€è¶Ÿæ’åºå°†å¾…æ’è®°å½•åˆ†éš”æˆç‹¬ç«‹çš„ä¸¤éƒ¨åˆ†ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†è®°å½•çš„å…³é”®å­—å‡æ¯”å¦ä¸€éƒ¨åˆ†çš„å…³é”®å­—å°ï¼Œåˆ™å¯åˆ†åˆ«å¯¹è¿™ä¸¤éƒ¨åˆ†è®°å½•ç»§ç»­è¿›è¡Œæ’åºï¼Œä»¥è¾¾åˆ°æ•´ä¸ªåºåˆ—æœ‰åºã€‚ å¿«é€Ÿæ’åºåŸºæœ¬æ­¥éª¤ï¼š ä»æ•°åˆ—ä¸­æŒ‘å‡ºä¸€ä¸ªå…ƒç´ ï¼Œç§°ä¸º \"åŸºå‡†\"ï¼ˆpivotï¼‰ï¼› é‡æ–°æ’åºæ•°åˆ—ï¼Œæ‰€æœ‰å…ƒç´ æ¯”åŸºå‡†å€¼å°çš„æ‘†æ”¾åœ¨åŸºå‡†å‰é¢ï¼Œæ‰€æœ‰å…ƒç´ æ¯”åŸºå‡†å€¼å¤§çš„æ‘†åœ¨åŸºå‡†çš„åé¢ï¼ˆç›¸åŒçš„æ•°å¯ä»¥åˆ°ä»»ä¸€è¾¹ï¼‰ã€‚åœ¨è¿™ä¸ªåˆ†åŒºé€€å‡ºä¹‹åï¼Œè¯¥åŸºå‡†å°±å¤„äºæ•°åˆ—çš„ä¸­é—´ä½ç½®ã€‚è¿™ä¸ªç§°ä¸ºåˆ†åŒºï¼ˆpartitionï¼‰æ“ä½œï¼› é€’å½’åœ°ï¼ˆrecursiveï¼‰æŠŠå°äºåŸºå‡†å€¼å…ƒç´ çš„å­æ•°åˆ—å’Œå¤§äºåŸºå‡†å€¼å…ƒç´ çš„å­æ•°åˆ—æ’åºã€‚ å…¶ä»£ç å®ç°å¦‚ä¸‹ï¼š def partition(nums, left, right): pivot = left # ä½¿ç”¨å·¦ç«¯å…ƒç´ ä½œä¸ºåŸºå‡† for i in range(left+1, right+1): if nums[i] = right: return pivot = partition(nums, left, right) quick_sort_helper(nums, left, pivot-1) quick_sort_helper(nums, pivot+1, right) return quick_sort_helper(nums, left, right) å¦‚æœä¸è¦æ±‚åœ¨åŸåœ°ä¿®æ”¹æ•°ç»„ï¼š def quick_sort2(arr): if len(arr) = arr[0]]) å¿«é€Ÿæ’åºçš„åŠ¨å›¾æ¼”ç¤ºå¦‚ä¸‹ï¼š åé¢çš„å‡ ç§æ’åºæ–¹æ³•æ¯”è¾ƒå°‘è§ï¼Œä»…åœ¨æ¦‚å¿µä¸Šè¿›è¡Œè®²è§£ã€‚ å¸Œå°”æ’åº å…ˆå°†æ•´ä¸ªå¾…æ’å…ƒç´ åºåˆ—åˆ†å‰²æˆè‹¥å¹²å­åºåˆ—ï¼ˆç”±ç›¸éš”æŸä¸ªâ€œå¢é‡â€çš„å…ƒç´ ç»„æˆçš„ï¼‰åˆ†åˆ«è¿›è¡Œç›´æ¥æ’å…¥æ’åºï¼Œç„¶åä¾æ¬¡ç¼©å‡å¢é‡å†è¿›è¡Œæ’åºï¼Œå¾…æ•´ä¸ªåºåˆ—ä¸­çš„å…ƒç´ åŸºæœ¬æœ‰åºï¼ˆå¢é‡è¶³å¤Ÿå°ï¼‰æ—¶ï¼Œå†å¯¹å…¨ä½“å…ƒç´ è¿›è¡Œä¸€æ¬¡ç›´æ¥æ’å…¥æ’åºï¼ˆå¢é‡ä¸º1ï¼‰ã€‚ å †æ’åº å †æ’åºï¼ˆHeapsortï¼‰æ˜¯æŒ‡åˆ©ç”¨å †è¿™ç§æ•°æ®ç»“æ„æ‰€è®¾è®¡çš„ä¸€ç§æ’åºç®—æ³•ã€‚å †ç§¯æ˜¯ä¸€ä¸ªè¿‘ä¼¼å®Œå…¨äºŒå‰æ ‘çš„ç»“æ„ï¼Œå¹¶åŒæ—¶æ»¡è¶³å †ç§¯çš„æ€§è´¨ï¼šå³å­ç»“ç‚¹çš„é”®å€¼æˆ–ç´¢å¼•æ€»æ˜¯å°äºï¼ˆæˆ–è€…å¤§äºï¼‰å®ƒçš„çˆ¶èŠ‚ç‚¹ã€‚ æ­¥éª¤ï¼š åˆ›å»ºæœ€å¤§å †:å°†å †æ‰€æœ‰æ•°æ®é‡æ–°æ’åºï¼Œä½¿å…¶æˆä¸ºæœ€å¤§å † æœ€å¤§å †è°ƒæ•´:ä½œç”¨æ˜¯ä¿æŒæœ€å¤§å †çš„æ€§è´¨ï¼Œæ˜¯åˆ›å»ºæœ€å¤§å †çš„æ ¸å¿ƒå­ç¨‹åº å †æ’åº:ç§»é™¤ä½åœ¨ç¬¬ä¸€ä¸ªæ•°æ®çš„æ ¹èŠ‚ç‚¹ï¼Œå¹¶åšæœ€å¤§å †è°ƒæ•´çš„é€’å½’è¿ç®— è®¡æ•°æ’åº è®¡æ•°æ’åºä½¿ç”¨ä¸€ä¸ªé¢å¤–çš„æ•°ç»„Cï¼Œå…¶ä¸­ç¬¬iä¸ªå…ƒç´ æ˜¯å¾…æ’åºæ•°ç»„Aä¸­å€¼ç­‰äºiçš„å…ƒç´ çš„ä¸ªæ•°ã€‚ç„¶åæ ¹æ®æ•°ç»„Cæ¥å°†Aä¸­çš„å…ƒç´ æ’åˆ°æ­£ç¡®çš„ä½ç½®ã€‚ ç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š æ‰¾å‡ºå¾…æ’åºçš„æ•°ç»„ä¸­æœ€å¤§å’Œæœ€å°çš„å…ƒç´  ç»Ÿè®¡æ•°ç»„ä¸­æ¯ä¸ªå€¼ä¸ºiçš„å…ƒç´ å‡ºç°çš„æ¬¡æ•°ï¼Œå­˜å…¥æ•°ç»„Cçš„ç¬¬ié¡¹ å¯¹æ‰€æœ‰çš„è®¡æ•°ç´¯åŠ ï¼ˆä»Cä¸­çš„ä½ç½®ä¸º1çš„å…ƒç´ å¼€å§‹ï¼Œæ¯ä¸€é¡¹å’Œå‰ä¸€é¡¹ç›¸åŠ ï¼‰ åå‘å¡«å……ç›®æ ‡æ•°ç»„ï¼šå°†æ¯ä¸ªå…ƒç´ iæ”¾åœ¨æ–°æ•°ç»„çš„ç¬¬C(i)é¡¹ï¼Œæ¯æ”¾ä¸€ä¸ªå…ƒç´ å°±å°†C(i)å‡å»1 ç”±äºç”¨æ¥è®¡æ•°çš„æ•°ç»„Cçš„é•¿åº¦å–å†³äºå¾…æ’åºæ•°ç»„ä¸­æ•°æ®çš„èŒƒå›´ï¼ˆç­‰äºå¾…æ’åºæ•°ç»„çš„æœ€å¤§å€¼ä¸æœ€å°å€¼çš„å·®åŠ ä¸Š1ï¼‰ï¼Œè¿™ä½¿å¾—è®¡æ•°æ’åºå¯¹äºæ•°æ®èŒƒå›´å¾ˆå¤§çš„æ•°ç»„ï¼Œéœ€è¦å¤§é‡æ—¶é—´å’Œå†…å­˜ã€‚ æ¡¶æ’åº æ¡¶æ’åº (Bucket sort)æˆ–æ‰€è°“çš„ç®±æ’åºï¼Œæ˜¯ä¸€ä¸ªæ’åºç®—æ³•ï¼Œå·¥ä½œçš„åŸç†æ˜¯å°†æ•°ç»„åˆ†åˆ°æœ‰é™æ•°é‡çš„æ¡¶å­é‡Œã€‚æ¯ä¸ªæ¡¶å­å†ä¸ªåˆ«æ’åºï¼ˆæœ‰å¯èƒ½å†ä½¿ç”¨åˆ«çš„æ’åºç®—æ³•æˆ–æ˜¯ä»¥é€’å½’æ–¹å¼ç»§ç»­ä½¿ç”¨æ¡¶æ’åºè¿›è¡Œæ’åºï¼‰ æ¡¶æ’åºä»¥ä¸‹åˆ—ç¨‹åºè¿›è¡Œï¼š è®¾ç½®ä¸€ä¸ªå®šé‡çš„æ•°ç»„å½“ä½œç©ºæ¡¶å­ã€‚ å¯»è®¿ä¸²è¡Œï¼Œå¹¶ä¸”æŠŠé¡¹ç›®ä¸€ä¸ªä¸€ä¸ªæ”¾åˆ°å¯¹åº”çš„æ¡¶å­å»ã€‚ï¼ˆhashï¼‰ å¯¹æ¯ä¸ªä¸æ˜¯ç©ºçš„æ¡¶å­è¿›è¡Œæ’åºã€‚ ä»ä¸æ˜¯ç©ºçš„æ¡¶å­é‡ŒæŠŠé¡¹ç›®å†æ”¾å›åŸæ¥çš„ä¸²è¡Œä¸­ã€‚ åŸºæ•°æ’åº åŸºæ•°æ’åºï¼ˆè‹±è¯­ï¼šRadix sortï¼‰æ˜¯ä¸€ç§éæ¯”è¾ƒå‹æ•´æ•°æ’åºç®—æ³•ï¼Œå…¶åŸç†æ˜¯å°†æ•´æ•°æŒ‰ä½æ•°åˆ‡å‰²æˆä¸åŒçš„æ•°å­—ï¼Œç„¶åæŒ‰æ¯ä¸ªä½æ•°åˆ†åˆ«æ¯”è¾ƒã€‚ç”±äºæ•´æ•°ä¹Ÿå¯ä»¥è¡¨è¾¾å­—ç¬¦ä¸²ï¼ˆæ¯”å¦‚åå­—æˆ–æ—¥æœŸï¼‰å’Œç‰¹å®šæ ¼å¼çš„æµ®ç‚¹æ•°ï¼Œæ‰€ä»¥åŸºæ•°æ’åºä¹Ÿä¸æ˜¯åªèƒ½ä½¿ç”¨äºæ•´æ•°ã€‚ å®ƒæ˜¯è¿™æ ·å®ç°çš„ï¼šå°†æ‰€æœ‰å¾…æ¯”è¾ƒæ•°å€¼ï¼ˆæ­£æ•´æ•°ï¼‰ç»Ÿä¸€ä¸ºåŒæ ·çš„æ•°ä½é•¿åº¦ï¼Œæ•°ä½è¾ƒçŸ­çš„æ•°å‰é¢è¡¥é›¶ã€‚ç„¶åï¼Œä»æœ€ä½ä½å¼€å§‹ï¼Œä¾æ¬¡è¿›è¡Œä¸€æ¬¡æ’åºã€‚è¿™æ ·ä»æœ€ä½ä½æ’åºä¸€ç›´åˆ°æœ€é«˜ä½æ’åºå®Œæˆä»¥å, æ•°åˆ—å°±å˜æˆä¸€ä¸ªæœ‰åºåºåˆ—ã€‚ å»¶ä¼¸-å¤–éƒ¨æ’åº ä»æ˜¯å¦ä½¿ç”¨å¤–å­˜æ–¹é¢æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ’åºç®—æ³•åˆ†ä¸ºå†…éƒ¨æ’åºå’Œå¤–éƒ¨æ’åºï¼š ä¸Šé¢è®²çš„åç§æ’åºç®—æ³•éƒ½å±äºå†…éƒ¨æ’åºç®—æ³•ï¼Œä¹Ÿå°±æ˜¯æ’åºçš„æ•´ä¸ªè¿‡ç¨‹éƒ½åœ¨å†…å­˜ä¸­å®Œæˆã€‚è€Œå½“å¾…æ’åºçš„æ–‡ä»¶æ¯”å†…å­˜çš„å¯ä½¿ç”¨å®¹é‡è¿˜å¤§æ—¶ï¼Œæ–‡ä»¶æ— æ³•ä¸€æ¬¡æ€§æ”¾åˆ°å†…å­˜ä¸­è¿›è¡Œæ’åºï¼Œéœ€è¦å€ŸåŠ©äºå¤–éƒ¨å­˜å‚¨å™¨ï¼ˆä¾‹å¦‚ç¡¬ç›˜ã€Uç›˜ã€å…‰ç›˜ï¼‰ï¼Œè¿™æ—¶å°±éœ€è¦ç”¨åˆ°å¤–éƒ¨æ’åºç®—æ³•æ¥è§£å†³ã€‚ä¸‹é¢ç®€å•ä»‹ç»ä¸€ä¸‹å¤–éƒ¨æ’åºç®—æ³•ã€‚ å¤–éƒ¨æ’åºç®—æ³•ç”±ä¸¤ä¸ªé˜¶æ®µæ„æˆï¼š æŒ‰ç…§å†…å­˜å¤§å°ï¼Œå°†å¤§æ–‡ä»¶åˆ†æˆè‹¥å¹²é•¿åº¦ä¸º l çš„å­æ–‡ä»¶ï¼ˆl åº”å°äºå†…å­˜çš„å¯ä½¿ç”¨å®¹é‡ï¼‰ï¼Œç„¶åå°†å„ä¸ªå­æ–‡ä»¶ä¾æ¬¡è¯»å…¥å†…å­˜ï¼Œä½¿ç”¨é€‚å½“çš„å†…éƒ¨æ’åºç®—æ³•å¯¹å…¶è¿›è¡Œæ’åºï¼ˆæ’å¥½åºçš„å­æ–‡ä»¶ç»Ÿç§°ä¸ºâ€œå½’å¹¶æ®µâ€æˆ–è€…â€œé¡ºæ®µâ€ï¼‰ï¼Œå°†æ’å¥½åºçš„å½’å¹¶æ®µé‡æ–°å†™å…¥å¤–å­˜ï¼Œä¸ºä¸‹ä¸€ä¸ªå­æ–‡ä»¶æ’åºè…¾å‡ºå†…å­˜ç©ºé—´ï¼› å¯¹å¾—åˆ°çš„é¡ºæ®µè¿›è¡Œåˆå¹¶ï¼Œç›´è‡³å¾—åˆ°æ•´ä¸ªæœ‰åºçš„æ–‡ä»¶ä¸ºæ­¢ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬è¦å¯¹ä¸€ä¸ªå¤§æ–‡ä»¶ï¼ˆæ— æ³•æ”¾è¿›å†…å­˜ï¼‰è¿›è¡Œæ’åºï¼Œå¯ä»¥å°†å…¶åˆ†æˆå¤šä¸ªå¤§å°å¯ä»¥æ”¾è¿›å†…å­˜çš„ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åå°†è¿™äº›è¾ƒå°çš„ä¸´æ—¶æ–‡ä»¶ä¾æ¬¡è¿›å…¥å†…å­˜ï¼Œé‡‡å–é€‚å½“çš„å†…å­˜æ’åºç®—æ³•å¯¹å…¶ä¸­çš„è®°å½•è¿›è¡Œæ’åºï¼Œå°†å¾—åˆ°çš„æœ‰åºæ–‡ä»¶ï¼ˆåˆå§‹å½’å¹¶æ®µï¼‰ç§»è‡³å¤–å­˜ï¼›ä¹‹åå†å¯¹è¿™äº›æ’åºå¥½çš„ä¸´æ—¶æ–‡ä»¶ä¸¤ä¸¤å½’å¹¶ï¼Œç›´è‡³å¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„æœ‰åºæ–‡ä»¶ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š æ€»ç»“ å…¶ä¸­ï¼Œnè¡¨ç¤ºæ•°æ®è§„æ¨¡ï¼Œkè¡¨ç¤ºæ¡¶çš„ä¸ªæ•°ï¼ŒIn-placeè¡¨ç¤ºä¸å ç”¨é¢å¤–å†…å­˜ï¼ŒOut-placeè¡¨ç¤ºå ç”¨é¢å¤–å†…å­˜ã€‚ "},"code_technique/python/opencv.html":{"url":"code_technique/python/opencv.html","title":"opencv-pythonæé€Ÿå…¥é—¨","keywords":"","body":"opencv-python æé€Ÿå…¥é—¨ ä»€ä¹ˆæ˜¯OpenCV-Python? OpenCVæ˜¯ä¸€ä¸ªå¼€æºçš„è®¡ç®—æœºè§†è§‰ï¼ˆcomputer visionï¼‰å’Œæœºå™¨å­¦ä¹ åº“ã€‚å®ƒæ‹¥æœ‰è¶…è¿‡2500ä¸ªä¼˜åŒ–ç®—æ³•ï¼ŒåŒ…æ‹¬ç»å…¸å’Œæœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒæœ‰å¾ˆå¤šè¯­è¨€æ¥å£ï¼ŒåŒ…æ‹¬Pythonã€Javaã€c++å’ŒMatlabã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬å°†å¤„ç†Pythonæ¥å£ã€‚ å®‰è£… åœ¨Windowsä¸Š, è¯·åœ¨è¿™é‡ŒæŸ¥çœ‹æŒ‡å—ã€‚åœ°å€ï¼šhttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_setup_in_windows/py_setup_in_windows.html åœ¨ Linuxä¸Š, è¯·åœ¨è¿™é‡ŒæŸ¥çœ‹æŒ‡å—ã€‚åœ°å€ï¼šhttps://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html å›¾åƒå¯¼å…¥&æ˜¾ç¤º è­¦å‘Š1: é€šè¿‡openCVè¯»å–å›¾åƒæ—¶ï¼Œå®ƒä¸æ˜¯ä»¥RGB é¢œè‰²ç©ºé—´æ¥è¯»å–ï¼Œè€Œæ˜¯ä»¥BGR é¢œè‰²ç©ºé—´ã€‚æœ‰æ—¶å€™è¿™å¯¹ä½ æ¥è¯´ä¸æ˜¯é—®é¢˜ï¼Œåªæœ‰å½“ä½ æƒ³åœ¨å›¾ç‰‡ä¸­æ·»åŠ ä¸€äº›é¢œè‰²æ—¶ï¼Œä½ æ‰ä¼šé‡åˆ°é—®é¢˜ã€‚ æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆ: å°†Râ€Šâ€”â€Šç¬¬ä¸€ä¸ªé¢œè‰²å€¼(çº¢è‰²)å’ŒB â€Šâ€”â€Šç¬¬ä¸‰ä¸ªé¢œè‰²å€¼(è“è‰²) äº¤æ¢, è¿™æ ·çº¢è‰²å°±æ˜¯ (0,0,255) è€Œä¸æ˜¯(255,0,0)ã€‚ å°†é¢œè‰²ç©ºé—´å˜æˆRGB: ä½¿ç”¨rgb_imageä»£æ›¿imageç»§ç»­å¤„ç†ä»£ç ã€‚ è­¦å‘Š2: è¦å…³é—­æ˜¾ç¤ºå›¾åƒçš„çª—å£ï¼Œè¯·æŒ‰ä»»æ„æŒ‰é’®ã€‚å¦‚æœä½ ä½¿ç”¨å…³é—­æŒ‰é’®ï¼Œå®ƒå¯èƒ½ä¼šå¯¼è‡´çª—å£å†»ç»“(æˆ‘åœ¨Jupyterç¬”è®°æœ¬ä¸Šè¿è¡Œä»£ç æ—¶å‘ç”Ÿäº†è¿™ç§æƒ…å†µ)ã€‚ ä¸ºäº†ç®€å•èµ·è§ï¼Œåœ¨æ•´ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥æŸ¥çœ‹å›¾åƒ: æ¥æºï¼šPixabay è£å‰ª æ¥æºï¼šPixabay è£å‰ªåçš„ç‹—ç‹— å…¶ä¸­ï¼š image[10:500,500:200] æ˜¯ image[y:y+h,x:x+w]ã€‚ è°ƒæ•´å¤§å° æ¥æºï¼šPexels è°ƒæ•´å¤§å°åˆ°20%å è¿™ä¸ªè°ƒæ•´å¤§å°å‡½æ•°ä¼šä¿æŒåŸå§‹å›¾åƒçš„å°ºå¯¸æ¯”ä¾‹ã€‚ æ›´å¤šå›¾åƒç¼©æ”¾å‡½æ•°ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œã€‚ï¼ˆhttps://www.tutorialkart.com/opencv/python/opencv-python-resize-image/ ï¼‰ æ—‹è½¬ å·¦å›¾: å›¾ç‰‡æ¥è‡ªPexelsçš„Jonathan Meyerã€‚å³å›¾: è¿›è¡Œ180åº¦æ—‹è½¬ä¹‹åçš„ç‹—ç‹—ã€‚ image.shapeè¾“å‡ºé«˜åº¦ã€å®½åº¦å’Œé€šé“ã€‚Mæ˜¯æ—‹è½¬çŸ©é˜µâ€”â€”å®ƒå°†å›¾åƒå›´ç»•å…¶ä¸­å¿ƒæ—‹è½¬180åº¦ã€‚ -veè¡¨ç¤ºé¡ºæ—¶é’ˆæ—‹è½¬å›¾åƒçš„è§’åº¦ & +veé€†è¡¨ç¤ºé€†æ—¶é’ˆæ—‹è½¬å›¾åƒçš„è§’åº¦ã€‚ ç°åº¦å’Œé˜ˆå€¼(é»‘ç™½æ•ˆæœ) æ¥æºï¼šPexels gray_image æ˜¯ç°åº¦å›¾åƒçš„å•é€šé“ç‰ˆæœ¬ã€‚ è¿™ä¸ªthresholdå‡½æ•°å°†æŠŠæ‰€æœ‰æ¯”127æ·±(å°)çš„åƒç´ ç‚¹é˜´å½±å€¼è®¾å®šä¸º0ï¼Œæ‰€æœ‰æ¯”127äº®(å¤§)çš„åƒç´ ç‚¹é˜´å½±å€¼è®¾å®šä¸º255ã€‚ å¦ä¸€ä¸ªä¾‹å­: è¿™å°†æŠŠæ‰€æœ‰é˜´å½±å€¼å°äº150çš„åƒç´ ç‚¹è®¾å®šä¸º10å’Œæ‰€æœ‰å¤§äº150çš„åƒç´ ç‚¹è®¾å®šä¸º200ã€‚ æ›´å¤šæœ‰å…³thresholdingå‡½æ•°çš„å†…å®¹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œã€‚ï¼ˆhttps://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html ï¼‰ å·¦å›¾ï¼šç°é˜¶ç‹—ç‹—ã€‚å³å›¾ï¼šé»‘ç™½ç‹—ç‹—ã€‚ æ¨¡ç³Š/å¹³æ»‘ å·¦å›¾ï¼šå›¾åƒæ¥è‡ªPixabayã€‚å³å›¾ï¼šæ¨¡ç³Šåçš„ç‹—ç‹—ã€‚ é«˜æ–¯æ¨¡ç³Šå‡½æ•°æ¥å—3ä¸ªå‚æ•°: ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦æ¨¡ç³Šçš„å›¾åƒã€‚ ç¬¬äºŒä¸ªå‚æ•°å¿…é¡»æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªæ­£å¥‡æ•°ç»„æˆçš„å…ƒç»„ã€‚å½“å®ƒä»¬å¢åŠ ï¼Œæ¨¡ç³Šæ•ˆæœä¹Ÿä¼šå¢åŠ ã€‚ ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯sigmaXå’ŒsigmaYã€‚å½“å·¦è¾¹ä½äº0æ—¶ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨ä»å†…éƒ¨å¤§å°è®¡ç®—å‡ºæ¥ã€‚ æ›´å¤šå…³äºæ¨¡ç³Šå‡½æ•°çš„å†…å®¹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œã€‚ï¼ˆhttps://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html ï¼‰ åœ¨å›¾åƒä¸Šç»˜åˆ¶çŸ©å½¢æ¡†æˆ–è¾¹æ¡† å·¦å›¾ï¼šå›¾åƒæ¥è‡ªPixabayã€‚å³å›¾ï¼šè„¸ä¸Šæœ‰ä¸€ä¸ªçŸ©å½¢æ¡†çš„ç‹—ç‹—ã€‚ rectangleå‡½æ•°æ¥å—5ä¸ªå‚æ•°: ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å›¾åƒã€‚ ç¬¬äºŒä¸ªå‚æ•°æ˜¯x1, y1 -å·¦ä¸Šè§’åæ ‡ã€‚ ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯x2, y2 -å³ä¸‹è§’åæ ‡ã€‚ ç¬¬å››ä¸ªå‚æ•°æ˜¯çŸ©å½¢é¢œè‰²(GBR/RGBï¼Œå–å†³äºä½ å¦‚ä½•å¯¼å…¥å›¾åƒ)ã€‚ ç¬¬äº”ä¸ªå‚æ•°æ˜¯çŸ©å½¢çº¿å®½ã€‚ ç»˜åˆ¶ä¸€æ¡çº¿ å·¦å›¾ï¼šå›¾åƒæ¥è‡ªPixabayã€‚å³å›¾ï¼šä¸¤åªç‹—ç‹—ç”¨ä¸€æ¡çº¿åˆ†å¼€ã€‚ lineå‡½æ•°æ¥å—5ä¸ªå‚æ•°: ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦ç”»çš„çº¿æ‰€åœ¨çš„å›¾åƒã€‚ ç¬¬äºŒä¸ªå‚æ•°æ˜¯x1, y1ã€‚ ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯x2, y2ã€‚ ç¬¬å››ä¸ªå‚æ•°æ˜¯çº¿æ¡é¢œè‰²(GBR/RGBï¼Œå–å†³äºä½ å¦‚ä½•å¯¼å…¥å›¾åƒ)ã€‚ ç¬¬äº”ä¸ªå‚æ•°æ˜¯çº¿å®½ã€‚ åœ¨å›¾ç‰‡ä¸Šå†™å…¥æ–‡å­— å·¦å›¾ï¼šå›¾åƒæ¥è‡ªPixabayã€‚å³å›¾ï¼šä¸¤åªç‹—ç‹—ç”¨ä¸€æ¡çº¿åˆ†å¼€ã€‚ putTextå‡½æ•°æ¥å— ä¸ƒä¸ªå‚æ•°ï¼š ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦å†™å…¥æ–‡æœ¬çš„å›¾åƒã€‚ ç¬¬äºŒä¸ªå‚æ•°æ˜¯å¾…å†™å…¥æ–‡æœ¬ã€‚ ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯x, yâ€”â€”æ–‡æœ¬å¼€å§‹çš„å·¦ä¸‹è§’åæ ‡ã€‚ ç¬¬å››ä¸ªå‚æ•°æ˜¯å­—ä½“ç±»å‹ã€‚ ç¬¬äº”ä¸ªå‚æ•°æ˜¯å­—ä½“å¤§å°ã€‚ ç¬¬å…­ä¸ªå‚æ•°æ˜¯é¢œè‰²(GBR/RGBï¼Œå–å†³äºä½ å¦‚ä½•å¯¼å…¥å›¾åƒ)ã€‚ ç¬¬ä¸ƒä¸ªå‚æ•°æ˜¯æ–‡æœ¬çº¿æ¡çš„ç²—ç»†ã€‚ äººè„¸æ£€æµ‹ è¿™é‡Œæ²¡æœ‰æ‰¾åˆ°ç‹—ç‹—ç…§ç‰‡ï¼Œå¾ˆé—æ†¾ï¼šï¼ˆ å›¾ç‰‡æ¥è‡ªPixabay,ä½œè€…ï¼šFree-Photosã€‚ detectMultiScaleå‡½æ•°æ˜¯ä¸€ç§æ£€æµ‹å¯¹è±¡çš„é€šç”¨å‡½æ•°ã€‚å› ä¸ºæˆ‘ä»¬è°ƒç”¨çš„æ˜¯äººè„¸çº§è”ï¼Œæ‰€ä»¥å®ƒä¼šæ£€æµ‹åˆ°äººè„¸ã€‚ detectMultiScaleå‡½æ•°æ¥å—4ä¸ªå‚æ•°ï¼š ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ç°é˜¶å›¾åƒã€‚ ç¬¬äºŒä¸ªå‚æ•°æ˜¯scaleFactorã€‚å› ä¸ºæœ‰äº›äººè„¸å¯èƒ½ç¦»é•œå¤´æ›´è¿‘ï¼Œæ‰€ä»¥çœ‹èµ·æ¥ä¼šæ¯”åå°çš„äººè„¸æ›´å¤§ã€‚æ¯”ä¾‹ç³»æ•°å¼¥è¡¥äº†è¿™ä¸€ç‚¹ã€‚ æ£€æµ‹ç®—æ³•ä½¿ç”¨ä¸€ä¸ªç§»åŠ¨çª—å£æ¥æ£€æµ‹å¯¹è±¡ã€‚minNeighborså®šä¹‰åœ¨å½“å‰å¯¹è±¡é™„è¿‘æ£€æµ‹åˆ°å¤šå°‘å¯¹è±¡ï¼Œç„¶åå†å£°æ˜æ£€æµ‹åˆ°äººè„¸ã€‚ ä¸æ­¤åŒæ—¶ï¼Œminsizeç»™å‡ºäº†æ¯ä¸ªçª—å£çš„å¤§å°ã€‚ æ£€æµ‹åˆ°ä¸¤å¼ äººè„¸ã€‚ è½®å»“â€”â€”ä¸€ç§å¯¹è±¡æ£€æµ‹æ–¹æ³• ä½¿ç”¨åŸºäºé¢œè‰²çš„å›¾åƒåˆ†å‰²ï¼Œä½ å¯ä»¥æ¥æ£€æµ‹å¯¹è±¡ã€‚ cv2.findContours & cv2.drawContours è¿™ä¸¤ä¸ªå‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ã€‚ æœ€è¿‘ï¼Œæˆ‘å†™äº†ä¸€ç¯‡éå¸¸è¯¦ç»†çš„æ–‡ç« ï¼Œå«åšã€Šä½¿ç”¨Pythoné€šè¿‡åŸºäºé¢œè‰²çš„å›¾åƒåˆ†å‰²æ¥è¿›è¡Œå¯¹è±¡æ£€æµ‹ã€‹ã€‚ä½ éœ€è¦çŸ¥é“çš„å…³äºè½®å»“çš„ä¸€åˆ‡éƒ½åœ¨é‚£é‡Œã€‚ï¼ˆhttps://towardsdatascience.com/object-detection-via-color-based-image-segmentation-using-python-e9b7c72f0e11 ï¼‰ æœ€ç»ˆ,ä¿å­˜å›¾ç‰‡ æ€»ç»“ OpenCVæ˜¯ä¸€ä¸ªéå¸¸å®¹æ˜“ä½¿ç”¨çš„ç®—æ³•åº“ï¼Œå¯ä»¥ç”¨äº3Då»ºæ¨¡ã€é«˜çº§å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ã€è·Ÿè¸ªè§†é¢‘ä¸­çš„æ ‡è¯†å¯¹è±¡ã€å¯¹è§†é¢‘ä¸­æ­£åœ¨åšæŸä¸ªåŠ¨ä½œçš„äººè¿›è¡Œåˆ†ç±»ã€ä»å›¾åƒæ•°æ®é›†ä¸­æ‰¾åˆ°ç›¸ä¼¼çš„å›¾åƒï¼Œç­‰ç­‰ã€‚ æœ€é‡è¦çš„æ˜¯ï¼Œå­¦ä¹ OpenCVå¯¹äºé‚£äº›æƒ³è¦å‚ä¸ä¸å›¾åƒç›¸å…³çš„æœºå™¨å­¦ä¹ é¡¹ç›®çš„äººæ¥è¯´æ˜¯è‡³å…³é‡è¦çš„ã€‚ "},"code_technique/pytorch/pytorch1.html":{"url":"code_technique/pytorch/pytorch1.html","title":"pytorchå¸¸ç”¨ä»£ç ","keywords":"","body":"æœ¬æ–‡ä»£ç åŸºäºPyTorch 1.0ç‰ˆæœ¬ï¼Œéœ€è¦ç”¨åˆ°ä»¥ä¸‹åŒ… import collections import os import shutil import tqdm import numpy as np import PIL.Image import torch import torchvision 1. åŸºç¡€é…ç½® æ£€æŸ¥PyTorchç‰ˆæœ¬ torch.__version__ # PyTorch version torch.version.cuda # Corresponding CUDA version torch.backends.cudnn.version() # Corresponding cuDNN version torch.cuda.get_device_name(0) # GPU type æ›´æ–°PyTorch PyTorchå°†è¢«å®‰è£…åœ¨anaconda3/lib/python3.7/site-packages/torch/ç›®å½•ä¸‹ã€‚ conda update pytorch torchvision -c pytorch å›ºå®šéšæœºç§å­ torch.manual_seed(0) torch.cuda.manual_seed_all(0) æŒ‡å®šç¨‹åºè¿è¡Œåœ¨ç‰¹å®šGPUå¡ä¸Š åœ¨å‘½ä»¤è¡ŒæŒ‡å®šç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES=0,1 python train.py æˆ–åœ¨ä»£ç ä¸­æŒ‡å®š os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' åˆ¤æ–­æ˜¯å¦æœ‰CUDAæ”¯æŒ torch.cuda.is_available() è®¾ç½®ä¸ºcuDNN benchmarkæ¨¡å¼ Benchmarkæ¨¡å¼ä¼šæå‡è®¡ç®—é€Ÿåº¦ï¼Œä½†æ˜¯ç”±äºè®¡ç®—ä¸­æœ‰éšæœºæ€§ï¼Œæ¯æ¬¡ç½‘ç»œå‰é¦ˆç»“æœç•¥æœ‰å·®å¼‚ã€‚ torch.backends.cudnn.benchmark = True å¦‚æœæƒ³è¦é¿å…è¿™ç§ç»“æœæ³¢åŠ¨ï¼Œè®¾ç½® torch.backends.cudnn.deterministic = True æ¸…é™¤GPUå­˜å‚¨ æœ‰æ—¶Control-Cä¸­æ­¢è¿è¡ŒåGPUå­˜å‚¨æ²¡æœ‰åŠæ—¶é‡Šæ”¾ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç©ºã€‚åœ¨PyTorchå†…éƒ¨å¯ä»¥ torch.cuda.empty_cache() æˆ–åœ¨å‘½ä»¤è¡Œå¯ä»¥å…ˆä½¿ç”¨psæ‰¾åˆ°ç¨‹åºçš„PIDï¼Œå†ä½¿ç”¨killç»“æŸè¯¥è¿›ç¨‹ ps aux | grep python kill -9 [pid] æˆ–è€…ç›´æ¥é‡ç½®æ²¡æœ‰è¢«æ¸…ç©ºçš„GPU nvidia-smi --gpu-reset -i [gpu_id] 2. å¼ é‡å¤„ç† å¼ é‡åŸºæœ¬ä¿¡æ¯ tensor.type() # Data type tensor.size() # Shape of the tensor. It is a subclass of Python tuple tensor.dim() # Number of dimensions. æ•°æ®ç±»å‹è½¬æ¢ # Set default tensor type. Float in PyTorch is much faster than double. torch.set_default_tensor_type(torch.FloatTensor) # Type convertions. tensor = tensor.cuda() tensor = tensor.cpu() tensor = tensor.float() tensor = tensor.long() torch.Tensorä¸np.ndarrayè½¬æ¢ # torch.Tensor -> np.ndarray. ndarray = tensor.cpu().numpy() # np.ndarray -> torch.Tensor. tensor = torch.from_numpy(ndarray).float() tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride torch.Tensorä¸PIL.Imageè½¬æ¢ PyTorchä¸­çš„å¼ é‡é»˜è®¤é‡‡ç”¨NÃ—DÃ—HÃ—Wçš„é¡ºåºï¼Œå¹¶ä¸”æ•°æ®èŒƒå›´åœ¨[0, 1]ï¼Œéœ€è¦è¿›è¡Œè½¬ç½®å’Œè§„èŒƒåŒ–ã€‚ # torch.Tensor -> PIL.Image. image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255 ).byte().permute(1, 2, 0).cpu().numpy()) image = torchvision.transforms.functional.to_pil_image(tensor) # Equivalently way # PIL.Image -> torch.Tensor. tensor = torch.from_numpy(np.asarray(PIL.Image.open(path)) ).permute(2, 0, 1).float() / 255 tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way np.ndarrayä¸PIL.Imageè½¬æ¢ # np.ndarray -> PIL.Image. image = PIL.Image.fromarray(ndarray.astypde(np.uint8)) # PIL.Image -> np.ndarray. ndarray = np.asarray(PIL.Image.open(path)) ä»åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„å¼ é‡ä¸­æå–å€¼ è¿™åœ¨è®­ç»ƒæ—¶ç»Ÿè®¡lossçš„å˜åŒ–è¿‡ç¨‹ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚å¦åˆ™è¿™å°†ç´¯ç§¯è®¡ç®—å›¾ï¼Œä½¿GPUå­˜å‚¨å ç”¨é‡è¶Šæ¥è¶Šå¤§ã€‚ value = tensor.item() å¼ é‡å½¢å˜ å¼ é‡å½¢å˜å¸¸å¸¸éœ€è¦ç”¨äºå°†å·ç§¯å±‚ç‰¹å¾è¾“å…¥å…¨è¿æ¥å±‚çš„æƒ…å½¢ã€‚ç›¸æ¯”torch.viewï¼Œtorch.reshapeå¯ä»¥è‡ªåŠ¨å¤„ç†è¾“å…¥å¼ é‡ä¸è¿ç»­çš„æƒ…å†µã€‚ tensor = torch.reshape(tensor, shape) æ‰“ä¹±é¡ºåº tensor = tensor[torch.randperm(tensor.size(0))] # Shuffle the first dimension æ°´å¹³ç¿»è½¬ PyTorchä¸æ”¯æŒtensor[::-1]è¿™æ ·çš„è´Ÿæ­¥é•¿æ“ä½œï¼Œæ°´å¹³ç¿»è½¬å¯ä»¥ç”¨å¼ é‡ç´¢å¼•å®ç°ã€‚ # Assume tensor has shape N*D*H*W. tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()] å¤åˆ¶å¼ é‡ æœ‰ä¸‰ç§å¤åˆ¶çš„æ–¹å¼ï¼Œå¯¹åº”ä¸åŒçš„éœ€æ±‚ã€‚ # Operation | New/Shared memory | Still in computation graph | tensor.clone() # | New | Yes | tensor.detach() # | Shared | No | tensor.detach.clone()() # | New | No | æ‹¼æ¥å¼ é‡ æ³¨æ„torch.catå’Œtorch.stackçš„åŒºåˆ«åœ¨äºtorch.catæ²¿ç€ç»™å®šçš„ç»´åº¦æ‹¼æ¥ï¼Œè€Œtorch.stackä¼šæ–°å¢ä¸€ç»´ã€‚ä¾‹å¦‚å½“å‚æ•°æ˜¯3ä¸ª10Ã—5çš„å¼ é‡ï¼Œtorch.catçš„ç»“æœæ˜¯30Ã—5çš„å¼ é‡ï¼Œè€Œtorch.stackçš„ç»“æœæ˜¯3Ã—10Ã—5çš„å¼ é‡ã€‚ tensor = torch.cat(list_of_tensors, dim=0) tensor = torch.stack(list_of_tensors, dim=0) å°†æ•´æ•°æ ‡è®°è½¬æ¢æˆç‹¬çƒ­ï¼ˆone-hotï¼‰ç¼–ç  PyTorchä¸­çš„æ ‡è®°é»˜è®¤ä»0å¼€å§‹ã€‚ N = tensor.size(0) one_hot = torch.zeros(N, num_classes).long() one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long()) å¾—åˆ°éé›¶/é›¶å…ƒç´  torch.nonzero(tensor) # Index of non-zero elements torch.nonzero(tensor == 0) # Index of zero elements torch.nonzero(tensor).size(0) # Number of non-zero elements torch.nonzero(tensor == 0).size(0) # Number of zero elements åˆ¤æ–­ä¸¤ä¸ªå¼ é‡ç›¸ç­‰ torch.allclose(tensor1, tensor2) # float tensor torch.equal(tensor1, tensor2) # int tensor å¼ é‡æ‰©å±• # Expand tensor of shape 64*512 to shape 64*512*7*7. torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7) çŸ©é˜µä¹˜æ³• # Matrix multiplication: (m*n) * (n*p) -> (m*p). result = torch.mm(tensor1, tensor2) # Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p). result = torch.bmm(tensor1, tensor2) # Element-wise multiplication. result = tensor1 * tensor2 è®¡ç®—ä¸¤ç»„æ•°æ®ä¹‹é—´çš„ä¸¤ä¸¤æ¬§å¼è·ç¦» # X1 is of shape m*d, X2 is of shape n*d. dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2)) 3. æ¨¡å‹å®šä¹‰ å·ç§¯å±‚ æœ€å¸¸ç”¨çš„å·ç§¯å±‚é…ç½®æ˜¯ conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True) conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True) å¦‚æœå·ç§¯å±‚é…ç½®æ¯”è¾ƒå¤æ‚ï¼Œä¸æ–¹ä¾¿è®¡ç®—è¾“å‡ºå¤§å°æ—¶ï¼Œå¯ä»¥åˆ©ç”¨å¦‚ä¸‹å¯è§†åŒ–å·¥å…·è¾…åŠ© Convolution Visualizerezyang.github.io GAPï¼ˆGlobal average poolingï¼‰å±‚ gap = torch.nn.AdaptiveAvgPool2d(output_size=1) åŒçº¿æ€§æ±‡åˆï¼ˆbilinear poolingï¼‰[1] X = torch.reshape(N, D, H * W) # Assume X has shape N*D*H*W X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W) # Bilinear pooling assert X.size() == (N, D, D) X = torch.reshape(X, (N, D * D)) X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5) # Signed-sqrt normalization X = torch.nn.functional.normalize(X) # L2 normalization å¤šå¡åŒæ­¥BNï¼ˆBatch normalizationï¼‰ å½“ä½¿ç”¨torch.nn.DataParallelå°†ä»£ç è¿è¡Œåœ¨å¤šå¼ GPUå¡ä¸Šæ—¶ï¼ŒPyTorchçš„BNå±‚é»˜è®¤æ“ä½œæ˜¯å„å¡ä¸Šæ•°æ®ç‹¬ç«‹åœ°è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ŒåŒæ­¥BNä½¿ç”¨æ‰€æœ‰å¡ä¸Šçš„æ•°æ®ä¸€èµ·è®¡ç®—BNå±‚çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç¼“è§£äº†å½“æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰æ¯”è¾ƒå°æ—¶å¯¹å‡å€¼å’Œæ ‡å‡†å·®ä¼°è®¡ä¸å‡†çš„æƒ…å†µï¼Œæ˜¯åœ¨ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ä¸€ä¸ªæœ‰æ•ˆçš„æå‡æ€§èƒ½çš„æŠ€å·§ã€‚ vacancy/Synchronized-BatchNorm-PyTorchgithub.com ç°åœ¨PyTorchå®˜æ–¹å·²ç»æ”¯æŒåŒæ­¥BNæ“ä½œ sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) å°†å·²æœ‰ç½‘ç»œçš„æ‰€æœ‰BNå±‚æ”¹ä¸ºåŒæ­¥BNå±‚ def convertBNtoSyncBN(module, process_group=None): '''Recursively replace all BN layers to SyncBN layer. Args: module[torch.nn.Module]. Network ''' if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group) sync_bn.running_mean = module.running_mean sync_bn.running_var = module.running_var if module.affine: sync_bn.weight = module.weight.clone().detach() sync_bn.bias = module.bias.clone().detach() return sync_bn else: for name, child_module in module.named_children(): setattr(module, name) = convert_syncbn_model(child_module, process_group=process_group)) return module ç±»ä¼¼BNæ»‘åŠ¨å¹³å‡ å¦‚æœè¦å®ç°ç±»ä¼¼BNæ»‘åŠ¨å¹³å‡çš„æ“ä½œï¼Œåœ¨forwardå‡½æ•°ä¸­è¦ä½¿ç”¨åŸåœ°ï¼ˆinplaceï¼‰æ“ä½œç»™æ»‘åŠ¨å¹³å‡èµ‹å€¼ã€‚ class BN(torch.nn.Module) def __init__(self): ... self.register_buffer('running_mean', torch.zeros(num_features)) def forward(self, X): ... self.running_mean += momentum * (current - self.running_mean) è®¡ç®—æ¨¡å‹æ•´ä½“å‚æ•°é‡ num_parameters = sum(torch.numel(parameter) for parameter in model.parameters()) ç±»ä¼¼Kerasçš„model.summary()è¾“å‡ºæ¨¡å‹ä¿¡æ¯ sksq96/pytorch-summarygithub.com æ¨¡å‹æƒå€¼åˆå§‹åŒ– æ³¨æ„model.modules()å’Œmodel.children()çš„åŒºåˆ«ï¼šmodel.modules()ä¼šè¿­ä»£åœ°éå†æ¨¡å‹çš„æ‰€æœ‰å­å±‚ï¼Œè€Œmodel.children()åªä¼šéå†æ¨¡å‹ä¸‹çš„ä¸€å±‚ã€‚ # Common practise for initialization. for layer in model.modules(): if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.BatchNorm2d): torch.nn.init.constant_(layer.weight, val=1.0) torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.Linear): torch.nn.init.xavier_normal_(layer.weight) if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) # Initialization with given tensor. layer.weight = torch.nn.Parameter(tensor) éƒ¨åˆ†å±‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ æ³¨æ„å¦‚æœä¿å­˜çš„æ¨¡å‹æ˜¯torch.nn.DataParallelï¼Œåˆ™å½“å‰çš„æ¨¡å‹ä¹Ÿéœ€è¦æ˜¯torch.nn.DataParallelã€‚torch.nn.DataParallel(model).module == modelã€‚ model.load_state_dict(torch.load('model,pth'), strict=False) å°†åœ¨GPUä¿å­˜çš„æ¨¡å‹åŠ è½½åˆ°CPU model.load_state_dict(torch.load('model,pth', map_location='cpu')) 4. æ•°æ®å‡†å¤‡ã€ç‰¹å¾æå–ä¸å¾®è°ƒ å›¾åƒåˆ†å—æ‰“æ•£ï¼ˆimage shuffleï¼‰/åŒºåŸŸæ··æ·†æœºåˆ¶ï¼ˆregion confusion mechanismï¼ŒRCMï¼‰[2] # X is torch.Tensor of size N*D*H*W. # Shuffle rows Q = (torch.unsqueeze(torch.arange(num_blocks), dim=1) * torch.ones(1, num_blocks).long() + torch.randint(low=-neighbour, high=neighbour, size=(num_blocks, num_blocks))) Q = torch.argsort(Q, dim=0) assert Q.size() == (num_blocks, num_blocks) X = [torch.chunk(row, chunks=num_blocks, dim=2) for row in torch.chunk(X, chunks=num_blocks, dim=1)] X = [[X[Q[i, j].item()][j] for j in range(num_blocks)] for i in range(num_blocks)] # Shulle columns. Q = (torch.ones(num_blocks, 1).long() * torch.unsqueeze(torch.arange(num_blocks), dim=0) + torch.randint(low=-neighbour, high=neighbour, size=(num_blocks, num_blocks))) Q = torch.argsort(Q, dim=1) assert Q.size() == (num_blocks, num_blocks) X = [[X[i][Q[i, j].item()] for j in range(num_blocks)] for i in range(num_blocks)] Y = torch.cat([torch.cat(row, dim=2) for row in X], dim=1) å¾—åˆ°è§†é¢‘æ•°æ®åŸºæœ¬ä¿¡æ¯ import cv2 video = cv2.VideoCapture(mp4_path) height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)) num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) fps = int(video.get(cv2.CAP_PROP_FPS)) video.release() TSNæ¯æ®µï¼ˆsegmentï¼‰é‡‡æ ·ä¸€å¸§è§†é¢‘[3] K = self._num_segments if is_train: if num_frames > K: # Random index for each segment. frame_indices = torch.randint( high=num_frames // K, size=(K,), dtype=torch.long) frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.randint( high=num_frames, size=(K - num_frames,), dtype=torch.long) frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), frame_indices)))[0] else: if num_frames > K: # Middle index for each segment. frame_indices = num_frames / K // 2 frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), torch.arange(K - num_frames))))[0] assert frame_indices.size() == (K,) return [frame_indices[i] for i in range(K)] æå–ImageNeté¢„è®­ç»ƒæ¨¡å‹æŸå±‚çš„å·ç§¯ç‰¹å¾ # VGG-16 relu5-3 feature. model = torchvision.models.vgg16(pretrained=True).features[:-1] # VGG-16 pool5 feature. model = torchvision.models.vgg16(pretrained=True).features # VGG-16 fc7 feature. model = torchvision.models.vgg16(pretrained=True) model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3]) # ResNet GAP feature. model = torchvision.models.resnet18(pretrained=True) model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) with torch.no_grad(): model.eval() conv_representation = model(image) æå–ImageNeté¢„è®­ç»ƒæ¨¡å‹å¤šå±‚çš„å·ç§¯ç‰¹å¾ class FeatureExtractor(torch.nn.Module): \"\"\"Helper class to extract several convolution features from the given pre-trained model. Attributes: _model, torch.nn.Module. _layers_to_extract, list or set Example: >>> model = torchvision.models.resnet152(pretrained=True) >>> model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) >>> conv_representation = FeatureExtractor( pretrained_model=model, layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image) \"\"\" def __init__(self, pretrained_model, layers_to_extract): torch.nn.Module.__init__(self) self._model = pretrained_model self._model.eval() self._layers_to_extract = set(layers_to_extract) def forward(self, x): with torch.no_grad(): conv_representation = [] for name, layer in self._model.named_children(): x = layer(x) if name in self._layers_to_extract: conv_representation.append(x) return conv_representation å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ Cadene/pretrained-models.pytorchgithub.com å¾®è°ƒå…¨è¿æ¥å±‚ model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False model.fc = nn.Linear(512, 100) # Replace the last fc layer optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4) ä»¥è¾ƒå¤§å­¦ä¹ ç‡å¾®è°ƒå…¨è¿æ¥å±‚ï¼Œè¾ƒå°å­¦ä¹ ç‡å¾®è°ƒå·ç§¯å±‚ model = torchvision.models.resnet18(pretrained=True) finetuned_parameters = list(map(id, model.fc.parameters())) conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters) parameters = [{'params': conv_parameters, 'lr': 1e-3}, {'params': model.fc.parameters()}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) 5. æ¨¡å‹è®­ç»ƒ å¸¸ç”¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é¢„å¤„ç† å…¶ä¸­ToTensoræ“ä½œä¼šå°†PIL.Imageæˆ–å½¢çŠ¶ä¸ºHÃ—WÃ—Dï¼Œæ•°å€¼èŒƒå›´ä¸º[0, 255]çš„np.ndarrayè½¬æ¢ä¸ºå½¢çŠ¶ä¸ºDÃ—HÃ—Wï¼Œæ•°å€¼èŒƒå›´ä¸º[0.0, 1.0]çš„torch.Tensorã€‚ train_transform = torchvision.transforms.Compose([ torchvision.transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0)), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) val_transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(256), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) è®­ç»ƒåŸºæœ¬ä»£ç æ¡†æ¶ for t in epoch(80): for images, labels in tqdm.tqdm(train_loader, desc='Epoch %3d' % (t + 1)): images, labels = images.cuda(), labels.cuda() scores = model(images) loss = loss_function(scores, labels) optimizer.zero_grad() loss.backward() optimizer.step() æ ‡è®°å¹³æ»‘ï¼ˆlabel smoothingï¼‰[4] for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() N = labels.size(0) # C is the number of classes. smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda() smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9) score = model(images) log_prob = torch.nn.functional.log_softmax(score, dim=1) loss = -torch.sum(log_prob * smoothed_labels) / N optimizer.zero_grad() loss.backward() optimizer.step() Mixup[5] beta_distribution = torch.distributions.beta.Beta(alpha, alpha) for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() # Mixup images. lambda_ = beta_distribution.sample([]).item() index = torch.randperm(images.size(0)).cuda() mixed_images = lambda_ * images + (1 - lambda_) * images[index, :] # Mixup loss. scores = model(mixed_images) loss = (lambda_ * loss_function(scores, labels) + (1 - lambda_) * loss_function(scores, labels[index])) optimizer.zero_grad() loss.backward() optimizer.step() L1æ­£åˆ™åŒ– l1_regularization = torch.nn.L1Loss(reduction='sum') loss = ... # Standard cross-entropy loss for param in model.parameters(): loss += lambda_ * torch.sum(torch.abs(param)) loss.backward() ä¸å¯¹åç½®é¡¹è¿›è¡ŒL2æ­£åˆ™åŒ–/æƒå€¼è¡°å‡ï¼ˆweight decayï¼‰ bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias') others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias') parameters = [{'parameters': bias_list, 'weight_decay': 0}, {'parameters': others_list}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) æ¢¯åº¦è£å‰ªï¼ˆgradient clippingï¼‰ torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20) è®¡ç®—Softmaxè¾“å‡ºçš„å‡†ç¡®ç‡ score = model(images) prediction = torch.argmax(score, dim=1) num_correct = torch.sum(prediction == labels).item() accuruacy = num_correct / labels.size(0) å¯è§†åŒ–æ¨¡å‹å‰é¦ˆçš„è®¡ç®—å›¾ szagoruyko/pytorchvizgithub.com å¯è§†åŒ–å­¦ä¹ æ›²çº¿ æœ‰Facebookè‡ªå·±å¼€å‘çš„Visdomå’ŒTensorboardï¼ˆä»å¤„äºå®éªŒé˜¶æ®µï¼‰ä¸¤ä¸ªé€‰æ‹©ã€‚ facebookresearch/visdomgithub.com torch.utils.tensorboard - PyTorch master documentationpytorch.org # Example using Visdom. vis = visdom.Visdom(env='Learning curve', use_incoming_socket=False) assert self._visdom.check_connection() self._visdom.close() options = collections.namedtuple('Options', ['loss', 'acc', 'lr'])( loss={'xlabel': 'Epoch', 'ylabel': 'Loss', 'showlegend': True}, acc={'xlabel': 'Epoch', 'ylabel': 'Accuracy', 'showlegend': True}, lr={'xlabel': 'Epoch', 'ylabel': 'Learning rate', 'showlegend': True}) for t in epoch(80): tran(...) val(...) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]), name='train', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]), name='val', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]), name='train', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]), name='val', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]), win='Learning rate', update='append', opts=options.lr) å¾—åˆ°å½“å‰å­¦ä¹ ç‡ # If there is one global learning rate (which is the common case). lr = next(iter(optimizer.param_groups))['lr'] # If there are multiple learning rates for different layers. all_lr = [] for param_group in optimizer.param_groups: all_lr.append(param_group['lr']) å­¦ä¹ ç‡è¡°å‡ # Reduce learning rate when validation accuarcy plateau. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True) for t in range(0, 80): train(...); val(...) scheduler.step(val_acc) # Cosine annealing learning rate. scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80) # Reduce learning rate by 10 at given epochs. scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1) for t in range(0, 80): scheduler.step() train(...); val(...) # Learning rate warmup by 10 epochs. scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10) for t in range(0, 10): scheduler.step() train(...); val(...) ä¿å­˜ä¸åŠ è½½æ–­ç‚¹ æ³¨æ„ä¸ºäº†èƒ½å¤Ÿæ¢å¤è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼Œä»¥åŠå½“å‰çš„è®­ç»ƒè½®æ•°ã€‚ # Save checkpoint. is_best = current_acc > best_acc best_acc = max(best_acc, current_acc) checkpoint = { 'best_acc': best_acc, 'epoch': t + 1, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), } model_path = os.path.join('model', 'checkpoint.pth.tar') torch.save(checkpoint, model_path) if is_best: shutil.copy('checkpoint.pth.tar', model_path) # Load checkpoint. if resume: model_path = os.path.join('model', 'checkpoint.pth.tar') assert os.path.isfile(model_path) checkpoint = torch.load(model_path) best_acc = checkpoint['best_acc'] start_epoch = checkpoint['epoch'] model.load_state_dict(checkpoint['model']) optimizer.load_state_dict(checkpoint['optimizer']) print('Load checkpoint at epoch %d.' % start_epoch) è®¡ç®—å‡†ç¡®ç‡ã€æŸ¥å‡†ç‡ï¼ˆprecisionï¼‰ã€æŸ¥å…¨ç‡ï¼ˆrecallï¼‰ # data['label'] and data['prediction'] are groundtruth label and prediction # for each image, respectively. accuracy = np.mean(data['label'] == data['prediction']) * 100 # Compute recision and recall for each class. for c in range(len(num_classes)): tp = np.dot((data['label'] == c).astype(int), (data['prediction'] == c).astype(int)) tp_fp = np.sum(data['prediction'] == c) tp_fn = np.sum(data['label'] == c) precision = tp / tp_fp * 100 recall = tp / tp_fn * 100 6. æ¨¡å‹æµ‹è¯• è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŸ¥å‡†ç‡ï¼ˆprecisionï¼‰ã€æŸ¥å…¨ç‡ï¼ˆrecallï¼‰ã€F1å’Œæ€»ä½“æŒ‡æ ‡ import sklearn.metrics all_label = [] all_prediction = [] for images, labels in tqdm.tqdm(data_loader): # Data. images, labels = images.cuda(), labels.cuda() # Forward pass. score = model(images) # Save label and predictions. prediction = torch.argmax(score, dim=1) all_label.append(labels.cpu().numpy()) all_prediction.append(prediction.cpu().numpy()) # Compute RP and confusion matrix. all_label = np.concatenate(all_label) assert len(all_label.shape) == 1 all_prediction = np.concatenate(all_prediction) assert all_label.shape == all_prediction.shape micro_p, micro_r, micro_f1, _ = sklearn.metrics.precision_recall_fscore_support( all_label, all_prediction, average='micro', labels=range(num_classes)) class_p, class_r, class_f1, class_occurence = sklearn.metrics.precision_recall_fscore_support( all_label, all_prediction, average=None, labels=range(num_classes)) # Ci,j = #{y=i and hat_y=j} confusion_mat = sklearn.metrics.confusion_matrix( all_label, all_prediction, labels=range(num_classes)) assert confusion_mat.shape == (num_classes, num_classes) å°†å„ç±»ç»“æœå†™å…¥ç”µå­è¡¨æ ¼ import csv # Write results onto disk. with open(os.path.join(path, filename), 'wt', encoding='utf-8') as f: f = csv.writer(f) f.writerow(['Class', 'Label', '# occurence', 'Precision', 'Recall', 'F1', 'Confused class 1', 'Confused class 2', 'Confused class 3', 'Confused 4', 'Confused class 5']) for c in range(num_classes): index = np.argsort(confusion_mat[:, c])[::-1][:5] f.writerow([ label2class[c], c, class_occurence[c], '%4.3f' % class_p[c], '%4.3f' % class_r[c], '%4.3f' % class_f1[c], '%s:%d' % (label2class[index[0]], confusion_mat[index[0], c]), '%s:%d' % (label2class[index[1]], confusion_mat[index[1], c]), '%s:%d' % (label2class[index[2]], confusion_mat[index[2], c]), '%s:%d' % (label2class[index[3]], confusion_mat[index[3], c]), '%s:%d' % (label2class[index[4]], confusion_mat[index[4], c])]) f.writerow(['All', '', np.sum(class_occurence), micro_p, micro_r, micro_f1, '', '', '', '', '']) 7. PyTorchå…¶ä»–æ³¨æ„äº‹é¡¹ æ¨¡å‹å®šä¹‰ å»ºè®®æœ‰å‚æ•°çš„å±‚å’Œæ±‡åˆï¼ˆpoolingï¼‰å±‚ä½¿ç”¨torch.nnæ¨¡å—å®šä¹‰ï¼Œæ¿€æ´»å‡½æ•°ç›´æ¥ä½¿ç”¨torch.nn.functionalã€‚torch.nnæ¨¡å—å’Œtorch.nn.functionalçš„åŒºåˆ«åœ¨äºï¼Œtorch.nnæ¨¡å—åœ¨è®¡ç®—æ—¶åº•å±‚è°ƒç”¨äº†torch.nn.functionalï¼Œä½†torch.nnæ¨¡å—åŒ…æ‹¬è¯¥å±‚å‚æ•°ï¼Œè¿˜å¯ä»¥åº”å¯¹è®­ç»ƒå’Œæµ‹è¯•ä¸¤ç§ç½‘ç»œçŠ¶æ€ã€‚ä½¿ç”¨torch.nn.functionalæ—¶è¦æ³¨æ„ç½‘ç»œçŠ¶æ€ï¼Œå¦‚ def forward(self, x): ... x = torch.nn.functional.dropout(x, p=0.5, training=self.training) model(x)å‰ç”¨model.train()å’Œmodel.eval()åˆ‡æ¢ç½‘ç»œçŠ¶æ€ã€‚ ä¸éœ€è¦è®¡ç®—æ¢¯åº¦çš„ä»£ç å—ç”¨with torch.nograd()åŒ…å«èµ·æ¥ã€‚model.eval()å’Œtorch.no**grad()çš„åŒºåˆ«åœ¨äºï¼Œmodel.eval()æ˜¯å°†ç½‘ç»œåˆ‡æ¢ä¸ºæµ‹è¯•çŠ¶æ€ï¼Œä¾‹å¦‚BNå’Œéšæœºå¤±æ´»ï¼ˆdropoutï¼‰åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä½¿ç”¨ä¸åŒçš„è®¡ç®—æ–¹æ³•ã€‚torch.no_grad()æ˜¯å…³é—­PyTorchå¼ é‡çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ï¼Œä»¥å‡å°‘å­˜å‚¨ä½¿ç”¨å’ŒåŠ é€Ÿè®¡ç®—ï¼Œå¾—åˆ°çš„ç»“æœæ— æ³•è¿›è¡Œloss.backward()ã€‚ torch.nn.CrossEntropyLossçš„è¾“å…¥ä¸éœ€è¦ç»è¿‡Softmaxã€‚torch.nn.CrossEntropyLossç­‰ä»·äºtorch.nn.functional.log_softmax + torch.nn.NLLLossã€‚ loss.backward()å‰ç”¨optimizer.zero_grad()æ¸…é™¤ç´¯ç§¯æ¢¯åº¦ã€‚optimizer.zero_grad()å’Œmodel.zero_grad()æ•ˆæœä¸€æ ·ã€‚ PyTorchæ€§èƒ½ä¸è°ƒè¯• torch.utils.data.DataLoaderä¸­å°½é‡è®¾ç½®pin_memory=Trueï¼Œå¯¹ç‰¹åˆ«å°çš„æ•°æ®é›†å¦‚MNISTè®¾ç½®pin_memory=Falseåè€Œæ›´å¿«ä¸€äº›ã€‚num_workersçš„è®¾ç½®éœ€è¦åœ¨å®éªŒä¸­æ‰¾åˆ°æœ€å¿«çš„å–å€¼ã€‚ ç”¨delåŠæ—¶åˆ é™¤ä¸ç”¨çš„ä¸­é—´å˜é‡ï¼ŒèŠ‚çº¦GPUå­˜å‚¨ã€‚ ä½¿ç”¨inplaceæ“ä½œå¯èŠ‚çº¦GPUå­˜å‚¨ï¼Œå¦‚ x = torch.nn.functional.relu(x, inplace=True) æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡torch.utils.checkpointå‰å‘ä¼ æ’­æ—¶åªä¿ç•™ä¸€éƒ¨åˆ†ä¸­é—´ç»“æœæ¥èŠ‚çº¦GPUå­˜å‚¨ä½¿ç”¨ï¼Œåœ¨åå‘ä¼ æ’­æ—¶éœ€è¦çš„å†…å®¹ä»æœ€è¿‘ä¸­é—´ç»“æœä¸­è®¡ç®—å¾—åˆ°ã€‚ å‡å°‘CPUå’ŒGPUä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚ä¾‹å¦‚å¦‚æœä½ æƒ³çŸ¥é“ä¸€ä¸ªepochä¸­æ¯ä¸ªmini-batchçš„losså’Œå‡†ç¡®ç‡ï¼Œå…ˆå°†å®ƒä»¬ç´¯ç§¯åœ¨GPUä¸­ç­‰ä¸€ä¸ªepochç»“æŸä¹‹åä¸€èµ·ä¼ è¾“å›CPUä¼šæ¯”æ¯ä¸ªmini-batchéƒ½è¿›è¡Œä¸€æ¬¡GPUåˆ°CPUçš„ä¼ è¾“æ›´å¿«ã€‚ ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°half()ä¼šæœ‰ä¸€å®šçš„é€Ÿåº¦æå‡ï¼Œå…·ä½“æ•ˆç‡ä¾èµ–äºGPUå‹å·ã€‚éœ€è¦å°å¿ƒæ•°å€¼ç²¾åº¦è¿‡ä½å¸¦æ¥çš„ç¨³å®šæ€§é—®é¢˜ã€‚ æ—¶å¸¸ä½¿ç”¨assert tensor.size() == (N, D, H, W)ä½œä¸ºè°ƒè¯•æ‰‹æ®µï¼Œç¡®ä¿å¼ é‡ç»´åº¦å’Œä½ è®¾æƒ³ä¸­ä¸€è‡´ã€‚ é™¤äº†æ ‡è®°yå¤–ï¼Œå°½é‡å°‘ä½¿ç”¨ä¸€ç»´å¼ é‡ï¼Œä½¿ç”¨n*1çš„äºŒç»´å¼ é‡ä»£æ›¿ï¼Œå¯ä»¥é¿å…ä¸€äº›æ„æƒ³ä¸åˆ°çš„ä¸€ç»´å¼ é‡è®¡ç®—ç»“æœã€‚ ç»Ÿè®¡ä»£ç å„éƒ¨åˆ†è€—æ—¶ with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile: ... print(profile) æˆ–è€…åœ¨å‘½ä»¤è¡Œè¿è¡Œ python -m torch.utils.bottleneck main.py è‡´è°¢ æ„Ÿè°¢ äº›è®¸æµå¹´ ã€ El tnoto ã€ FlyCharles çš„å‹˜è¯¯ï¼Œæ„Ÿè°¢ oatmeal æä¾›çš„æ›´ç®€æ´çš„æ–¹æ³•ã€‚ç”±äºä½œè€…æ‰ç–å­¦æµ…ï¼Œæ›´å…¼æ—¶é—´å’Œç²¾åŠ›æ‰€é™ï¼Œä»£ç ä¸­é”™è¯¯ä¹‹å¤„åœ¨æ‰€éš¾å…ï¼Œæ•¬è¯·è¯»è€…æ‰¹è¯„æŒ‡æ­£ã€‚ å‚è€ƒèµ„æ–™ PyTorchå®˜æ–¹ä»£ç ï¼špytorch/examples PyTorchè®ºå›ï¼šPyTorch Forums PyTorchæ–‡æ¡£ï¼šhttp://pytorch.org/docs/stable/index.html å…¶ä»–åŸºäºPyTorchçš„å…¬å¼€å®ç°ä»£ç ï¼Œæ— æ³•ä¸€ä¸€åˆ—ä¸¾ å‚è€ƒ ^T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models for fine-grained visual recognition. In ICCV, 2015. ^Y. Chen, Y. Bai, W. Zhang, and T. Mei. Destruction and construction learning for fine-grained image recognition. In CVPR, 2019. ^L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016. ^C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna: Rethinking the Inception architecture for computer vision. In CVPR, 2016. ^H. Zhang, M. CissÃ©, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. "},"code_technique/pytorch/pytorch2.html":{"url":"code_technique/pytorch/pytorch2.html","title":"pytorchå¸¸ç”¨ä»£ç æ®µåˆé›†","keywords":"","body":"PyTorchå¸¸ç”¨ä»£ç æ®µæ•´ç†åˆé›† HudsonEvangeline å‘å¸ƒäº2å°æ—¶å‰ é˜…è¯»61æ¬¡ 5 äººç‚¹èµ 0 æ¡è¯„è®º æœ¬æ–‡ä»£ç åŸºäº PyTorch 1.0 ç‰ˆæœ¬ï¼Œéœ€è¦ç”¨åˆ°ä»¥ä¸‹åŒ… import collections import os import shutil import tqdm import numpy as np import PIL.Image import torch import torchvision åŸºç¡€é…ç½® æ£€æŸ¥ PyTorch ç‰ˆæœ¬ torch.__version__ # PyTorch version torch.version.cuda # Corresponding CUDA version torch.backends.cudnn.version() # Corresponding cuDNN version torch.cuda.get_device_name(0) # GPU type æ›´æ–° PyTorch PyTorch å°†è¢«å®‰è£…åœ¨ anaconda3/lib/python3.7/site-packages/torch/ç›®å½•ä¸‹ã€‚ conda update pytorch torchvision -c pytorch å›ºå®šéšæœºç§å­ torch.manual_seed(0) torch.cuda.manual_seed_all(0) æŒ‡å®šç¨‹åºè¿è¡Œåœ¨ç‰¹å®š GPU å¡ä¸Š åœ¨å‘½ä»¤è¡ŒæŒ‡å®šç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES=0,1 python train.py æˆ–åœ¨ä»£ç ä¸­æŒ‡å®š os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' åˆ¤æ–­æ˜¯å¦æœ‰ CUDA æ”¯æŒ torch.cuda.is_available() è®¾ç½®ä¸º cuDNN benchmark æ¨¡å¼ Benchmark æ¨¡å¼ä¼šæå‡è®¡ç®—é€Ÿåº¦ï¼Œä½†æ˜¯ç”±äºè®¡ç®—ä¸­æœ‰éšæœºæ€§ï¼Œæ¯æ¬¡ç½‘ç»œå‰é¦ˆç»“æœç•¥æœ‰å·®å¼‚ã€‚ torch.backends.cudnn.benchmark = True å¦‚æœæƒ³è¦é¿å…è¿™ç§ç»“æœæ³¢åŠ¨ï¼Œè®¾ç½® torch.backends.cudnn.deterministic = True æ¸…é™¤ GPU å­˜å‚¨ æœ‰æ—¶ Control-C ä¸­æ­¢è¿è¡Œå GPU å­˜å‚¨æ²¡æœ‰åŠæ—¶é‡Šæ”¾ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç©ºã€‚åœ¨ PyTorch å†…éƒ¨å¯ä»¥ torch.cuda.empty_cache() æˆ–åœ¨å‘½ä»¤è¡Œå¯ä»¥å…ˆä½¿ç”¨ ps æ‰¾åˆ°ç¨‹åºçš„ PIDï¼Œå†ä½¿ç”¨ kill ç»“æŸè¯¥è¿›ç¨‹ ps aux | grep pythonkill -9 [pid] æˆ–è€…ç›´æ¥é‡ç½®æ²¡æœ‰è¢«æ¸…ç©ºçš„ GPU nvidia-smi --gpu-reset -i [gpu_id] å¼ é‡å¤„ç† å¼ é‡åŸºæœ¬ä¿¡æ¯ tensor.type() # Data type tensor.size() # Shape of the tensor. It is a subclass of Python tuple tensor.dim() # Number of dimensions. æ•°æ®ç±»å‹è½¬æ¢ # Set default tensor type. Float in PyTorch is much faster than double. torch.set_default_tensor_type(torch.FloatTensor) # Type convertions. tensor = tensor.cuda() tensor = tensor.cpu() tensor = tensor.float() tensor = tensor.long() torch.Tensor ä¸ np.ndarray è½¬æ¢ # torch.Tensor -> np.ndarray. ndarray = tensor.cpu().numpy() # np.ndarray -> torch.Tensor. tensor = torch.from_numpy(ndarray).float() tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride torch.Tensor ä¸ PIL.Image è½¬æ¢ PyTorch ä¸­çš„å¼ é‡é»˜è®¤é‡‡ç”¨ NÃ—DÃ—HÃ—W çš„é¡ºåºï¼Œå¹¶ä¸”æ•°æ®èŒƒå›´åœ¨ [0, 1]ï¼Œéœ€è¦è¿›è¡Œè½¬ç½®å’Œè§„èŒƒåŒ–ã€‚ # torch.Tensor -> PIL.Image. image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255 ).byte().permute(1, 2, 0).cpu().numpy()) image = torchvision.transforms.functional.to_pil_image(tensor) # Equivalently way # PIL.Image -> torch.Tensor. tensor = torch.from_numpy(np.asarray(PIL.Image.open(path)) ).permute(2, 0, 1).float() / 255 tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way np.ndarray ä¸ PIL.Image è½¬æ¢ # np.ndarray -> PIL.Image. image = PIL.Image.fromarray(ndarray.astypde(np.uint8)) # PIL.Image -> np.ndarray. ndarray = np.asarray(PIL.Image.open(path)) ä»åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„å¼ é‡ä¸­æå–å€¼ è¿™åœ¨è®­ç»ƒæ—¶ç»Ÿè®¡ loss çš„å˜åŒ–è¿‡ç¨‹ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚å¦åˆ™è¿™å°†ç´¯ç§¯è®¡ç®—å›¾ï¼Œä½¿ GPU å­˜å‚¨å ç”¨é‡è¶Šæ¥è¶Šå¤§ã€‚ value = tensor.item() å¼ é‡å½¢å˜ å¼ é‡å½¢å˜å¸¸å¸¸éœ€è¦ç”¨äºå°†å·ç§¯å±‚ç‰¹å¾è¾“å…¥å…¨è¿æ¥å±‚çš„æƒ…å½¢ã€‚ç›¸æ¯” torch.viewï¼Œtorch.reshape å¯ä»¥è‡ªåŠ¨å¤„ç†è¾“å…¥å¼ é‡ä¸è¿ç»­çš„æƒ…å†µã€‚ tensor = torch.reshape(tensor, shape) æ‰“ä¹±é¡ºåº tensor = tensor[torch.randperm(tensor.size(0))] # Shuffle the first dimension æ°´å¹³ç¿»è½¬ PyTorch ä¸æ”¯æŒ tensor[::-1] è¿™æ ·çš„è´Ÿæ­¥é•¿æ“ä½œï¼Œæ°´å¹³ç¿»è½¬å¯ä»¥ç”¨å¼ é‡ç´¢å¼•å®ç°ã€‚ # Assume tensor has shape N*D*H*W.tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()] å¤åˆ¶å¼ é‡ æœ‰ä¸‰ç§å¤åˆ¶çš„æ–¹å¼ï¼Œå¯¹åº”ä¸åŒçš„éœ€æ±‚ã€‚ # Operation | New/Shared memory | Still in computation graph | tensor.clone() # | New | Yes | tensor.detach() # | Shared | No | tensor.detach.clone()() # | New | No | æ‹¼æ¥å¼ é‡ æ³¨æ„ torch.cat å’Œ torch.stack çš„åŒºåˆ«åœ¨äº torch.cat æ²¿ç€ç»™å®šçš„ç»´åº¦æ‹¼æ¥ï¼Œè€Œ torch.stack ä¼šæ–°å¢ä¸€ç»´ã€‚ä¾‹å¦‚å½“å‚æ•°æ˜¯ 3 ä¸ª 10Ã—5 çš„å¼ é‡ï¼Œtorch.cat çš„ç»“æœæ˜¯ 30Ã—5 çš„å¼ é‡ï¼Œè€Œ torch.stack çš„ç»“æœæ˜¯ 3Ã—10Ã—5 çš„å¼ é‡ã€‚ tensor = torch.cat(list_of_tensors, dim=0) tensor = torch.stack(list_of_tensors, dim=0) å°†æ•´æ•°æ ‡è®°è½¬æ¢æˆç‹¬çƒ­ï¼ˆone-hotï¼‰ç¼–ç  PyTorch ä¸­çš„æ ‡è®°é»˜è®¤ä» 0 å¼€å§‹ã€‚ N = tensor.size(0) one_hot = torch.zeros(N, num_classes).long() one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long()) å¾—åˆ°éé›¶/é›¶å…ƒç´  torch.nonzero(tensor) # Index of non-zero elements torch.nonzero(tensor == 0) # Index of zero elements torch.nonzero(tensor).size(0) # Number of non-zero elements torch.nonzero(tensor == 0).size(0) # Number of zero elements å¼ é‡æ‰©å±• # Expand tensor of shape 64*512 to shape 64*512*7*7. torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7) çŸ©é˜µä¹˜æ³• # Matrix multiplication: (m*n) * (n*p) -> (m*p). result = torch.mm(tensor1, tensor2) # Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p). result = torch.bmm(tensor1, tensor2) # Element-wise multiplication. result = tensor1 * tensor2 è®¡ç®—ä¸¤ç»„æ•°æ®ä¹‹é—´çš„ä¸¤ä¸¤æ¬§å¼è·ç¦» # X1 is of shape m*d. X1 = torch.unsqueeze(X1, dim=1).expand(m, n, d) # X2 is of shape n*d. X2 = torch.unsqueeze(X2, dim=0).expand(m, n, d) # dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2) dist = torch.sqrt(torch.sum((X1 - X2) ** 2, dim=2)) æ¨¡å‹å®šä¹‰ å·ç§¯å±‚ æœ€å¸¸ç”¨çš„å·ç§¯å±‚é…ç½®æ˜¯ conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True) å¦‚æœå·ç§¯å±‚é…ç½®æ¯”è¾ƒå¤æ‚ï¼Œä¸æ–¹ä¾¿è®¡ç®—è¾“å‡ºå¤§å°æ—¶ï¼Œå¯ä»¥åˆ©ç”¨å¦‚ä¸‹å¯è§†åŒ–å·¥å…·è¾…åŠ© é“¾æ¥ï¼šhttps://ezyang.github.io/convolution-visualizer/index.html 0GAPï¼ˆGlobal average poolingï¼‰å±‚ gap = torch.nn.AdaptiveAvgPool2d(output_size=1) åŒçº¿æ€§æ±‡åˆï¼ˆbilinear poolingï¼‰ X = torch.reshape(N, D, H * W) # Assume X has shape N*D*H*W X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W) # Bilinear pooling assert X.size() == (N, D, D) X = torch.reshape(X, (N, D * D)) X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5) # Signed-sqrt normalization X = torch.nn.functional.normalize(X) # L2 normalization å¤šå¡åŒæ­¥ BNï¼ˆBatch normalizationï¼‰ å½“ä½¿ç”¨ torch.nn.DataParallel å°†ä»£ç è¿è¡Œåœ¨å¤šå¼  GPU å¡ä¸Šæ—¶ï¼ŒPyTorch çš„ BN å±‚é»˜è®¤æ“ä½œæ˜¯å„å¡ä¸Šæ•°æ®ç‹¬ç«‹åœ°è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ŒåŒæ­¥ BN ä½¿ç”¨æ‰€æœ‰å¡ä¸Šçš„æ•°æ®ä¸€èµ·è®¡ç®— BN å±‚çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç¼“è§£äº†å½“æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰æ¯”è¾ƒå°æ—¶å¯¹å‡å€¼å’Œæ ‡å‡†å·®ä¼°è®¡ä¸å‡†çš„æƒ…å†µï¼Œæ˜¯åœ¨ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ä¸€ä¸ªæœ‰æ•ˆçš„æå‡æ€§èƒ½çš„æŠ€å·§ã€‚ é“¾æ¥ï¼šhttps://github.com/vacancy/Synchronized-BatchNorm-PyTorch ç±»ä¼¼ BN æ»‘åŠ¨å¹³å‡ å¦‚æœè¦å®ç°ç±»ä¼¼ BN æ»‘åŠ¨å¹³å‡çš„æ“ä½œï¼Œåœ¨ forward å‡½æ•°ä¸­è¦ä½¿ç”¨åŸåœ°ï¼ˆinplaceï¼‰æ“ä½œç»™æ»‘åŠ¨å¹³å‡èµ‹å€¼ã€‚ class BN(torch.nn.Module) def __init__(self): ... self.register_buffer('running_mean', torch.zeros(num_features)) def forward(self, X): ... self.running_mean += momentum * (current - self.running_mean) è®¡ç®—æ¨¡å‹æ•´ä½“å‚æ•°é‡ num_parameters = sum(torch.numel(parameter) for parameter in model.parameters()) ç±»ä¼¼ Keras çš„ model.summary() è¾“å‡ºæ¨¡å‹ä¿¡æ¯ é“¾æ¥ï¼šhttps://github.com/sksq96/pytorch-summary æ¨¡å‹æƒå€¼åˆå§‹åŒ– æ³¨æ„ model.modules() å’Œ model.children() çš„åŒºåˆ«ï¼šmodel.modules() ä¼šè¿­ä»£åœ°éå†æ¨¡å‹çš„æ‰€æœ‰å­å±‚ï¼Œè€Œ model.children() åªä¼šéå†æ¨¡å‹ä¸‹çš„ä¸€å±‚ã€‚ # Common practise for initialization. for layer in model.modules(): if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.BatchNorm2d): torch.nn.init.constant_(layer.weight, val=1.0) torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.Linear): torch.nn.init.xavier_normal_(layer.weight) if layer.bias is not None: torch.nn.init.constant_(layer.bias, val=0.0) # Initialization with given tensor. layer.weight = torch.nn.Parameter(tensor) éƒ¨åˆ†å±‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ æ³¨æ„å¦‚æœä¿å­˜çš„æ¨¡å‹æ˜¯ torch.nn.DataParallelï¼Œåˆ™å½“å‰çš„æ¨¡å‹ä¹Ÿéœ€è¦æ˜¯ model.load_state_dict(torch.load('model,pth'), strict=False) å°†åœ¨ GPU ä¿å­˜çš„æ¨¡å‹åŠ è½½åˆ° CPU model.load_state_dict(torch.load('model,pth', map_location='cpu')) æ•°æ®å‡†å¤‡ã€ç‰¹å¾æå–ä¸å¾®è°ƒ å¾—åˆ°è§†é¢‘æ•°æ®åŸºæœ¬ä¿¡æ¯ import cv2 video = cv2.VideoCapture(mp4_path) height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)) width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)) num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) fps = int(video.get(cv2.CAP_PROP_FPS)) video.release() TSN æ¯æ®µï¼ˆsegmentï¼‰é‡‡æ ·ä¸€å¸§è§†é¢‘ K = self._num_segments if is_train: if num_frames > K: # Random index for each segment. frame_indices = torch.randint( high=num_frames // K, size=(K,), dtype=torch.long) frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.randint( high=num_frames, size=(K - num_frames,), dtype=torch.long) frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), frame_indices)))[0] else: if num_frames > K: # Middle index for each segment. frame_indices = num_frames / K // 2 frame_indices += num_frames // K * torch.arange(K) else: frame_indices = torch.sort(torch.cat(( torch.arange(num_frames), torch.arange(K - num_frames))))[0] assert frame_indices.size() == (K,) return [frame_indices[i] for i in range(K)] æå– ImageNet é¢„è®­ç»ƒæ¨¡å‹æŸå±‚çš„å·ç§¯ç‰¹å¾ # VGG-16 relu5-3 feature. model = torchvision.models.vgg16(pretrained=True).features[:-1] # VGG-16 pool5 feature. model = torchvision.models.vgg16(pretrained=True).features # VGG-16 fc7 feature. model = torchvision.models.vgg16(pretrained=True) model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3]) # ResNet GAP feature. model = torchvision.models.resnet18(pretrained=True) model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) with torch.no_grad(): model.eval() conv_representation = model(image) æå– ImageNet é¢„è®­ç»ƒæ¨¡å‹å¤šå±‚çš„å·ç§¯ç‰¹å¾ class FeatureExtractor(torch.nn.Module): \"\"\"Helper class to extract several convolution features from the given pre-trained model. Attributes: _model, torch.nn.Module. _layers_to_extract, list or set Example: >>> model = torchvision.models.resnet152(pretrained=True) >>> model = torch.nn.Sequential(collections.OrderedDict( list(model.named_children())[:-1])) >>> conv_representation = FeatureExtractor( pretrained_model=model, layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image) \"\"\" def __init__(self, pretrained_model, layers_to_extract): torch.nn.Module.__init__(self) self._model = pretrained_model self._model.eval() self._layers_to_extract = set(layers_to_extract) def forward(self, x): with torch.no_grad(): conv_representation = [] for name, layer in self._model.named_children(): x = layer(x) if name in self._layers_to_extract: conv_representation.append(x) return conv_representation å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ é“¾æ¥ï¼šhttps://github.com/Cadene/pretrained-models.pytorch å¾®è°ƒå…¨è¿æ¥å±‚ model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False model.fc = nn.Linear(512, 100) # Replace the last fc layer optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4) ä»¥è¾ƒå¤§å­¦ä¹ ç‡å¾®è°ƒå…¨è¿æ¥å±‚ï¼Œè¾ƒå°å­¦ä¹ ç‡å¾®è°ƒå·ç§¯å±‚ model = torchvision.models.resnet18(pretrained=True) finetuned_parameters = list(map(id, model.fc.parameters())) conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters) parameters = [{'params': conv_parameters, 'lr': 1e-3}, {'params': model.fc.parameters()}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) æ¨¡å‹è®­ç»ƒ å¸¸ç”¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é¢„å¤„ç† å…¶ä¸­ ToTensor æ“ä½œä¼šå°† PIL.Image æˆ–å½¢çŠ¶ä¸º HÃ—WÃ—Dï¼Œæ•°å€¼èŒƒå›´ä¸º [0, 255] çš„ np.ndarray è½¬æ¢ä¸ºå½¢çŠ¶ä¸º DÃ—HÃ—Wï¼Œæ•°å€¼èŒƒå›´ä¸º [0.0, 1.0] çš„ torch.Tensorã€‚ train_transform = torchvision.transforms.Compose([ torchvision.transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0)), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) val_transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(224), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ]) è®­ç»ƒåŸºæœ¬ä»£ç æ¡†æ¶ for t in epoch(80): for images, labels in tqdm.tqdm(train_loader, desc='Epoch %3d' % (t + 1)): images, labels = images.cuda(), labels.cuda() scores = model(images) loss = loss_function(scores, labels) optimizer.zero_grad() loss.backward() optimizer.step() æ ‡è®°å¹³æ»‘ï¼ˆlabel smoothingï¼‰ for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() N = labels.size(0) # C is the number of classes. smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda() smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9) score = model(images) log_prob = torch.nn.functional.log_softmax(score, dim=1) loss = -torch.sum(log_prob * smoothed_labels) / N optimizer.zero_grad() loss.backward() optimizer.step() Mixup beta_distribution = torch.distributions.beta.Beta(alpha, alpha) for images, labels in train_loader: images, labels = images.cuda(), labels.cuda() # Mixup images. lambda_ = beta_distribution.sample([]).item() index = torch.randperm(images.size(0)).cuda() mixed_images = lambda_ * images + (1 - lambda_) * images[index, :] # Mixup loss. scores = model(mixed_images) loss = (lambda_ * loss_function(scores, labels) + (1 - lambda_) * loss_function(scores, labels[index])) optimizer.zero_grad() loss.backward() optimizer.step() L1 æ­£åˆ™åŒ– l1_regularization = torch.nn.L1Loss(reduction='sum') loss = ... # Standard cross-entropy loss for param in model.parameters(): loss += torch.sum(torch.abs(param)) loss.backward() ä¸å¯¹åç½®é¡¹è¿›è¡Œ L2 æ­£åˆ™åŒ–/æƒå€¼è¡°å‡ï¼ˆweight decayï¼‰ bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias') others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias') parameters = [{'parameters': bias_list, 'weight_decay': 0}, {'parameters': others_list}] optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4) æ¢¯åº¦è£å‰ªï¼ˆgradient clippingï¼‰ torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20) è®¡ç®— Softmax è¾“å‡ºçš„å‡†ç¡®ç‡ score = model(images) prediction = torch.argmax(score, dim=1) num_correct = torch.sum(prediction == labels).item() accuruacy = num_correct / labels.size(0) å¯è§†åŒ–æ¨¡å‹å‰é¦ˆçš„è®¡ç®—å›¾ é“¾æ¥ï¼šhttps://github.com/szagoruyko/pytorchviz å¯è§†åŒ–å­¦ä¹ æ›²çº¿ æœ‰ Facebook è‡ªå·±å¼€å‘çš„ Visdom å’Œ Tensorboard ä¸¤ä¸ªé€‰æ‹©ã€‚ https://github.com/facebookresearch/visdom https://github.com/lanpa/tensorboardX # Example using Visdom. vis = visdom.Visdom(env='Learning curve', use_incoming_socket=False) assert self._visdom.check_connection() self._visdom.close() options = collections.namedtuple('Options', ['loss', 'acc', 'lr'])( loss={'xlabel': 'Epoch', 'ylabel': 'Loss', 'showlegend': True}, acc={'xlabel': 'Epoch', 'ylabel': 'Accuracy', 'showlegend': True}, lr={'xlabel': 'Epoch', 'ylabel': 'Learning rate', 'showlegend': True}) for t in epoch(80): tran(...) val(...) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]), name='train', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]), name='val', win='Loss', update='append', opts=options.loss) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]), name='train', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]), name='val', win='Accuracy', update='append', opts=options.acc) vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]), win='Learning rate', update='append', opts=options.lr) å¾—åˆ°å½“å‰å­¦ä¹ ç‡ # If there is one global learning rate (which is the common case). lr = next(iter(optimizer.param_groups))['lr'] # If there are multiple learning rates for different layers. all_lr = [] for param_group in optimizer.param_groups: all_lr.append(param_group['lr']) å­¦ä¹ ç‡è¡°å‡ # Reduce learning rate when validation accuarcy plateau. scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True) for t in range(0, 80): train(...); val(...) scheduler.step(val_acc) # Cosine annealing learning rate. scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80) # Reduce learning rate by 10 at given epochs. scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1) for t in range(0, 80): scheduler.step() train(...); val(...) # Learning rate warmup by 10 epochs. scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10) for t in range(0, 10): scheduler.step() train(...); val(...) ä¿å­˜ä¸åŠ è½½æ–­ç‚¹ æ³¨æ„ä¸ºäº†èƒ½å¤Ÿæ¢å¤è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼Œä»¥åŠå½“å‰çš„è®­ç»ƒè½®æ•°ã€‚ # Save checkpoint. is_best = current_acc > best_acc best_acc = max(best_acc, current_acc) checkpoint = { 'best_acc': best_acc, 'epoch': t + 1, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), } model_path = os.path.join('model', 'checkpoint.pth.tar') torch.save(checkpoint, model_path) if is_best: shutil.copy('checkpoint.pth.tar', model_path) # Load checkpoint. if resume: model_path = os.path.join('model', 'checkpoint.pth.tar') assert os.path.isfile(model_path) checkpoint = torch.load(model_path) best_acc = checkpoint['best_acc'] start_epoch = checkpoint['epoch'] model.load_state_dict(checkpoint['model']) optimizer.load_state_dict(checkpoint['optimizer']) print('Load checkpoint at epoch %d.' % start_epoch) è®¡ç®—å‡†ç¡®ç‡ã€æŸ¥å‡†ç‡ï¼ˆprecisionï¼‰ã€æŸ¥å…¨ç‡ï¼ˆrecallï¼‰ # data['label'] and data['prediction'] are groundtruth label and prediction # for each image, respectively. accuracy = np.mean(data['label'] == data['prediction']) * 100 # Compute recision and recall for each class. for c in range(len(num_classes)): tp = np.dot((data['label'] == c).astype(int), (data['prediction'] == c).astype(int)) tp_fp = np.sum(data['prediction'] == c) tp_fn = np.sum(data['label'] == c) precision = tp / tp_fp * 100 recall = tp / tp_fn * 100 PyTorch å…¶ä»–æ³¨æ„äº‹é¡¹ æ¨¡å‹å®šä¹‰ å»ºè®®æœ‰å‚æ•°çš„å±‚å’Œæ±‡åˆï¼ˆpoolingï¼‰å±‚ä½¿ç”¨ torch.nn æ¨¡å—å®šä¹‰ï¼Œæ¿€æ´»å‡½æ•°ç›´æ¥ä½¿ç”¨ torch.nn.functionalã€‚torch.nn æ¨¡å—å’Œ torch.nn.functional çš„åŒºåˆ«åœ¨äºï¼Œtorch.nn æ¨¡å—åœ¨è®¡ç®—æ—¶åº•å±‚è°ƒç”¨äº† torch.nn.functionalï¼Œä½† torch.nn æ¨¡å—åŒ…æ‹¬è¯¥å±‚å‚æ•°ï¼Œè¿˜å¯ä»¥åº”å¯¹è®­ç»ƒå’Œæµ‹è¯•ä¸¤ç§ç½‘ç»œçŠ¶æ€ã€‚ä½¿ç”¨ torch.nn.functional æ—¶è¦æ³¨æ„ç½‘ç»œçŠ¶æ€ï¼Œå¦‚ def forward(self, x): ... x = torch.nn.functional.dropout(x, p=0.5, training=self.training) model(x) å‰ç”¨ model.train() å’Œ model.eval() åˆ‡æ¢ç½‘ç»œçŠ¶æ€ã€‚ ä¸éœ€è¦è®¡ç®—æ¢¯åº¦çš„ä»£ç å—ç”¨ with torch.no_grad() åŒ…å«èµ·æ¥ã€‚model.eval() å’Œ torch.no_grad() çš„åŒºåˆ«åœ¨äºï¼Œmodel.eval() æ˜¯å°†ç½‘ç»œåˆ‡æ¢ä¸ºæµ‹è¯•çŠ¶æ€ï¼Œä¾‹å¦‚ BN å’Œéšæœºå¤±æ´»ï¼ˆdropoutï¼‰åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä½¿ç”¨ä¸åŒçš„è®¡ç®—æ–¹æ³•ã€‚torch.no_grad() æ˜¯å…³é—­ PyTorch å¼ é‡çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ï¼Œä»¥å‡å°‘å­˜å‚¨ä½¿ç”¨å’ŒåŠ é€Ÿè®¡ç®—ï¼Œå¾—åˆ°çš„ç»“æœæ— æ³•è¿›è¡Œ loss.backward()ã€‚ torch.nn.CrossEntropyLoss çš„è¾“å…¥ä¸éœ€è¦ç»è¿‡ Softmaxã€‚torch.nn.CrossEntropyLoss ç­‰ä»·äº torch.nn.functional.log_softmax + torch.nn.NLLLossã€‚ loss.backward() å‰ç”¨ optimizer.zero_grad() æ¸…é™¤ç´¯ç§¯æ¢¯åº¦ã€‚optimizer.zero_grad() å’Œ model.zero_grad() æ•ˆæœä¸€æ ·ã€‚ PyTorch æ€§èƒ½ä¸è°ƒè¯• torch.utils.data.DataLoader ä¸­å°½é‡è®¾ç½® pin_memory=Trueï¼Œå¯¹ç‰¹åˆ«å°çš„æ•°æ®é›†å¦‚ MNIST è®¾ç½® pin_memory=False åè€Œæ›´å¿«ä¸€äº›ã€‚num_workers çš„è®¾ç½®éœ€è¦åœ¨å®éªŒä¸­æ‰¾åˆ°æœ€å¿«çš„å–å€¼ã€‚ ç”¨ del åŠæ—¶åˆ é™¤ä¸ç”¨çš„ä¸­é—´å˜é‡ï¼ŒèŠ‚çº¦ GPU å­˜å‚¨ã€‚ ä½¿ç”¨ inplace æ“ä½œå¯èŠ‚çº¦ GPU å­˜å‚¨ï¼Œå¦‚ x = torch.nn.functional.relu(x, inplace=True) å‡å°‘ CPU å’Œ GPU ä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚ä¾‹å¦‚å¦‚æœä½ æƒ³çŸ¥é“ä¸€ä¸ª epoch ä¸­æ¯ä¸ª mini-batch çš„ loss å’Œå‡†ç¡®ç‡ï¼Œå…ˆå°†å®ƒä»¬ç´¯ç§¯åœ¨ GPU ä¸­ç­‰ä¸€ä¸ª epoch ç»“æŸä¹‹åä¸€èµ·ä¼ è¾“å› CPU ä¼šæ¯”æ¯ä¸ª mini-batch éƒ½è¿›è¡Œä¸€æ¬¡ GPU åˆ° CPU çš„ä¼ è¾“æ›´å¿«ã€‚ ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•° half() ä¼šæœ‰ä¸€å®šçš„é€Ÿåº¦æå‡ï¼Œå…·ä½“æ•ˆç‡ä¾èµ–äº GPU å‹å·ã€‚éœ€è¦å°å¿ƒæ•°å€¼ç²¾åº¦è¿‡ä½å¸¦æ¥çš„ç¨³å®šæ€§é—®é¢˜ã€‚ æ—¶å¸¸ä½¿ç”¨ assert tensor.size() == (N, D, H, W) ä½œä¸ºè°ƒè¯•æ‰‹æ®µï¼Œç¡®ä¿å¼ é‡ç»´åº¦å’Œä½ è®¾æƒ³ä¸­ä¸€è‡´ã€‚ é™¤äº†æ ‡è®° y å¤–ï¼Œå°½é‡å°‘ä½¿ç”¨ä¸€ç»´å¼ é‡ï¼Œä½¿ç”¨ n*1 çš„äºŒç»´å¼ é‡ä»£æ›¿ï¼Œå¯ä»¥é¿å…ä¸€äº›æ„æƒ³ä¸åˆ°çš„ä¸€ç»´å¼ é‡è®¡ç®—ç»“æœã€‚ ç»Ÿè®¡ä»£ç å„éƒ¨åˆ†è€—æ—¶ with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile: ... print(profile) æˆ–è€…åœ¨å‘½ä»¤è¡Œè¿è¡Œ python -m torch.utils.bottleneck main.py è‡´è°¢ æ„Ÿè°¢ @äº›è®¸æµå¹´å’Œ@El tnotoçš„å‹˜è¯¯ã€‚ç”±äºä½œè€…æ‰ç–å­¦æµ…ï¼Œæ›´å…¼æ—¶é—´å’Œç²¾åŠ›æ‰€é™ï¼Œä»£ç ä¸­é”™è¯¯ä¹‹å¤„åœ¨æ‰€éš¾å…ï¼Œæ•¬è¯·è¯»è€…æ‰¹è¯„æŒ‡æ­£ã€‚ å‚è€ƒèµ„æ–™ PyTorch å®˜æ–¹ä»£ç ï¼špytorch/examples (https://link.zhihu.com/?target=https%3A//github.com/pytorch/examples) PyTorch è®ºå›ï¼šPyTorch Forums (https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/latest%3Forder%3Dviews) PyTorch æ–‡æ¡£ï¼šhttp://pytorch.org/docs/stable/index.html (https://link.zhihu.com/?target=http%3A//pytorch.org/docs/stable/index.html) å…¶ä»–åŸºäº PyTorch çš„å…¬å¼€å®ç°ä»£ç ï¼Œæ— æ³•ä¸€ä¸€åˆ—ä¸¾ å¼ çš“ï¼šå—äº¬å¤§å­¦è®¡ç®—æœºç³»æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜æ‰€ï¼ˆLAMDAï¼‰ç¡•å£«ç”Ÿï¼Œç ”ç©¶æ–¹å‘ä¸ºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯†åˆ«å’Œæ·±åº¦å­¦ä¹ ã€‚ä¸ªäººä¸»é¡µï¼šhttp://lamda.nju.edu.cn/zhangh/ åŸçŸ¥ä¹é“¾æ¥ï¼š https://zhuanlan.zhihu.com/p/59205847? æŸ¥çœ‹åŸæ–‡ï¼š ç‚¹èµæ”¶è—ï¼šPyTorchå¸¸ç”¨ä»£ç æ®µæ•´ç†åˆé›† "},"code_technique/pytorch/pytorch_train.html":{"url":"code_technique/pytorch/pytorch_train.html","title":"pytorchè®­ç»ƒæŠ€å·§","keywords":"","body":"Pytorchè®­ç»ƒæŠ€å·§ [TOC] 1ã€æŒ‡å®šGPUç¼–å· è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä»…ä¸º0å·è®¾å¤‡ï¼Œè®¾å¤‡åç§°ä¸º /gpu:0ï¼šos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä¸º0,1å·ä¸¤ä¸ªè®¾å¤‡ï¼Œåç§°ä¾æ¬¡ä¸º /gpu:0ã€/gpu:1ï¼š os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" ï¼Œæ ¹æ®é¡ºåºè¡¨ç¤ºä¼˜å…ˆä½¿ç”¨0å·è®¾å¤‡,ç„¶åä½¿ç”¨1å·è®¾å¤‡ã€‚ æŒ‡å®šGPUçš„å‘½ä»¤éœ€è¦æ”¾åœ¨å’Œç¥ç»ç½‘ç»œç›¸å…³çš„ä¸€ç³»åˆ—æ“ä½œçš„å‰é¢ã€‚ 2ã€æŸ¥çœ‹æ¨¡å‹æ¯å±‚è¾“å‡ºè¯¦æƒ… Kerasæœ‰ä¸€ä¸ªç®€æ´çš„APIæ¥æŸ¥çœ‹æ¨¡å‹çš„æ¯ä¸€å±‚è¾“å‡ºå°ºå¯¸ï¼Œè¿™åœ¨è°ƒè¯•ç½‘ç»œæ—¶éå¸¸æœ‰ç”¨ã€‚ç°åœ¨åœ¨PyTorchä¸­ä¹Ÿå¯ä»¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä½¿ç”¨å¾ˆç®€å•ï¼Œå¦‚ä¸‹ç”¨æ³•ï¼š from torchsummary import summary summary(your_model, input_size=(channels, H, W)) input_size æ˜¯æ ¹æ®ä½ è‡ªå·±çš„ç½‘ç»œæ¨¡å‹çš„è¾“å…¥å°ºå¯¸è¿›è¡Œè®¾ç½®ã€‚ pytorch-summarygithub.com 3ã€æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ import torch.nn as nn outputs = model(data) loss= loss_fn(outputs, target) optimizer.zero_grad() loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) optimizer.step() nn.utils.clip_grad_norm_ çš„å‚æ•°ï¼š parameters â€“ ä¸€ä¸ªåŸºäºå˜é‡çš„è¿­ä»£å™¨ï¼Œä¼šè¿›è¡Œæ¢¯åº¦å½’ä¸€åŒ– max_norm â€“ æ¢¯åº¦çš„æœ€å¤§èŒƒæ•° norm_type â€“ è§„å®šèŒƒæ•°çš„ç±»å‹ï¼Œé»˜è®¤ä¸ºL2 4ã€æ‰©å±•å•å¼ å›¾ç‰‡ç»´åº¦ å› ä¸ºåœ¨è®­ç»ƒæ—¶çš„æ•°æ®ç»´åº¦ä¸€èˆ¬éƒ½æ˜¯ (batch_size, c, h, w)ï¼Œè€Œåœ¨æµ‹è¯•æ—¶åªè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œæ‰€ä»¥éœ€è¦æ‰©å±•ç»´åº¦ï¼Œæ‰©å±•ç»´åº¦æœ‰å¤šä¸ªæ–¹æ³•ï¼š import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.view(1, *image.size()) print(img.size()) # output: # torch.Size([h, w, c]) # torch.Size([1, h, w, c]) æˆ– import cv2 import numpy as np image = cv2.imread(img_path) print(image.shape) img = image[np.newaxis, :, :, :] print(img.shape) # output: # (h, w, c) # (1, h, w, c) æˆ–ï¼ˆæ„Ÿè°¢çŸ¥ä¹ç”¨æˆ·coldleafçš„è¡¥å……ï¼‰ import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.unsqueeze(dim=0) print(img.size()) img = img.squeeze(dim=0) print(img.size()) # output: # torch.Size([(h, w, c)]) # torch.Size([1, h, w, c]) # torch.Size([h, w, c]) tensor.unsqueeze(dim)ï¼šæ‰©å±•ç»´åº¦ï¼ŒdimæŒ‡å®šæ‰©å±•å“ªä¸ªç»´åº¦ã€‚ tensor.squeeze(dim)ï¼šå»é™¤dimæŒ‡å®šçš„ä¸”sizeä¸º1çš„ç»´åº¦ï¼Œç»´åº¦å¤§äº1æ—¶ï¼Œsqueeze()ä¸èµ·ä½œç”¨ï¼Œä¸æŒ‡å®šdimæ—¶ï¼Œå»é™¤æ‰€æœ‰sizeä¸º1çš„ç»´åº¦ã€‚ 5ã€ç‹¬çƒ­ç¼–ç  åœ¨PyTorchä¸­ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°çš„æ—¶å€™ä¼šè‡ªåŠ¨æŠŠlabelè½¬åŒ–æˆonehotï¼Œæ‰€ä»¥ä¸ç”¨æ‰‹åŠ¨è½¬åŒ–ï¼Œè€Œä½¿ç”¨MSEéœ€è¦æ‰‹åŠ¨è½¬åŒ–æˆonehotç¼–ç ã€‚ import torch class_num = 8 batch_size = 4 def one_hot(label): \"\"\" å°†ä¸€ç»´åˆ—è¡¨è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç  \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # ä» value ä¸­å–å€¼ï¼Œç„¶åæ ¹æ® dim å’Œ index ç»™ç›¸åº”ä½ç½®èµ‹å€¼ onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor -> Numpy label = torch.LongTensor(batch_size).random_() % class_num # å¯¹éšæœºæ•°å–ä½™ print(one_hot(label)) # output: [[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] Convert int into one-hot formatdiscuss.pytorch.org 6ã€é˜²æ­¢éªŒè¯æ¨¡å‹æ—¶çˆ†æ˜¾å­˜ éªŒè¯æ¨¡å‹æ—¶ä¸éœ€è¦æ±‚å¯¼ï¼Œå³ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œå…³é—­autogradï¼Œå¯ä»¥æé«˜é€Ÿåº¦ï¼ŒèŠ‚çº¦å†…å­˜ã€‚å¦‚æœä¸å…³é—­å¯èƒ½ä¼šçˆ†æ˜¾å­˜ã€‚ with torch.no_grad(): # ä½¿ç”¨modelè¿›è¡Œé¢„æµ‹çš„ä»£ç  pass Pytorch è®­ç»ƒæ—¶æ— ç”¨çš„ä¸´æ—¶å˜é‡å¯èƒ½ä¼šè¶Šæ¥è¶Šå¤šï¼Œå¯¼è‡´ out of memory ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢è¯­å¥æ¥æ¸…ç†è¿™äº›ä¸éœ€è¦çš„å˜é‡ã€‚ torch.cuda.empty_cache() æ›´è¯¦ç»†çš„ä¼˜åŒ–å¯ä»¥æŸ¥çœ‹ ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ å’Œ æ˜¾å­˜åˆ©ç”¨é—®é¢˜ã€‚ 7ã€å­¦ä¹ ç‡è¡°å‡ import torch.optim as optim from torch.optim import lr_scheduler # è®­ç»ƒå‰çš„åˆå§‹åŒ– optimizer = optim.Adam(net.parameters(), lr=0.001) scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # æ¯è¿‡10ä¸ªepochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥0.1 # è®­ç»ƒè¿‡ç¨‹ä¸­ for n in n_epoch: scheduler.step() ... 8ã€å†»ç»“æŸäº›å±‚çš„å‚æ•° å‚è€ƒï¼šPytorch å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„æŸä¸€å±‚ åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ‰æ—¶æƒ³å†»ç»“å‰é¢å‡ å±‚ï¼Œä½¿å…¶å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å‘ç”Ÿå˜åŒ–ã€‚ æˆ‘ä»¬éœ€è¦å…ˆçŸ¥é“æ¯ä¸€å±‚çš„åå­—ï¼Œé€šè¿‡å¦‚ä¸‹ä»£ç æ‰“å°ï¼š net = Network() # è·å–è‡ªå®šä¹‰ç½‘ç»œç»“æ„ for name, value in net.named_parameters(): print('name: {0},\\t grad: {1}'.format(name, value.requires_grad)) å‡è®¾å‰å‡ å±‚ä¿¡æ¯å¦‚ä¸‹ï¼š name: cnn.VGG_16.convolution1_1.weight, grad: True name: cnn.VGG_16.convolution1_1.bias, grad: True name: cnn.VGG_16.convolution1_2.weight, grad: True name: cnn.VGG_16.convolution1_2.bias, grad: True name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True åé¢çš„Trueè¡¨ç¤ºè¯¥å±‚çš„å‚æ•°å¯è®­ç»ƒï¼Œç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¦å†»ç»“çš„å±‚çš„åˆ—è¡¨ï¼š no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias' ] å†»ç»“æ–¹æ³•å¦‚ä¸‹ï¼š net = Net.CTPN() # è·å–ç½‘ç»œç»“æ„ for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True å†»ç»“åæˆ‘ä»¬å†æ‰“å°æ¯å±‚çš„ä¿¡æ¯ï¼š name: cnn.VGG_16.convolution1_1.weight, grad: False name: cnn.VGG_16.convolution1_1.bias, grad: False name: cnn.VGG_16.convolution1_2.weight, grad: False name: cnn.VGG_16.convolution1_2.bias, grad: False name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True å¯ä»¥çœ‹åˆ°å‰ä¸¤å±‚çš„weightå’Œbiasçš„requires_gradéƒ½ä¸ºFalseï¼Œè¡¨ç¤ºå®ƒä»¬ä¸å¯è®­ç»ƒã€‚ æœ€ååœ¨å®šä¹‰ä¼˜åŒ–å™¨æ—¶ï¼Œåªå¯¹requires_gradä¸ºTrueçš„å±‚çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) "},"code_technique/pytorch/pytorch_.html":{"url":"code_technique/pytorch/pytorch_.html","title":"pytorchè§£å†»","keywords":"","body":"ä½œè€…ï¼šycszen é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/25980324 æ¥æºï¼šçŸ¥ä¹ è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ åŠ è½½éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹ å…¶å®å¤§å¤šæ•°æ—¶å€™æˆ‘ä»¬éœ€è¦æ ¹æ®æˆ‘ä»¬çš„ä»»åŠ¡è°ƒèŠ‚æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ‰€ä»¥å¾ˆéš¾ä¿è¯æ¨¡å‹å’Œå…¬å¼€çš„æ¨¡å‹å®Œå…¨ä¸€æ ·ï¼Œä½†æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ç¡®å®æœ‰åŠ©äºæé«˜è®­ç»ƒçš„å‡†ç¡®ç‡ï¼Œä¸ºäº†ç»“åˆäºŒè€…çš„ä¼˜ç‚¹ï¼Œå°±éœ€è¦æˆ‘ä»¬åŠ è½½éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹ã€‚ pretrained_dict = model_zoo.load_url(model_urls['resnet152']) model_dict = model.state_dict() # å°†pretrained_dicté‡Œä¸å±äºmodel_dictçš„é”®å‰”é™¤æ‰ pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict} # æ›´æ–°ç°æœ‰çš„model_dict model_dict.update(pretrained_dict) # åŠ è½½æˆ‘ä»¬çœŸæ­£éœ€è¦çš„state_dict model.load_state_dict(model_dict) å› ä¸ºéœ€è¦å‰”é™¤åŸæ¨¡å‹ä¸­ä¸åŒ¹é…çš„é”®ï¼Œä¹Ÿå°±æ˜¯å±‚çš„åå­—ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„æ–°æ¨¡å‹æ”¹å˜äº†çš„å±‚éœ€è¦å’ŒåŸæ¨¡å‹å¯¹åº”å±‚çš„åå­—ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ï¼šresnetæœ€åä¸€å±‚çš„åå­—æ˜¯fc(PyTorchä¸­)ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¿®æ”¹è¿‡çš„resnetçš„æœ€åä¸€å±‚å°±ä¸èƒ½å–è¿™ä¸ªåå­—ï¼Œå¯ä»¥å«fc_ å¾®æ”¹åŸºç¡€æ¨¡å‹ PyTorchä¸­çš„torchvisioné‡Œå·²ç»æœ‰å¾ˆå¤šå¸¸ç”¨çš„æ¨¡å‹äº†ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ï¼š AlexNet VGG ResNet SqueezeNet DenseNet import torchvision.models as models resnet18 = models.resnet18() alexnet = models.alexnet() squeezenet = models.squeezenet1_0() densenet = models.densenet_161() ä½†æ˜¯å¯¹äºæˆ‘ä»¬çš„ä»»åŠ¡è€Œè¨€æœ‰äº›å±‚å¹¶ä¸æ˜¯ç›´æ¥èƒ½ç”¨ï¼Œéœ€è¦æˆ‘ä»¬å¾®å¾®æ”¹ä¸€ä¸‹ï¼Œæ¯”å¦‚ï¼Œresnetæœ€åçš„å…¨è¿æ¥å±‚æ˜¯åˆ†1000ç±»ï¼Œè€Œæˆ‘ä»¬åªæœ‰21ç±»ï¼›åˆæ¯”å¦‚ï¼Œresnetç¬¬ä¸€å±‚å·ç§¯æ¥æ”¶çš„é€šé“æ˜¯3ï¼Œ æˆ‘ä»¬å¯èƒ½è¾“å…¥å›¾ç‰‡çš„é€šé“æ˜¯4ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•ä¿®æ”¹ï¼š resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False) resnet.fc = nn.Linear(2048, 21) "},"code_technique/pytorch/pytorch_vision.html":{"url":"code_technique/pytorch/pytorch_vision.html","title":"pytorchç½‘ç»œå¯è§†åŒ–","keywords":"","body":"Pytorchç½‘ç»œç»“æ„å¯è§†åŒ– å®‰è£… å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„å‘½ä»¤è¿›è¡Œå®‰è£… conda install pytorch-nightly -c pytorch conda install graphviz conda install torchvision conda install tensorwatch æœ¬æ•™ç¨‹åŸºäºä»¥ä¸‹çš„ç‰ˆæœ¬ï¼š torchvision.__version__ '0.2.1' torch.__version__ '1.2.0.dev20190610' sys.version '3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) \\n[GCC 7.3.0]' è½½å…¥åº“ import sys import torch import tensorwatch as tw import torchvision.models ç½‘ç»œç»“æ„å¯è§†åŒ– alexnet_model = torchvision.models.alexnet() tw.draw_model(alexnet_model, [1, 3, 224, 224]) è½½å…¥alexnetï¼Œdraw_modelå‡½æ•°éœ€è¦ä¼ å…¥ä¸‰ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªä¸ºmodelï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºinput_shapeï¼Œç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºorientationï¼Œå¯ä»¥é€‰æ‹©'LR'æˆ–è€…'TB'ï¼Œåˆ†åˆ«ä»£è¡¨å·¦å³å¸ƒå±€ä¸ä¸Šä¸‹å¸ƒå±€ã€‚ åœ¨notebookä¸­ï¼Œæ‰§è¡Œå®Œä¸Šé¢çš„ä»£ç ä¼šæ˜¾ç¤ºå¦‚ä¸‹çš„å›¾ï¼Œå°†ç½‘ç»œçš„ç»“æ„åŠå„ä¸ªå±‚çš„nameå’Œshapeè¿›è¡Œäº†å¯è§†åŒ–ã€‚ ç»Ÿè®¡ç½‘ç»œå‚æ•° å¯ä»¥é€šè¿‡model_statsæ–¹æ³•ç»Ÿè®¡å„å±‚çš„å‚æ•°æƒ…å†µã€‚ tw.model_stats(alexnet_model, [1, 3, 224, 224]) [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! [MAdd]: Dropout is not supported! [Flops]: Dropout is not supported! [Memory]: Dropout is not supported! alexnet_model.features Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) alexnet_model.classifier Sequential( (0): Dropout(p=0.5) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) å‚è€ƒ https://github.com/microsoft/te "},"code_technique/pytorch/PSNR_SSIM.html":{"url":"code_technique/pytorch/PSNR_SSIM.html","title":"PSNR&&SSIM","keywords":"","body":" import torch import torch.nn.functional as F from math import exp import numpy as np â€‹ # è®¡ç®—ä¸€ç»´çš„é«˜æ–¯åˆ†å¸ƒå‘é‡ def gaussian(window_size, sigma): gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)]) return gauss/gauss.sum() â€‹ # åˆ›å»ºé«˜æ–¯æ ¸ï¼Œé€šè¿‡ä¸¤ä¸ªä¸€ç»´é«˜æ–¯åˆ†å¸ƒå‘é‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•å¾—åˆ° # å¯ä»¥è®¾å®šchannelå‚æ•°æ‹“å±•ä¸º3é€šé“ def create_window(window_size, channel=1): _1D_window = gaussian(window_size, 1.5).unsqueeze(1) _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0) window = _2D_window.expand(channel, 1, window_size, window_size).contiguous() return window â€‹ # è®¡ç®—SSIM # ç›´æ¥ä½¿ç”¨SSIMçš„å…¬å¼ï¼Œä½†æ˜¯åœ¨è®¡ç®—å‡å€¼æ—¶ï¼Œä¸æ˜¯ç›´æ¥æ±‚åƒç´ å¹³å‡å€¼ï¼Œè€Œæ˜¯é‡‡ç”¨å½’ä¸€åŒ–çš„é«˜æ–¯æ ¸å·ç§¯æ¥ä»£æ›¿ã€‚ # åœ¨è®¡ç®—æ–¹å·®å’Œåæ–¹å·®æ—¶ç”¨åˆ°äº†å…¬å¼Var(X)=E[X^2]-E[X]^2, cov(X,Y)=E[XY]-E[X]E[Y]. # æ­£å¦‚å‰é¢æåˆ°çš„ï¼Œä¸Šé¢æ±‚æœŸæœ›çš„æ“ä½œé‡‡ç”¨é«˜æ–¯æ ¸å·ç§¯ä»£æ›¿ã€‚ def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None): # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh). if val_range is None: if torch.max(img1) > 128: max_val = 255 else: max_val = 1 if torch.min(img1) â€‹â€‹ # Classes to re-use window class SSIM(torch.nn.Module): def __init__(self, window_size=11, size_average=True, val_range=None): super(SSIM, self).__init__() self.window_size = window_size self.size_average = size_average self.val_range = val_range # Assume 1 channel for SSIM self.channel = 1 self.window = create_window(window_size) def forward(self, img1, img2): (_, channel, _, _) = img1.size() if channel == self.channel and self.window.dtype == img1.dtype: window = self.window else: window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype) self.window = window self.channel = channel return ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average) æˆ‘å†™å¥½çš„æ–‡ä»¶å¤¹ä¸‹åå­—å¯¹å…¶ PSNR/SSIM ''' calculate the PSNR and SSIM. same as MATLAB's results ''' import os import math import numpy as np import cv2 import glob import sys def main(): # Configurations # GT - Ground-truth; # Gen: Generated / Restored / Recovered images # folder_GT = 'DMDM40256no_gen' # folder_GT = 'testDNDN256no_gen' folder_GT = 'testDWNW_199_256no_gen' folder_Gen = 'testB' crop_border = 4 suffix = '/' # suffix for Gen images test_Y = False # True: test Y channel only; False: test RGB channels PSNR_all = [] SSIM_all = [] img_list = sorted(glob.glob(folder_GT + '/*')) if test_Y: print('Testing Y channel.') else: print('Testing RGB channels.') print(len(img_list)) # fo = open(\"DN-gt.txt\", \"w\") fo = open(\"DW-gt.txt\", \"w\") # fo = open(\"DM-gt.txt\", \"w\") for i, img_path in enumerate(img_list): base_name = os.path.splitext(os.path.basename(img_path))[0] # print(base_name,img_path,folder_Gen,suffix) im_GT = cv2.imread(img_path) # print(type(im_GT)) im_GT = im_GT / 255 im_Gen = cv2.imread(os.path.join(folder_Gen, base_name + '.jpg')) # print(type(im_Gen),os.path.join(folder_Gen, base_name + '.jpg')) im_Gen = im_Gen / 255 if test_Y and im_GT.shape[2] == 3: # evaluate on Y channel in YCbCr color space im_GT_in = bgr2ycbcr(im_GT) im_Gen_in = bgr2ycbcr(im_Gen) else: im_GT_in = im_GT im_Gen_in = im_Gen # crop borders if im_GT_in.ndim == 3: cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border, :] cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border, :] elif im_GT_in.ndim == 2: cropped_GT = im_GT_in[crop_border:-crop_border, crop_border:-crop_border] cropped_Gen = im_Gen_in[crop_border:-crop_border, crop_border:-crop_border] else: raise ValueError('Wrong image dimension: {}. Should be 2 or 3.'.format(im_GT_in.ndim)) # calculate PSNR and SSIM PSNR = calculate_psnr(cropped_GT * 255, cropped_Gen * 255) SSIM = calculate_ssim(cropped_GT * 255, cropped_Gen * 255) # print('{:3d} - {:25}. \\tPSNR: {:.6f} dB, \\tSSIM: {:.6f}'.format(i + 1, base_name, PSNR, SSIM)) # print (\"æ–‡ä»¶å: \", fo.name) fo.write('{:3d} - {:25}. \\tPSNR: {:.6f} dB, \\tSSIM: {:.6f}\\n'.format( i + 1, base_name, PSNR, SSIM)) # print(i,\"/\",len(img_list),i/len(img_list),\"%\") print(i) PSNR_all.append(PSNR) SSIM_all.append(SSIM) # print('Average: PSNR: {:.6f} dB, SSIM: {:.6f}'.format(sum(PSNR_all) / len(PSNR_all),sum(SSIM_all) / len(SSIM_all))) fo.write('Average: PSNR: {:.6f} dB, SSIM: {:.6f}\\n'.format( sum(PSNR_all) / len(PSNR_all), sum(SSIM_all) / len(SSIM_all))) # å…³é—­æ‰“å¼€çš„æ–‡ä»¶ fo.close() def calculate_psnr(img1, img2): # img1 and img2 have range [0, 255] img1 = img1.astype(np.float64) img2 = img2.astype(np.float64) mse = np.mean((img1 - img2)**2) if mse == 0: return float('inf') return 20 * math.log10(255.0 / math.sqrt(mse)) def ssim(img1, img2): C1 = (0.01 * 255)**2 C2 = (0.03 * 255)**2 img1 = img1.astype(np.float64) img2 = img2.astype(np.float64) kernel = cv2.getGaussianKernel(11, 1.5) window = np.outer(kernel, kernel.transpose()) mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5] # valid mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5] mu1_sq = mu1**2 mu2_sq = mu2**2 mu1_mu2 = mu1 * mu2 sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2 ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)) return ssim_map.mean() def calculate_ssim(img1, img2): '''calculate SSIM the same outputs as MATLAB's img1, img2: [0, 255] ''' if not img1.shape == img2.shape: raise ValueError('Input images must have the same dimensions.') if img1.ndim == 2: return ssim(img1, img2) elif img1.ndim == 3: if img1.shape[2] == 3: ssims = [] for i in range(3): ssims.append(ssim(img1, img2)) return np.array(ssims).mean() elif img1.shape[2] == 1: return ssim(np.squeeze(img1), np.squeeze(img2)) else: raise ValueError('Wrong input image dimensions.') def bgr2ycbcr(img, only_y=True): '''same as matlab rgb2ycbcr only_y: only return Y channel Input: uint8, [0, 255] float, [0, 1] ''' in_img_type = img.dtype img.astype(np.float32) if in_img_type != np.uint8: img *= 255. # convert if only_y: rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0 else: rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128] if in_img_type == np.uint8: rlt = rlt.round() else: rlt /= 255. return rlt.astype(in_img_type) if __name__ == '__main__': main() ÃŸ #PSNR per_image_mse_loss = F.mse_loss (gen_hr,imgs_hr, reduction='none') per_image_psnr = 10 * torch.log10 (10 / per_image_mse_loss) tensor_average_psnr = torch.mean (per_image_psnr).item () #SSIM import pytorch_ssim import torch from torch.autograd import Variable img1 = Variable(torch.rand(1, 1, 256, 256)) img2 = Variable(torch.rand(1, 1, 256, 256)) if torch.cuda.is_available(): img1 = img1.cuda() img2 = img2.cuda() print(pytorch_ssim.ssim(img1, img2)) ssim_loss = pytorch_ssim.SSIM(window_size = 11) print(ssim_loss(img1, img2)) #MSSSIM import pytorch_ssim import torch from torch.autograd import Variable device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') m = pytorch_msssim.MSSSIM() img1 = torch.rand(1, 1, 256, 256) img2 = torch.rand(1, 1, 256, 256) print(pytorch_msssim.msssim(img1, img2)) print(m(img1, img2))) "},"code_technique/pytorch/SPP.html":{"url":"code_technique/pytorch/SPP.html","title":"SPP","keywords":"","body":"ä½¿ç”¨ Spatial Pyramid Pooling è®© CNN æ¥å—å¯å˜å°ºå¯¸çš„å›¾åƒ https://oidiotlin.com/sppnet-tutorial/ ç›®å½• ä¼ ç»Ÿ CNN çš„å¼Šç«¯ SPP-Net æ¦‚è¿° SPP-Net ç»“æ„ç»†èŠ‚ SPP-Net è®­ç»ƒæ–¹æ³• åœ¨ pytorch æ¡†æ¶ä¸­å®ç° SPP å‚è€ƒè®ºæ–‡ï¼šSpatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition ä¼ ç»Ÿ CNN çš„å¼Šç«¯ åœ¨ä¼ ç»Ÿ CNN ä¸­ï¼Œç”±äº Fully-Connected å±‚çš„å­˜åœ¨ï¼Œè¾“å…¥å›¾åƒçš„å°ºå¯¸å—åˆ°äº†ä¸¥æ ¼é™åˆ¶ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹åŸå§‹å›¾ç‰‡è¿›è¡Œè£å‰ªï¼ˆcropï¼‰æˆ–å˜å½¢ï¼ˆwarpï¼‰çš„æ“ä½œæ¥è°ƒæ•´å…¶å°ºå¯¸ä½¿å…¶é€‚é…äº CNNã€‚ç„¶è€Œè£å‰ªè¿‡çš„å›¾ç‰‡å¯èƒ½åŒ…å«ä¸äº†æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼Œè€Œæ”¹å˜çºµæ¨ªæ¯”çš„å˜å½¢æ“ä½œä¹Ÿå¯èƒ½ä¼šä½¿å…³é”®éƒ¨åˆ†äº§ç”ŸéæœŸæœ›çš„å½¢å˜ã€‚ç”±äºå›¾ç‰‡å†…å®¹çš„ä¸¢å¤±æˆ–å¤±çœŸï¼Œæ¨¡å‹çš„å‡†ç¡®åº¦ä¼šå—åˆ°å¾ˆå¤§çš„å½±å“ã€‚ ä¸Šå›¾ä¸­åˆ†åˆ«è¡¨ç°äº†ä¸¤ç§ resize çš„æ–¹æ³•ï¼šè£å‰ªï¼ˆå·¦ï¼‰ã€å˜å½¢ï¼ˆå³ï¼‰ã€‚å®ƒä»¬å¯¹åŸå›¾éƒ½é€ æˆäº†éæœŸæœ›çš„å½±å“ã€‚ SPP-Net æ¦‚è¿° ä» CNN çš„ç»“æ„æ¥çœ‹ï¼Œæˆ‘ä»¬éœ€è¦è®©å›¾åƒåœ¨è¿›å…¥ FC å±‚å‰å°±å°†å°ºåº¦å›ºå®šåˆ°æŒ‡å®šå¤§å°ã€‚é€šè¿‡ä¿®æ”¹å·ç§¯å±‚æˆ–æ± åŒ–å±‚å‚æ•°å¯ä»¥æ”¹å˜å›¾ç‰‡å¤§å°ï¼Œå…¶ä¸­æ± åŒ–å±‚ä¸å…·æœ‰å­¦ä¹ åŠŸèƒ½ï¼Œå…¶å‚æ•°ä¸ä¼šéšç€è®­ç»ƒè¿‡ç¨‹å˜åŒ–ï¼Œè‡ªç„¶è€Œç„¶æ‰¿æ‹…èµ·æ”¹å˜ spatial size çš„å·¥ä½œã€‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ä¸ª FC å±‚å‰åŠ å…¥ä¸€ä¸ªç‰¹æ®Šçš„æ± åŒ–å±‚ï¼Œå…¶å‚æ•°æ˜¯éšç€è¾“å…¥å¤§å°è€Œæˆæ¯”ä¾‹å˜åŒ–çš„ã€‚ å›¾1. ä½¿ç”¨ crop æˆ– warp æ–¹æ³•çš„ CNN çš„å±‚çº§ç»“æ„ã€‚å›¾2. åœ¨å·ç§¯å±‚ä¸ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹é—´åŠ å…¥ SPP å±‚ã€‚ SPP-Net ä¸­æœ‰è‹¥å¹²ä¸ªå¹¶è¡Œçš„æ± åŒ–å±‚ï¼Œå°†å·ç§¯å±‚çš„ç»“æœ [wÃ—hÃ—d][wÃ—hÃ—d] æ± åŒ–æˆ [1Ã—1],[2Ã—2],[4Ã—4],â‹¯[1Ã—1],[2Ã—2],[4Ã—4],â‹¯ çš„ä¸€å±‚å±‚ç»“æœï¼Œå†å°†å…¶æ‰€æœ‰ç»“æœä¸ FC å±‚ç›¸è¿ã€‚ å½“è¾“å…¥ä¸ºä»»æ„å¤§å°çš„å›¾ç‰‡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥éšæ„è¿›è¡Œå·ç§¯ã€æ± åŒ–ï¼Œåœ¨ FC å±‚ä¹‹å‰ï¼Œé€šè¿‡ SPP å±‚ï¼Œå°†å›¾ç‰‡æŠ½è±¡å‡ºå›ºå®šå¤§å°çš„ç‰¹å¾ï¼ˆå³å¤šå°ºåº¦ç‰¹å¾ä¸‹çš„å›ºå®šç‰¹å¾å‘é‡æŠ½å–ï¼‰ã€‚ SPP-Net ç»“æ„ç»†èŠ‚ ç»“æ„å¦‚ä¸Šæ‰€ç¤ºï¼Œå·²çŸ¥è¾“å…¥ conv5 çš„å¤§å°æ˜¯ [wÃ—hÃ—d][wÃ—hÃ—d]ï¼ŒSPP ä¸­æŸä¸€å±‚è¾“å‡ºç»“æœå¤§å°ä¸º [nÃ—nÃ—d][nÃ—nÃ—d]ï¼Œé‚£ä¹ˆå¦‚ä½•è®¾å®šè¯¥å±‚çš„å‚æ•°å‘¢ï¼Ÿ æ„Ÿå—é‡å¤§å° [wrÃ—hr][wrÃ—hr]ï¼šwr=âŒˆwnâŒ‰wr=âŒˆwnâŒ‰ï¼Œhr=âŒˆhnâŒ‰hr=âŒˆhnâŒ‰ æ­¥é•¿ (sw,sh)(sw,sh)ï¼šsw=âŒŠwnâŒ‹sw=âŒŠwnâŒ‹ï¼Œsh=âŒŠhnâŒ‹sh=âŒŠhnâŒ‹ å‡è®¾è¾“å…¥æ˜¯ [30Ã—42Ã—256][30Ã—42Ã—256]ï¼Œå¯¹äº SPP ä¸­ [4Ã—4][4Ã—4] çš„å±‚è€Œè¨€ï¼Œå…¶ï¼š æ„Ÿå—é‡å¤§å°åº”ä¸º [âŒˆ304âŒ‰Ã—âŒˆ424âŒ‰]=[8Ã—11][âŒˆ304âŒ‰Ã—âŒˆ424âŒ‰]=[8Ã—11] æ­¥é•¿åº”ä¸º (âŒŠ304âŒ‹,âŒŠ424âŒ‹)=(7,10)(âŒŠ304âŒ‹,âŒŠ424âŒ‹)=(7,10) æœ€åå†å°† SPP ä¸­æ‰€æœ‰å±‚çš„æ± åŒ–ç»“æœï¼ˆæ± åŒ–æ“ä½œé€šå¸¸æ˜¯å–æ„Ÿå—é‡å†…çš„ maxï¼‰å˜æˆ 1 ç»´å‘é‡ï¼Œå¹¶ä¸ FC å±‚ä¸­çš„ç¥ç»å…ƒè¿æ¥ã€‚ å¦‚ä¸Šå›¾ä¸­çš„ SPP æœ‰ä¸‰å±‚([1Ã—1],[2Ã—2],[4Ã—4][1Ã—1],[2Ã—2],[4Ã—4])ï¼Œåˆ™é€šè¿‡ SPP åçš„ç‰¹å¾æœ‰ (1+4+16)Ã—256(1+4+16)Ã—256 ä¸ªã€‚ SPP-Net è®­ç»ƒæ–¹æ³• è™½ç„¶ä½¿ç”¨äº† SPPï¼Œç†è®ºä¸Šå¯ä»¥ç›´æ¥ç”¨å˜å°ºåº¦çš„å›¾åƒé›†ä½œä¸ºè¾“å…¥è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯å¸¸ç”¨çš„ä¸€äº›æ¡†æ¶ï¼ˆå¦‚ CUDA-convnetã€Caffeç­‰ï¼‰åœ¨åº•å±‚å®ç°ä¸­æ›´é€‚åˆå›ºå®šå°ºå¯¸çš„è®¡ç®—ï¼ˆæ•ˆç‡æ›´é«˜ï¼‰ã€‚åŸè®ºæ–‡ä¸­æåŠäº†ä¸¤ç§è®­ç»ƒæ–¹æ³•ï¼š Single-Sizeï¼šå°†æ‰€æœ‰çš„å›¾ç‰‡å›ºå®šåˆ°åŒä¸€å°ºåº¦ã€‚ Multi-Sizeï¼šå°†åŸå›¾ç‰‡é€šè¿‡ crop å¾—åˆ°æŸä¸€å°ºåº¦ Aï¼Œå†æŠŠ A é€šè¿‡ warp æ”¾ç¼©æˆæ›´å°çš„å°ºå¯¸ Bã€‚ä¹‹åç”¨ A å°ºåº¦è®­ç»ƒä¸€ä¸ª epochï¼Œå†ç”¨ B å°ºåº¦è®­ç»ƒä¸€ä¸ª epochï¼Œäº¤æ›¿è¿­ä»£ã€‚ ç”±ä½•å‡¯æ˜ç­‰äººçš„å®éªŒç»“æœå¯ä»¥å‘ç°ï¼Œé‡‡ç”¨ Multi-Size æ–¹æ³•è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹é”™è¯¯ç‡æ›´ä½ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚ åœ¨ pytorch æ¡†æ¶ä¸­å®ç° SPP class SpatialPyramidPool2D(nn.Module): \"\"\" Args: out_side (tuple): Length of side in the pooling results of each pyramid layer. Inputs: - `input`: the input Tensor to invert ([batch, channel, width, height]) \"\"\" def __init__(self, out_side): super(SpatialPyramidPool2D, self).__init__() self.out_side = out_side def forward(self, x): out = None for n in self.out_side: w_r, h_r = map(lambda s: math.ceil(s/n), x.size()[2:]) # Receptive Field Size s_w, s_h = map(lambda s: math.floor(s/n), x.size()[2:]) # Stride max_pool = nn.MaxPool2d(kernel_size=(w_r, h_r), stride=(s_w, s_h)) y = max_pool(x) if out is None: out = y.view(y.size()[0], -1) else: out = torch.cat((out, y.view(y.size()[0], -1)), 1) return out å¯ä»¥åœ¨æ¨¡å‹ä¸­æ’å…¥è¯¥æ¨¡å—ï¼Œå¦‚ï¼š nn.Sequential( nn.Conv2d( in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2, ), nn.ReLU(), SpatialPyramidPool2D(out_side=(1,2,4)) ) åœ¨ pytorch ä¸­å»ºç«‹è‡ªå·±çš„å›¾ç‰‡æ•°æ®é›† ç›®å½• å›¾ç‰‡æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ å›¾ç‰‡æ–‡ä»¶åœ¨ä¸åŒç›®å½•ä¸‹ é€šå¸¸æƒ…å†µä¸‹ï¼Œå¾…å¤„ç†çš„å›¾ç‰‡æ•°æ®æœ‰ä¸¤ç§å­˜æ”¾æ–¹å¼ï¼š æ‰€æœ‰å›¾ç‰‡åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¦æœ‰ä¸€ä»½æ–‡æœ¬æ–‡ä»¶è®°å½•äº†æ ‡ç­¾ã€‚ ä¸åŒæ ‡ç­¾çš„å›¾ç‰‡æ”¾åœ¨ä¸åŒç›®å½•ä¸‹ï¼Œæ–‡ä»¶å¤¹åå°±æ˜¯æ ‡ç­¾ã€‚ å¯¹äºè¿™ä¸¤ç§æƒ…å†µï¼Œæˆ‘ä»¬æœ‰ä¸åŒçš„è§£å†³æ–¹æ³•ã€‚ å›¾ç‰‡æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ å‡è®¾åœ¨ ./data/ ç›®å½•ä¸‹æœ‰æ‰€éœ€çš„æ‰€æœ‰çš„å›¾ç‰‡ï¼Œä»¥åŠä¸€ä»½æ ‡è®°äº†å›¾ç‰‡æ ‡ç­¾çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆåˆ—ä¸ºå›¾ç‰‡è·¯å¾„+æ ‡ç­¾ï¼‰./labels.txt ./data/IZvVCYcuOkcu6Ufj.jpg 0 ./data/2wuPp4yYoc2wJbZI.jpg 0 ./data/vzlBbG4Z1KKJ4P6L.jpg 1 ./data/nR8VZBPbjF92wNGC.jpg 2 ...... æ€è·¯æ˜¯ç»§æ‰¿ torch.utils.data.Datasetï¼Œå¹¶é‡ç‚¹é‡å†™å…¶ __getitem__ æ–¹æ³•ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š class CustomDataset(Dataset): def __init__(self, label_file_path): with open(label_file_path, 'r') as f: # (image_path(str), image_label(str)) self.imgs = list(map(lambda line: line.strip().split(' '), f)) def __getitem__(self, index): path, label = self.imgs[index] img = transforms.Compose([transforms.Scale(224), transforms.CenterCrop(224), transforms.ToTensor(),])(Image.open(path).convert('RGB')) label = int(label) return img, label def __len__(self): return len(self.imgs) dataset = CustomDataset('./labels.txt') loader = DataLoader(dataset, batch_size=64, shuffle=True) è‡³æ­¤ï¼Œå¯ä»¥ç”¨ enumerate(loader) çš„æ–¹å¼è¿­ä»£æ•°æ®äº†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ __getitem__æ—¶è¦ç¡®ä¿ batch å†…å›¾ç‰‡å°ºå¯¸ç›¸åŒï¼ˆä¸Šé¢çš„ä¾‹å­ç”¨äº† Scale+CenterCrop çš„æ–¹æ³•ï¼‰ï¼Œå¦åˆ™ä¼šå‡ºç° RuntimeError: inconsistent tensor sizes at ... çš„é”™è¯¯ã€‚ å›¾ç‰‡æ–‡ä»¶åœ¨ä¸åŒç›®å½•ä¸‹ å½“å›¾ç‰‡æ–‡ä»¶ä¾æ® label å¤„äºä¸åŒæ–‡ä»¶ä¸‹æ—¶ï¼Œå¦‚ï¼š â”€â”€â”€ data â”œâ”€â”€ è™¾é¥º â”‚ â”œâ”€â”€ 00856315f0df13536183d8ae6cbaf8d6a54f37ce.jpg â”‚ â””â”€â”€ 00ce9dccdf9a218d3b891e006c81f8e66524b1b3.jpg â”œâ”€â”€ å…«å®ç²¥ â”‚ â”œâ”€â”€ 055133235f649411e599ce5dba83627d58996209.jpg â”‚ â””â”€â”€ 0a72473884cb6c03191ca929a9aa0b2bbe4abb3d.jpg â””â”€â”€ é’µä»”ç³• â”œâ”€â”€ 1237b1e7b7e7da0ac78f9e1c8317b9462fe92803.jpg â””â”€â”€ 14a7d6c1a881d1dcfe855bf783064ad2c9d5aba4.jpg æ­¤æ—¶æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ torchvision.datasets.ImageFolder æ¥ç›´æ¥æ„é€ å‡º datasetï¼Œä»£ç å¦‚ä¸‹ï¼š dataset = ImageFolder(path) loader = DataLoader(dataset) ImageFolder ä¼šå°†ç›®å½•ä¸­çš„æ–‡ä»¶å¤¹åè‡ªåŠ¨è½¬åŒ–æˆåºåˆ—ï¼Œé‚£ä¹ˆ loader è½½å…¥æ—¶ï¼Œæ ‡ç­¾å°±æ˜¯æ•´æ•°åºåˆ—äº†ã€‚ ä½œè€…ï¼šé¾™é¹-è¨€æœ‰ä¸‰ é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/39455807 æ¥æºï¼šçŸ¥ä¹ è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ ä¸‹é¢ä¸€ä¸ªä¸€ä¸ªè§£é‡Šï¼Œå®Œæ•´ä»£ç è¯·ç§»æ­¥ Git å·¥ç¨‹ã€‚ ï¼ˆ1ï¼‰datasets.ImageFolder Pytorch çš„ torchvision æ¨¡å—ä¸­æä¾›äº†ä¸€ä¸ª dataset åŒ…ï¼Œå®ƒåŒ…å«äº†ä¸€äº›åŸºæœ¬çš„æ•°æ®é›†å¦‚ mnistã€cocoã€imagenet å’Œä¸€ä¸ªé€šç”¨çš„æ•°æ®åŠ è½½å™¨ ImageFolderã€‚ å®ƒä¼šä»¥è¿™æ ·çš„å½¢å¼ç»„ç»‡æ•°æ®ï¼Œå…·ä½“çš„è¯·åˆ° Git å·¥ç¨‹ä¸­æŸ¥çœ‹ã€‚ root/left/1.png root/left/2.png root/left/3.png root/right/1.png root/right/2.png root/right/3.png imagefolder æœ‰3ä¸ªæˆå‘˜å˜é‡ã€‚ self.classesï¼šç”¨ä¸€ä¸ª list ä¿å­˜ç±»åï¼Œå°±æ˜¯æ–‡ä»¶å¤¹çš„åå­—ã€‚ self.class_to_idxï¼šç±»åå¯¹åº”çš„ç´¢å¼•ï¼Œå¯ä»¥ç†è§£ä¸º 0ã€1ã€2ã€3 ç­‰ã€‚ self.imgsï¼šä¿å­˜ï¼ˆimgpathï¼Œclassï¼‰ï¼Œæ˜¯å›¾ç‰‡å’Œç±»åˆ«çš„æ•°ç»„ã€‚ ä¸åŒæ–‡ä»¶å¤¹ä¸‹çš„å›¾ï¼Œä¼šè¢«å½“ä½œä¸åŒçš„ç±»ï¼Œå¤©ç”Ÿå°±ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚ ï¼ˆ2ï¼‰Transforms è¿™ä¸€ç‚¹è·Ÿ Caffe éå¸¸ç±»ä¼¼ï¼Œå°±æ˜¯å®šä¹‰äº†ä¸€ç³»åˆ—æ•°æ®é›†çš„é¢„å¤„ç†å’Œå¢å¼ºæ“ä½œã€‚åˆ°æ­¤ï¼Œæ•°æ®æ¥å£å°±å®šä¹‰å®Œæ¯•äº†ï¼Œæ¥ä¸‹æ¥åœ¨è®­ç»ƒä»£ç ä¸­çœ‹å¦‚ä½•ä½¿ç”¨è¿­ä»£å™¨è¿›è¡Œæ•°æ®è¯»å–å°±å¯ä»¥äº†ï¼ŒåŒ…æ‹¬ scaleã€å‡å‡å€¼ç­‰ã€‚ ï¼ˆ3ï¼‰torch.utils.data.DataLoader è¿™å°±æ˜¯åˆ›å»ºäº†ä¸€ä¸ª batchï¼Œç”ŸæˆçœŸæ­£ç½‘ç»œçš„è¾“å…¥ã€‚å…³äºæ›´å¤š Pytorch çš„æ•°æ®è¯»å–æ–¹æ³•ï¼Œè¯·ç§»æ­¥çŸ¥ä¹ä¸“æ å’Œå…¬ä¼—å·ï¼Œé“¾æ¥åœ¨å‰é¢çš„è¯¾ç¨‹ä¸­æœ‰ç»™å‡ºã€‚ 2.2 æ¨¡å‹å®šä¹‰ å¦‚ä¸‹ï¼š import torch import torch.nn as nn import torch.nn.functional as F import numpy as np class simpleconv3(nn.Module):` def __init__(self): super(simpleconv3,self).__init__() self.conv1 = nn.Conv2d(3, 12, 3, 2) self.bn1 = nn.BatchNorm2d(12) self.conv2 = nn.Conv2d(12, 24, 3, 2) self.bn2 = nn.BatchNorm2d(24) self.conv3 = nn.Conv2d(24, 48, 3, 2) self.bn3 = nn.BatchNorm2d(48) self.fc1 = nn.Linear(48 * 5 * 5 , 1200) self.fc2 = nn.Linear(1200 , 128) self.fc3 = nn.Linear(128 , 2) def forward(self , x): x = F.relu(self.bn1(self.conv1(x))) #print \"bn1 shape\",x.shape x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = x.view(-1 , 48 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x è¿™ä¸‰èŠ‚è¯¾çš„ä»»åŠ¡ï¼Œéƒ½æ˜¯é‡‡ç”¨ä¸€ä¸ªç®€å•çš„ 3 å±‚å·ç§¯ + 2 å±‚å…¨è¿æ¥å±‚çš„ç½‘ç»œç»“æ„ã€‚æ ¹æ®ä¸Šé¢çš„ç½‘ç»œç»“æ„çš„å®šä¹‰ï¼Œéœ€è¦åšä»¥ä¸‹äº‹æƒ…ã€‚ ï¼ˆ1ï¼‰simpleconv3(nn.Module) ç»§æ‰¿ nn.Moduleï¼Œå‰é¢å·²ç»è¯´è¿‡ï¼ŒPytorch çš„ç½‘ç»œå±‚æ˜¯åŒ…å«åœ¨ nn.Module é‡Œï¼Œæ‰€ä»¥æ‰€æœ‰çš„ç½‘ç»œå®šä¹‰ï¼Œéƒ½éœ€è¦ç»§æ‰¿è¯¥ç½‘ç»œå±‚ã€‚ å¹¶å®ç° super æ–¹æ³•ï¼Œå¦‚ä¸‹ï¼š super(simpleconv3,self).__init__() è¿™ä¸ªï¼Œå°±å½“ä½œä¸€ä¸ªæ ‡å‡†ï¼Œæ‰§è¡Œå°±å¯ä»¥äº†ã€‚ ï¼ˆ2ï¼‰ç½‘ç»œç»“æ„çš„å®šä¹‰ éƒ½åœ¨ nn åŒ…é‡Œï¼Œä¸¾ä¾‹è¯´æ˜ï¼š torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) å®Œæ•´çš„æ¥å£å¦‚ä¸Šï¼Œå®šä¹‰çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚å¦‚ä¸‹ï¼š nn.Conv2d(3, 12, 3, 2) å³è¾“å…¥é€šé“ä¸º3ï¼Œè¾“å‡ºé€šé“ä¸º12ï¼Œå·ç§¯æ ¸å¤§å°ä¸º3ï¼Œstride=2ï¼Œå…¶ä»–çš„å±‚å°±ä¸ä¸€ä¸€ä»‹ç»äº†ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±å»çœ‹ nn çš„ APIã€‚ ï¼ˆ3ï¼‰forward backward æ–¹æ³•ä¸éœ€è¦è‡ªå·±å®ç°ï¼Œä½†æ˜¯ forward å‡½æ•°æ˜¯å¿…é¡»è¦è‡ªå·±å®ç°çš„ï¼Œä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œforward å‡½æ•°ä¹Ÿæ˜¯éå¸¸ç®€å•ï¼Œä¸²æ¥å„ä¸ªç½‘ç»œå±‚å°±å¯ä»¥äº†ã€‚ å¯¹æ¯” Caffe å’Œ TensorFlow å¯ä»¥çœ‹å‡ºï¼ŒPytorch çš„ç½‘ç»œå®šä¹‰æ›´åŠ ç®€å•ï¼Œåˆå§‹åŒ–æ–¹æ³•éƒ½æ²¡æœ‰æ˜¾ç¤ºå‡ºç°ï¼Œå› ä¸º Pytorch å·²ç»æä¾›äº†é»˜è®¤åˆå§‹åŒ–ã€‚ å¦‚æœæˆ‘ä»¬æƒ³å®ç°è‡ªå·±çš„åˆå§‹åŒ–ï¼Œå¯ä»¥è¿™ä¹ˆåšï¼š init.xavier_uniform(self.conv1.weight)init.constant(self.conv1.bias, 0.1) å®ƒä¼šå¯¹ conv1 çš„æƒé‡å’Œåç½®è¿›è¡Œåˆå§‹åŒ–ã€‚å¦‚æœè¦å¯¹æ‰€æœ‰ conv å±‚ä½¿ç”¨ xavier åˆå§‹åŒ–å‘¢ï¼Ÿå¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼š def weights_init(m): if isinstance(m, nn.Conv2d): xavier(m.weight.data) xavier(m.bias.data) net = Net() net.apply(weights_init) æ¨¡å‹è®­ç»ƒ ç½‘ç»œå®šä¹‰å’Œæ•°æ®åŠ è½½éƒ½å®šä¹‰å¥½ä¹‹åï¼Œå°±å¯ä»¥è¿›è¡Œè®­ç»ƒäº†ï¼Œè€è§„çŸ©å…ˆä¸Šä»£ç ï¼š def train_model(model, criterion, optimizer, scheduler, num_epochs=25): for epoch in range(num_epochs): print('Epoch {}/{}'.format(epoch, num_epochs - 1)) for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) else: model.train(False) running_loss = 0.0 running_corrects = 0.0 for data in dataloders[phase]: inputs, labels = data if use_gpu: inputs = Variable(inputs.cuda()) labels = Variable(labels.cuda()) else: inputs, labels = Variable(inputs), Variable(labels) optimizer.zero_grad() outputs = model(inputs) _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) if phase == 'train': loss.backward() optimizer.step() running_loss += loss.data.item() running_corrects += torch.sum(preds == labels).item() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] if phase == 'train': writer.add_scalar('data/trainloss', epoch_loss, epoch) writer.add_scalar('data/trainacc', epoch_acc, epoch) else: writer.add_scalar('data/valloss', epoch_loss, epoch) writer.add_scalar('data/valacc', epoch_acc, epoch) print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc)) writer.export_scalars_to_json(\"./all_scalars.json\") writer.close() return model åˆ†æä¸€ä¸‹ä¸Šé¢çš„ä»£ç ï¼Œå¤–å±‚å¾ªç¯æ˜¯ epochesï¼Œç„¶ååˆ©ç”¨ for data in dataloders[phase] å¾ªç¯å–ä¸€ä¸ª epoch çš„æ•°æ®ï¼Œå¹¶å¡å…¥ variableï¼Œé€å…¥ modelã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¯ä¸€æ¬¡ forward è¦å°†æ¢¯åº¦æ¸…é›¶ï¼Œå³ optimizer.zero_grad()ï¼Œå› ä¸ºæ¢¯åº¦ä¼šè®°å½•å‰ä¸€æ¬¡çš„çŠ¶æ€ï¼Œç„¶åè®¡ç®— lossï¼Œåå‘ä¼ æ’­ã€‚ loss.backward() optimizer.step() ä¸‹é¢å¯ä»¥åˆ†åˆ«å¾—åˆ°é¢„æµ‹ç»“æœå’Œ lossï¼Œæ¯ä¸€æ¬¡ epoch å®Œæˆè®¡ç®—ã€‚ epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects / dataset_sizes[phase] _, preds = torch.max(outputs.data, 1) loss = criterion(outputs, labels) "},"code_technique/pytorch/Tensor_to_img_imge_to_tensor.html":{"url":"code_technique/pytorch/Tensor_to_img_imge_to_tensor.html","title":"Tensor to img && imge to tensor","keywords":"","body":"Tensor to img && imge to tensor åœ¨pytorchä¸­ç»å¸¸ä¼šé‡åˆ°å›¾åƒæ ¼å¼çš„è½¬åŒ–ï¼Œä¾‹å¦‚å°†PILåº“è¯»å–å‡ºæ¥çš„å›¾ç‰‡è½¬åŒ–ä¸ºTensorï¼Œäº¦æˆ–è€…å°†Tensorè½¬åŒ–ä¸ºnumpyæ ¼å¼çš„å›¾ç‰‡ã€‚è€Œä¸”ä½¿ç”¨ä¸åŒå›¾åƒå¤„ç†åº“è¯»å–å‡ºæ¥çš„å›¾ç‰‡æ ¼å¼ä¹Ÿä¸ç›¸åŒï¼Œå› æ­¤ï¼Œå¦‚ä½•åœ¨pytorchä¸­æ­£ç¡®è½¬åŒ–å„ç§å›¾ç‰‡æ ¼å¼(PILã€numpyã€Tensor)æ˜¯ä¸€ä¸ªåœ¨è°ƒè¯•ä¸­æ¯”è¾ƒé‡è¦çš„é—®é¢˜ã€‚ æœ¬æ–‡ä¸»è¦è¯´æ˜åœ¨pytorchä¸­å¦‚ä½•æ­£ç¡®å°†å›¾ç‰‡æ ¼å¼åœ¨å„ç§å›¾åƒåº“è¯»å–æ ¼å¼ä»¥åŠtensorå‘é‡ä¹‹é—´è½¬åŒ–çš„é—®é¢˜ã€‚ä»¥ä¸‹ä»£ç ç»è¿‡æµ‹è¯•éƒ½å¯ä»¥åœ¨Pytorch-0.4.0æˆ–0.3.0ç‰ˆæœ¬ç›´æ¥ä½¿ç”¨ã€‚ å¯¹pythonä¸åŒçš„å›¾åƒåº“è¯»å–æ ¼å¼æœ‰ç–‘é—®å¯ä»¥çœ‹è¿™é‡Œï¼šhttps://oldpan.me/archives/pytorch-transforms-opencv-scikit-image æ ¼å¼è½¬æ¢ æˆ‘ä»¬ä¸€èˆ¬åœ¨pytorchæˆ–è€…pythonä¸­å¤„ç†çš„å›¾åƒæ— éè¿™å‡ ç§æ ¼å¼ï¼š PILï¼šä½¿ç”¨pythonè‡ªå¸¦å›¾åƒå¤„ç†åº“è¯»å–å‡ºæ¥çš„å›¾ç‰‡æ ¼å¼ numpyï¼šä½¿ç”¨python-opencvåº“è¯»å–å‡ºæ¥çš„å›¾ç‰‡æ ¼å¼ tensorï¼špytorchä¸­è®­ç»ƒæ—¶æ‰€é‡‡å–çš„å‘é‡æ ¼å¼ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥è¯´å›¾ç‰‡ï¼‰ æ³¨æ„ï¼Œä¹‹åçš„è®²è§£å›¾ç‰‡æ ¼å¼çš†ä¸ºRGBä¸‰é€šé“ï¼Œ24-bitçœŸå½©è‰²ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¹³å¸¸ä½¿ç”¨çš„å›¾ç‰‡å½¢å¼ã€‚ PILä¸Tensor PILä¸Tensorçš„è½¬æ¢ç›¸å¯¹å®¹æ˜“äº›ï¼Œå› ä¸ºpytorchå·²ç»æä¾›äº†ç›¸å…³çš„ä»£ç ï¼Œæˆ‘ä»¬åªéœ€è¦æ­é…ä½¿ç”¨å³å¯ï¼š æ‰€æœ‰ä»£ç éƒ½å·²ç»å¼•ç”¨äº†ï¼ˆä¹‹åçš„ä»£ç çœç•¥å¼•ç”¨éƒ¨åˆ†ï¼‰ï¼š import torch from PIL import Image import matplotlib.pyplot as plt # loaderä½¿ç”¨torchvisionä¸­è‡ªå¸¦çš„transformså‡½æ•° loader = transforms.Compose([ transforms.ToTensor()]) unloader = transforms.ToPILImage() 1 PILè¯»å–å›¾ç‰‡è½¬åŒ–ä¸ºTensor # è¾“å…¥å›¾ç‰‡åœ°å€ # è¿”å›tensorå˜é‡ def image_loader(image_name): image = Image.open(image_name).convert('RGB') image = loader(image).unsqueeze(0) return image.to(device, torch.float) 2 å°†PILå›¾ç‰‡è½¬åŒ–ä¸ºTensor # è¾“å…¥PILæ ¼å¼å›¾ç‰‡ # è¿”å›tensorå˜é‡ def PIL_to_tensor(image): image = loader(image).unsqueeze(0) return image.to(device, torch.float) 3 Tensorè½¬åŒ–ä¸ºPILå›¾ç‰‡ # è¾“å…¥tensorå˜é‡ # è¾“å‡ºPILæ ¼å¼å›¾ç‰‡ def tensor_to_PIL(tensor): image = tensor.cpu().clone() image = image.squeeze(0) image = unloader(image) return image 4 ç›´æ¥å±•ç¤ºtensoræ ¼å¼å›¾ç‰‡ def imshow(tensor, title=None): image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) plt.imshow(image) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated 5 ç›´æ¥ä¿å­˜tensoræ ¼å¼å›¾ç‰‡ def save_image(tensor, **para): dir = 'results' image = tensor.cpu().clone() # we clone the tensor to not do changes on it image = image.squeeze(0) # remove the fake batch dimension image = unloader(image) if not osp.exists(dir): os.makedirs(dir) image.save('results_{}/s{}-c{}-l{}-e{}-sl{:4f}-cl{:4f}.jpg' .format(num, para['style_weight'], para['content_weight'], para['lr'], para['epoch'], para['style_loss'], para['content_loss'])) numpyä¸Tensor numpyæ ¼å¼æ˜¯ä½¿ç”¨cv2ï¼Œä¹Ÿå°±æ˜¯python-opencvåº“è¯»å–å‡ºæ¥çš„å›¾ç‰‡æ ¼å¼ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ç”¨python-opencvè¯»å–å‡ºæ¥çš„å›¾ç‰‡å’Œä½¿ç”¨PILè¯»å–å‡ºæ¥çš„å›¾ç‰‡æ•°æ®ç•¥å¾®ä¸åŒï¼Œç»æµ‹è¯•ç”¨python-opencvè¯»å–å‡ºæ¥çš„å›¾ç‰‡åœ¨è®­ç»ƒæ—¶çš„æ•ˆæœæ¯”ä½¿ç”¨PILè¯»å–å‡ºæ¥çš„ç•¥å·®ä¸€äº›(è¯¦ç»†è¿‡ç¨‹ä¹‹åå‘å¸ƒ)ã€‚ ä¹‹åæ‰€æœ‰ä»£ç å¼•ç”¨ï¼š import cv2 import torch import matplotlib.pyplot as plt numpyè½¬åŒ–ä¸ºtensor def toTensor(img): assert type(img) == np.ndarray,'the img type is {}, but ndarry expected'.format(type(img)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = torch.from_numpy(img.transpose((2, 0, 1))) return img.float().div(255).unsqueeze(0) # 255ä¹Ÿå¯ä»¥æ”¹ä¸º256 tensorè½¬åŒ–ä¸ºnumpy def tensor_to_np(tensor): img = tensor.mul(255).byte() img = img.cpu().numpy().squeeze(0).transpose((1, 2, 0)) return img å±•ç¤ºnumpyæ ¼å¼å›¾ç‰‡ def show_from_cv(img, title=None): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.figure() plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) å±•ç¤ºtensoræ ¼å¼å›¾ç‰‡ def show_from_tensor(tensor, title=None): img = tensor.clone() img = tensor_to_np(img) plt.figure() plt.imshow(img) if title is not None: plt.title(title) plt.pause(0.001) æ³¨æ„ ä¸Šé¢ä»‹ç»çš„éƒ½æ˜¯ä¸€å¼ å›¾ç‰‡çš„è½¬åŒ–ï¼Œå¦‚æœæ˜¯nå¼ å›¾ç‰‡ä¸€èµ·çš„è¯ï¼Œåªéœ€è¦ä¿®æ”¹ä¸€ä¸‹ç›¸åº”ä»£ç å³å¯ã€‚ ä¸¾ä¸ªä¾‹å­ï¼Œå°†ä¹‹å‰è¯´è¿‡çš„ä¿®æ”¹ç•¥å¾®ä¿®æ”¹ä¸€ä¸‹å³å¯ï¼š # å°† N x H x W X C çš„numpyæ ¼å¼å›¾ç‰‡è½¬åŒ–ä¸ºç›¸åº”çš„tensoræ ¼å¼ def toTensor(img): img = torch.from_numpy(img.transpose((0, 3, 1, 2))) return img.float().div(255).unsqueeze(0) orchvision transforms æ€»ç»“ 2018å¹´11æœˆ16æ—¥ 17:00:04 Hansry é˜…è¯»æ•°ï¼š2399 ä¸€.torchvision.transforms Transfoms æ˜¯å¾ˆå¸¸ç”¨çš„å›¾ç‰‡å˜æ¢æ–¹å¼ï¼Œå¯ä»¥é€šè¿‡composeå°†å„ä¸ªå˜æ¢ä¸²è”èµ·æ¥ 1. class torchvision.transforms.Compose (transforms) è¿™ä¸ªç±»å°†å¤šä¸ªå˜æ¢æ–¹å¼ç»“åˆåœ¨ä¸€èµ· å‚æ•°ï¼šå„ä¸ªå˜æ¢çš„å®ä¾‹å¯¹è±¡ ä¸¾ä¾‹ï¼š transforms.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ]) 1234 äºŒ. åœ¨PILæ ¼å¼å›¾ç‰‡ä¸Šçš„è½¬æ¢ 1.class torchvision.transforms.CenterCrop(size) å‰ªåˆ‡å¹¶è¿”å›PILå›¾ç‰‡ä¸Šä¸­å¿ƒåŒºåŸŸ å‚æ•°ï¼šsize (åºåˆ—æˆ–è€…æ•´å‹)ã€€â€”ã€€è¾“å‡ºçš„ä¸­å¿ƒåŒºåŸŸçš„å¤§å°ã€‚å¦‚æœè¾“å…¥çš„sizeæ˜¯æ•´å‹è€Œä¸æ˜¯ç±»ä¼¼äº (h,w)çš„åºåˆ—ï¼Œé‚£ä¹ˆå°†ä¼šè½¬æˆç±»ä¼¼(size, size)çš„åºåˆ—ã€‚ 2.class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) éšæœºæ”¹å˜å›¾ç‰‡çš„äº®åº¦ã€å¯¹æ¯”åº¦å’Œé¥±å’Œåº¦ å‚æ•°ï¼š brightness(äº®åº¦ï¼Œfloatç±»å‹)â€”â€”è°ƒæ•´äº®åº¦çš„ç¨‹åº¦ï¼Œäº®åº¦å› å­(brightness_factor)ä» [max(0,1-brightness), 1+brightness] ä¸­å‡åŒ€é€‰å–ã€‚ contrast(å¯¹æ¯”åº¦ï¼Œfloatç±»å‹)â€”â€”è°ƒæ•´å¯¹æ¯”åº¦çš„ç¨‹åº¦ï¼Œå¯¹æ¯”åº¦å› å­(contrast_factor)ä» [max(0,1-contrast),1+contrast] ä¸­å‡åŒ€é€‰å–ã€‚ saturation(é¥±å’Œåº¦ï¼Œfloatç±»å‹)â€”â€”è°ƒæ•´é¥±å’Œåº¦çš„ç¨‹åº¦ï¼Œé¥±å’Œåº¦å› å­(saturation_factor) [max(0,1-saturation),1+saturation] ä¸­å‡åŒ€é€‰å–ã€‚ hue(è‰²ç›¸ï¼Œfloatç±»å‹) â€”â€” è°ƒæ•´è‰²ç›¸çš„ç¨‹åº¦ï¼Œè‰²ç›¸å› å­(hue_factor)ä» [-hue,hue] ç­‰å‡åŒ€é€‰æ‹©, å…¶ä¸­hueçš„å¤§å°ä¸º [0, 0.5]ã€‚ å¯¹æ¯”åº¦ï¼šã€€å¯¹æ¯”åº¦æŒ‡ä¸åŒé¢œè‰²ä¹‹é—´çš„å·®åˆ«ã€‚å¯¹æ¯”åº¦è¶Šå¤§ï¼Œä¸åŒé¢œè‰²ä¹‹é—´çš„åå·®è¶Šå¤§ï¼Œæ‰€è°“é»‘ç™½åˆ†æ˜ï¼Œå¯¹æ¯”åº¦è¿‡å¤§ï¼Œå›¾åƒå°±ä¼šæ˜¾å¾—å¾ˆåˆºçœ¼ã€‚å¯¹æ¯”åº¦è¶Šå°ï¼Œä¸åŒé¢œè‰²ä¹‹é—´çš„åå·®å°±è¶Šå°ã€‚ äº®åº¦ï¼šã€€äº®åº¦æ˜¯æŒ‡ç…§å°„åœ¨æ™¯ç‰©æˆ–è€…å›¾åƒä¸Šå…‰çº¿çš„æ˜æš—ç¨‹åº¦ï¼Œå›¾åƒäº®åº¦å¢åŠ æ—¶ï¼Œä¼šæ˜¾å¾—åˆºçœ¼æˆ–è€€çœ¼ï¼Œäº®åº¦è¶Šå°ï¼Œä¼šæ˜¾å¾—ç°æš—ã€‚ è‰²ç›¸ï¼šã€€è‰²ç›¸å°±æ˜¯é¢œè‰²ï¼Œè°ƒæ•´è‰²ç›¸å°±æ˜¯è°ƒæ•´æ™¯ç‰©çš„é¢œè‰²ã€‚ é¥±å’Œåº¦ï¼šã€€é¥±å’Œåº¦æŒ‡å›¾åƒé¢œè‰²çš„æµ“åº¦ã€‚é¥±å’Œåº¦è¶Šé«˜ï¼Œé¢œè‰²è¶Šé¥±æ»¡ï¼Œæ‰€è°“çš„é’ç¿ æ¬²æ»´çš„æ„Ÿè§‰ã€‚é¥±å’Œåº¦è¶Šä½ï¼Œé¢œè‰²å°±ä¼šè¶Šé™ˆæ—§ï¼Œæƒ¨æ·¡ï¼Œé¥±å’Œåº¦ä¸º0æ—¶ï¼Œå›¾åƒå°±ä¸ºç°åº¦å›¾åƒã€‚ 3. class torchvision.transforms.FiveCrop(size) å°†ç»™å®šçš„PILå›¾åƒå‰ªè£æˆå››ä¸ªè§’è½åŒºåŸŸå’Œä¸­å¿ƒåŒºåŸŸ æ³¨æ„ï¼šã€€è¿™ä¸ªå˜æ¢è¿”å›çš„æ˜¯ä¸€ä¸ªå›¾åƒå…ƒç»„(tuple of images), å› æ­¤å…¶è¾“å‡ºè·Ÿè¾“å‡ºçš„æ•°é‡ä¼šä¸åŒ¹é…ã€‚ å‚æ•°ï¼šã€€size(åºåˆ—æˆ–è€…æ•´å‹) â€”â€” éœ€è¦è¿”å›çš„å‰ªè£åŒºåŸŸçš„å°ºå¯¸ã€‚å¦‚æœè¾“å…¥çš„æ˜¯æ•´å‹ï¼Œé‚£ä¹ˆä¼šè¢«è½¬æˆ(size,size)åºåˆ—ã€‚ ä¾‹å­ï¼š transform = Compose([ FiveCrop(size), Lambda(lambda crops:torch.stack([ToTensor()(crop) for crop in crops])) #return a 4D tensor ]) #in your test loop you can do the following: input ,target = batch #input is a 5d tensor, target is 2dã€€ bs, ncrops,c ,h ,w = input.size() result = model(input.view(-1,c,h,w)) ã€€#fuse batch size and ncrops è½¬æˆ(bs*ncrops, c, h , w) result_avg = result.view(bs, ncrops, -1).mean(1) #avg over crops è½¬æˆ(bsï¼Œncrops, c*h*w) 123456789 ï¼”. class torchvision.transforms.ï¼§rayscale(num_output_channels=1) å°†å›¾ç‰‡è½¬æˆç°åº¦å›¾ å‚æ•°ï¼šã€€num_output_channels(int) â€”â€”ã€€(1æˆ–è€…3)ï¼Œè¾“å‡ºå›¾ç‰‡çš„é€šé“æ•°é‡ è¿”å›ï¼šã€€è¾“å…¥å›¾ç‰‡çš„ç°åº¦å›¾ï¼Œå¦‚æœnum_output_channels=1, è¿”å›çš„å›¾ç‰‡ä¸ºå•é€šé“. å¦‚æœ num_output_channels=3, è¿”å›çš„å›¾ç‰‡ä¸º3é€šé“å›¾ç‰‡ï¼Œä¸”r=g=b è¿”å›ç±»å‹ï¼šPILå›¾ç‰‡ç±»å‹ 5.ã€€class torchvision.transforms.Pad(padding, fill=0, padding_mode=â€˜constantâ€™) å¯¹ç»™å®šçš„PILå›¾åƒçš„è¾¹ç¼˜è¿›è¡Œå¡«å……ï¼Œå¡«å……çš„æ•°å€¼ä¸ºç»™å®šå¡«å……æ•°å€¼ å‚æ•°ï¼š padding(intæˆ–è€…tuple)â€”â€”å¡«å……æ¯ä¸€ä¸ªè¾¹ç•Œã€‚å¦‚æœåªè¾“å…¥äº†ä¸€ä¸ªintç±»å‹çš„æ•°å€¼ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°å€¼ä¼šè¢«ç”¨æ¥å¡«å……æ‰€æœ‰çš„è¾¹ç•Œã€‚å¦‚æœè¾“å…¥çš„æ˜¯tupleä¸”é•¿åº¦ä¸º2ï¼Œé‚£ä¹ˆä¿©ä¸ªæ•°å€¼åˆ†åˆ«è¢«ç”¨äºå¡«å……left/right å’Œ top/bottomã€‚å¦‚æœè¾“å…¥çš„æ•°ç»„ä¸º4ï¼Œé‚£ä¹ˆåˆ†åˆ«è¢«ç”¨æ¥å¡«å……left, top ,right å’Œ bottomè¾¹ç•Œã€‚ fill (int æˆ–è€… tuple) â€”â€”ã€€å¡«å……çš„åƒç´ çš„æ•°å€¼ä¸ºfillã€‚é»˜è®¤ä¸º0ï¼Œå¦‚æœè¾“å…¥çš„å…ƒç»„çš„é•¿åº¦ä¸º3ï¼Œé‚£ä¹ˆåˆ†åˆ«è¢«ç”¨æ¥å¡«å……R,G,Bé€šé“ã€‚è¿™ä¸ªæ•°å€¼å½“padding_mode ç­‰äºâ€˜constantâ€™ã€€çš„æ—¶å€™æ‰ä¼šè¢«ä½¿ç”¨ã€‚ padding_mode (string) â€”â€”ã€€å¡«å……çš„ç±»å‹ï¼Œå¿…é¡»ä¸ºï¼šconstant, edge, reflect or symmetricï¼Œé»˜è®¤ä¸º constant. constant: ä»¥å¸¸é‡å€¼è¿›è¡Œå¡«å……ï¼Œå¸¸é‡å€¼ç”± fill ç¡®å®šã€‚ edge: ç”¨å›¾ç‰‡è¾¹ç•Œæœ€åä¸€ä¸ªå€¼è¿›è¡Œå¡«å…… reflect: pads with reflection of image without repeating the last value on the edge (è¿™å¥ä¸çŸ¥æ€ä¹ˆç¿»è¯‘ï¼Œçœ‹ä¸‹é¢ä¾‹å­) ä¾‹å­: ç”¨ä¿©ä¸ªå…ƒç´ å¡«å……[1,2,3,4], å°†ä¼šè¿”å›[3,2,1,2,3,4,3,2] symmetric: pads with reflection of image repeating the last value on the edge ä¾‹å­ï¼šç”¨ä¿©ä¸ªå‡½æ•°å…ƒç´ å¡«å…… [1,2,3,4]ï¼Œå°†ä¼šè¿”å›[2,1,1,2,3,4,4,3] 6. class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None) ä¿æŒä¸­å¿ƒä¸å˜çš„å¯¹å›¾ç‰‡è¿›è¡Œéšæœºä»¿å°„å˜åŒ– å‚æ•°ï¼šæ·»åŠ é“¾æ¥æè¿° degree (æ—‹è½¬ï¼Œsquenceæˆ–è€…floatæˆ–è€…int) â€”â€”ã€€æ—‹è½¬çš„è§’åº¦èŒƒå›´ã€‚å¦‚æœè§’åº¦æ˜¯æ•°å€¼è€Œä¸æ˜¯ç±»ä¼¼äº(min,max)çš„åºåˆ—ï¼Œé‚£ä¹ˆå°†ä¼šè½¬æ¢æˆ(-degree, +degree)åºåˆ—ã€‚è®¾ä¸º0åˆ™å–æ¶ˆæ—‹è½¬ã€‚ transalate (å¹³ç§»ï¼Œtupleï¼Œå¯é€‰)ã€€â€”â€”ã€€æ•°ç»„ï¼Œå…¶ä¸­å…ƒç´ ä¸ºä»£è¡¨æ°´å¹³å’Œå‚ç›´å˜æ¢çš„æœ€å¤§ç»å¯¹åˆ†æ•°ã€‚ä¾‹å¦‚translate=(a,b),é‚£ä¹ˆæ°´å¹³ä½ç§»æ•°å€¼ä¸ºä»ã€€-image_widthaã€€éšæœºé‡‡æ ·çš„ï¼ŒåŒæ—¶å‚ç›´ä½ç§»æ˜¯ä»ã€€-img_heightb éšæœºé‡‡æ ·çš„ã€‚é»˜è®¤æƒ…å†µä¸‹æ²¡æœ‰å¹³ç§»ã€‚ scale (ç¼©æ”¾ï¼Œtuple, å¯é€‰)ã€€â€”â€”ã€€ç¼©æ”¾å› å­åŒºé—´ã€‚è‹¥scale=(a,b), åˆ™ç¼©æ”¾çš„å€¼åœ¨a éšæœºé‡‡æ ·ã€‚é»˜è®¤æƒ…å†µä¸‹æ²¡æœ‰ç¼©æ”¾ã€‚ shear (é”™åˆ‡ï¼Œsequence æˆ–è€… float æˆ–è€… int, å¯é€‰)ã€€â€”â€”ã€€é”™åˆ‡çš„ç¨‹åº¦ã€‚å¦‚æœé”™åˆ‡çš„ç¨‹åº¦æ˜¯ä¸€ä¸ªå€¼ï¼Œé‚£ä¹ˆå°†ä¼šè½¬æ¢ä¸ºåºåˆ—å³(â€”degree, +degree)ã€‚é»˜è®¤æƒ…å†µä¸‹ä¸ä½¿ç”¨é”™åˆ‡ã€‚ resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, å¯é€‰)ã€‚ fillcolor(æ•´å‹) â€”â€” å¯é€‰æ‹©çš„åœ¨è¾“å‡ºå›¾ç‰‡ä¸­å¡«å……å˜æ¢ä»¥å¤–çš„åŒºåŸŸã€‚(Pillow>=5.0.0) 7.torchvision.transforms.RandomApply(transforms, p=0.5) éšæœºé€‰å–å˜æ¢ä¸­(å„ç§å˜æ¢å­˜å‚¨åœ¨åˆ—è¡¨ä¸­)çš„å…¶ä¸­ä¸€ä¸ªï¼ŒåŒæ—¶ç»™å®šä¸€å®šçš„æ¦‚ç‡ å‚æ•°ï¼šã€€ å˜æ¢ï¼ˆlistæˆ–è€…tupleï¼‰ â€”â€”ã€€è½¬æ¢çš„åˆ—è¡¨ p (float ç±»å‹) â€”â€” æ¦‚ç‡,é€‰å–æŸä¸ªå˜åŒ–éœ€è¦çš„æ¦‚ç‡ 8.transforms.RandomSizedCrop() RandomApply() RandomChoice() RadomCrop RamdomGrayscale() RamdomHorizontalFlip(p=0.5) RamdomRotation() â€¦ è¿˜æœ‰å„ç§Randomï¼Œè¯¦ç»†è¯·æŸ¥çœ‹torch.transforms 9.torchvision.transforms.Resize(size,interpolation=2) å°†è¾“å…¥çš„PILå›¾ç‰‡è½¬æ¢æˆç»™å®šçš„å°ºå¯¸çš„å¤§å° å‚æ•°ï¼š size(sequence æˆ–è€… int)ã€€â€”â€”ã€€éœ€è¦è¾“å‡ºçš„å›¾ç‰‡çš„å¤§å°ã€‚å¦‚æœsizeæ˜¯ç±»ä¼¼äº(h,w)çš„åºåˆ—ï¼Œè¾“å‡ºçš„å°ºå¯¸å°†ä¼šè·Ÿ(h,w)ä¸€è‡´ã€‚å¦‚æœsizeæ˜¯æ•´å‹ï¼Œå›¾ç‰‡è¾ƒå°çš„è¾¹ç•Œå°†ä¼šè¢«ç½®ä¸ºè¿™ä¸ªå°ºå¯¸ã€‚ä¾‹å¦‚ï¼Œå¦‚æœheight->width, å›¾ç‰‡å°†ä¼šè¢«ç½®ä¸º (size*height/width, size) Interpolation (int, å¯é€‰) â€”â€” é»˜è®¤ä¸º PIL.Image.BILINEAR ä¸‰. åœ¨torch.*Tensorä¸Šçš„è½¬æ¢ 1. class torchvision.transforms.Normalize(mean,std) ç”¨å‡å€¼å’Œæ ‡å‡†å·®å¯¹å¼ é‡å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚ç»™å®šné€šé“çš„å‡å€¼(M1, â€¦ , Mn) å’Œæ ‡å‡†å·®(S1, â€¦ ,Sn), è¿™ä¸ªå˜åŒ–å°†ä¼šå½’ä¸€åŒ–æ ¹æ®å‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–æ¯ä¸ªé€šé“å€¼ã€‚ä¾‹å¦‚ï¼Œinput[channel] = (input[channel]-mean[channel])/std(channel) å‚æ•°ï¼š mean (squence) â€”â€”ã€€æ¯ä¸ªé€šé“çš„å‡å€¼ std (sequence) â€”â€” æ¯ä¸ªé€šé“çš„æ ‡å‡†å·® __call__(tensor) å‚æ•°ï¼štensor(Tensor) , å°ºå¯¸ä¸º(C,H,W)çš„å›¾ç‰‡å°†ä¼šè¢«å½’ä¸€åŒ– ; è¿”å›ï¼šå½’ä¸€åŒ–åçš„Tensorç±»å‹å›¾ç‰‡ ; è¿”å›ç±»å‹ï¼šï¼´ensor å››. ç±»å‹è½¬æ¢å˜æ¢ (Conversion Transforms) 1. class torchvision.transforms.ToPILImage(mode=None) å°†tensorç±»å‹æˆ–è€…ndarrayè½¬æ¢æˆPILå›¾ç‰‡ å°† CxHxWå¤§å°çš„torch.*Tensoræˆ–è€…ï¼¨xWxC å¤§å°çš„numpy çŸ©é˜µè½¬æˆPILå›¾ç‰‡ å‚æ•°ï¼šå¦‚æœmodelä¸ºï¼®one,é‚£ä¹ˆå¦‚æœè¾“å…¥æœ‰ä¸‰ä¸ªé€šé“ï¼Œé‚£ä¹ˆmodeä¸ºRGB; å¦‚æœinputæœ‰4ä¸ªé€šé“ï¼Œmodeä¸ºRGBA. å¦‚æœè¾“å…¥æ˜¯1é€šé“ï¼Œmodeä¸ºæ•°æ®ç±»å‹ï¼Œå¦‚int, float, short __call__(pic) å‚æ•°ï¼špic (Tensoræˆ–è€…numpy.ndarrayç±»å‹çš„)ã€€â€”â€”ã€€è½¬æ¢æˆPILå›¾ç‰‡; è¿”å›PILå›¾ç‰‡; è¿”å›ç±»å‹ä¸ºPILç±»å‹ 1 2. torchvision.transforms.ToTensor å°†PILå›¾ç‰‡æˆ–è€…numpy.ndarrayè½¬æˆTensorç±»å‹çš„ å°†PILå›¾ç‰‡æˆ–è€…numpy.ndarray(HxWxC) (èŒƒå›´åœ¨0-255) è½¬æˆtorch.FloatTensor (CxHxW) (èŒƒå›´ä¸º0.0-1.0) __call__(pic) å‚æ•°ï¼špic(PILå›¾ç‰‡æˆ–è€…numpy.ndarray) â€”â€” å°†å›¾ç‰‡è½¬æˆå‘é‡; è¿”å›Tensorç±»å‹çš„å›¾ç‰‡ 1 äº”. ä¸€èˆ¬å˜æ¢ (Generic Transforms) 1. torchvision.transforms.Lambda(lambd) ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„lambdaä½œä¸ºè½¬æ¢ å‚æ•°ï¼šlambd(function) â€”â€”ã€€ç”¨Lambda/funtion ä½œä¸ºå˜æ¢ 2. torchvision.transforms.functional.adjust_brightness(img, brightness_factor) è°ƒæ•´å›¾ç‰‡çš„äº®åº¦ å‚æ•°ï¼š img(PIL å›¾ç‰‡)â€”â€”PILå›¾ç‰‡ brightness_factor(float)â€”â€”äº®åº¦è°ƒæ•´ç¨‹åº¦ã€‚ä¸èƒ½ä¸ºè´Ÿæ•°, ï¼ä»£è¡¨é»‘è‰²å›¾ç‰‡ï¼Œ1ä»£è¡¨åŸå§‹å›¾ç‰‡ï¼Œ2ä»£è¡¨å¢åŠ äº†2ä¸ªå› å­çš„äº®åº¦ã€‚ returns: è¿”å›è°ƒæ•´å®Œçš„å›¾ç‰‡ 3.torchvision.transforms.functional.adjust_contrast(img,contrast_factor) è°ƒæ•´å›¾ç‰‡çš„å¯¹æ¯”åº¦ å‚æ•°ï¼š img â€”â€” éœ€è¦è°ƒæ•´çš„PIL å›¾ç‰‡ constrast_factor(float) â€”â€” è°ƒæ•´å¯¹æ¯”åº¦çš„ç¨‹åº¦ã€‚å¯ä»¥æ˜¯éè´Ÿçš„æ•°ã€‚0ä¸ºç°åº¦å›¾ï¼Œ1ä¸ºåŸå›¾ï¼Œ2ä¸ºå¢åŠ å›¾ç‰‡2ä¸ªå¯¹æ¯”å› å­çš„å›¾ç‰‡ã€‚ returnsã€€â€”â€”ã€€è¿”å›è°ƒæ•´åçš„å¯¹æ¯”åº¦å›¾ç‰‡ 4. torchvison.transforms.function.adjust_gamma(img, gamma, gain=1) å¯¹å›¾ç‰‡è¿›è¡Œgammaæ ¡æ­£ï¼Œgammaæ ¡æ­£è¯¦æƒ… Iout=255âˆ—gainâˆ—(Iin/255)Î³I{out}=255gain(I{in}/255)^{\\gamma}Iou**tâ€‹=255âˆ—gai**nâˆ—(Iinâ€‹/255)Î³ å‚æ•°ï¼š img(PILå›¾ç‰‡)â€”â€”éœ€è¦è°ƒæ•´çš„PILå›¾ç‰‡ gamma (floatç±»å‹)â€”â€”éé›¶å®æ•°ï¼Œå…¬å¼ä¸­çš„Î³\\gammaÎ³ä¹Ÿæ˜¯éé›¶å®æ•°ã€‚gammaå¤§äº1ä½¿å¾—é˜´å½±éƒ¨åˆ†æ›´æš—ï¼Œgammaå°äº1ä½¿å¾—æš—çš„åŒºåŸŸäº®äº›ã€‚ gain(float) â€”â€”ã€€å¸¸é‡ä¹˜æ•° 5.torchvision.transforms.functional.ajust_hue (img,hue_factor) è°ƒæ•´å›¾ç‰‡çš„è‰²ç›¸ é€šè¿‡å°†å›¾åƒè½¬æ¢ä¸ºHSVæ¥è°ƒæ•´å›¾åƒçš„è‰²è°ƒï¼Œå¹¶åœ¨è‰²è°ƒé€šé“(H)ä¸­å¾ªç¯ç§»åŠ¨å¼ºåº¦ï¼Œç„¶åå°†å›¾åƒè½¬æ¢å›åŸå§‹å›¾åƒæ¨¡å¼ã€‚ è‰²ç›¸å› å­æ˜¯ï¼¨é€šé“å¹³ç§»é‡ï¼Œå…¶å¿…é¡»åœ¨åŒºé—´[-0.5,0.5]ä¸­ã€‚ å‚æ•° img (PIL å›¾ç‰‡)ã€€â€”â€”ã€€éœ€è¦è°ƒæ•´çš„PILå›¾ç‰‡ hue_factor (floatç±»å‹) â€”â€” è‰²ç›¸é€šé“å¹³ç§»çš„é‡ï¼Œå¿…é¡»åœ¨[-0.5,0.5]ä¹‹é—´ã€‚0.5å’Œ-0.5åˆ†åˆ«ä»£è¡¨åœ¨HSVç©ºé—´ä¸­æ­£è´Ÿæ–¹å‘å®Œå…¨ç›¸åçš„è‰²ç›¸é€šé“ã€‚0ä»£è¡¨æ²¡æœ‰å¹³ç§»ã€‚ 5. tochvision.transforms.functional.adjust_saturation(img, hue_factor) è°ƒæ•´å›¾ç‰‡çš„é¢œè‰²é¥±å’Œåº¦ å‚æ•°ï¼š img (PILå›¾ç‰‡)â€”â€”éœ€è¦è°ƒæ•´çš„PILå›¾ç‰‡ é¥±å’Œåº¦å› å­(floatç±»å‹)â€”â€”è°ƒæ•´é¥±å’Œåº¦çš„ç¨‹åº¦ã€‚0å°†ä¼šè¾“å‡ºé»‘ç™½å›¾ç‰‡ï¼Œ1å°†ä¼šè¾“å‡ºåŸå§‹å›¾ç‰‡ï¼Œ2å°†ä¼šå¢å¼º2ä¸ªå› å­çš„é¥±å’Œåº¦ã€‚ è¿”å›è°ƒæ•´åçš„å›¾ç‰‡ã€‚ 6. torchvision.transforms.functional.affine(img, angle, translate, scale, shear, resample=0, fillcolor=None) å¯¹å›¾ç‰‡è¿›è¡Œæ”¾å°„å˜æ¢ï¼Œä¿æŒä¸­å¿ƒä¸å˜ã€‚ å‚æ•°ï¼š img (PILå›¾ç‰‡)â€”â€”éœ€è¦å˜æ¢çš„PILå›¾ç‰‡ angle(float æˆ–è€… int)â€”â€”æ—‹è½¬çš„çš„è§’åº¦ï¼Œè§’åº¦èŒƒå›´ä¸º (-180,180), æ­£æ–¹å‘ä¸ºé¡ºæ—¶é’ˆæ–¹å‘ã€‚ translate(list æˆ–è€… tuple)â€”â€”æ°´å¹³æˆ–è€…å‚ç›´å¹³ç§» scale(float)â€”â€”æ€»ä½“ç¼©æ”¾ shear(é”™åˆ‡ï¼Œfloat)â€”â€”é”™åˆ‡çš„è§’åº¦ä½äº(-180,180)ï¼Œé¡ºæ—¶é’ˆæ–¹å‘ã€‚ resampleï¼ˆè¿™ä¸ªæœ‰ç‚¹çœ‹ä¸æ‡‚ï¼Œåº”è¯¥æ¯”è¾ƒå°‘ç”¨åˆ°â€”â€”PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional fillcolor (int) â€”â€” å¡«å……è¾“å‡ºå›¾ç‰‡ä¸­è¶…è¿‡å˜æ¢çš„åŒºåŸŸ(Pillow>=5.0) 7.torchvision.transforms.functional.crop(img,i,j,h,w) å‰ªè£ç»™å®šçš„PILå›¾ç‰‡ å‚æ•°ï¼š img(PILå›¾ç‰‡)â€”â€”è¢«å‰ªè£çš„å›¾ç‰‡ ï¼ˆi, j) â€”â€”å·¦ä¸Šè§’å›¾ç‰‡åæ ‡ (h,w)â€”â€”å‰ªè£çš„å›¾ç‰‡çš„é«˜å’Œå®½ returns: è¿”å›å‰ªå½©çš„å›¾ç‰‡ 8. torchvision.transforms.functional.normalize(tensor, mean, std) æ ¹æ®ç»™å®šçš„æ ‡å‡†å·®å’Œæ–¹å·®å½’ä¸€åŒ–tensorå›¾ç‰‡ å‚æ•°ï¼š tensor(Tensor)â€”â€” å½¢çŠ¶ä¸º(C,H,W)çš„Tensorå›¾ç‰‡ mean(squence) â€”â€” æ¯ä¸ªé€šé“çš„å‡å€¼ï¼Œåºåˆ— std (sequence) â€”â€” æ¯ä¸ªé€šé“çš„æ ‡å‡†å·®ï¼Œåºåˆ— è¿”å›ï¼šè¿”å›å½’ä¸€åŒ–åçš„Tensorå›¾ç‰‡ã€‚ 9.torchvision.transforms.functional.pad(img, padding, fill=0, padding_mode=â€˜constantâ€™)ã€torchvision.transforms.functional.resize(img, size, interpolation=2)ã€torchvision.transforms.functional.rotate(img, angle, resample=False, expand=False, center=None)ã€torchvision.transforms.functional.to_grayscale(img, num_output_channelsï¼ï¼‘) ç­‰å‡ä¸ä¸Šè¿°å‡½æ•°ç±»ä¼¼ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚ 10.torchvision.transforms.functional.to_pil_image(pic, mode=None) å°†tensoræˆ–è€…numpy.ndarrayè½¬æˆPILå›¾ç‰‡torchvision.transforms.functional.to_tensor(pic) å°†PILå›¾ç‰‡æˆ–è€…numpy.ndarrayè½¬æˆtensor "},"linux/":{"url":"linux/","title":"Linux","keywords":"","body":" LinuxæŠ€å·§ Linuxæ˜¾å¡é©±åŠ¨ä¿®å¤ "},"linux/linux_technique.html":{"url":"linux/linux_technique.html","title":"LinuxæŠ€å·§","keywords":"","body":"ä½œè€…ï¼šç¨‹åºå‘˜å®¢æ ˆ é“¾æ¥ï¼šhttps://www.zhihu.com/question/41115077/answer/602854935 æ¥æºï¼šçŸ¥ä¹ è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ æ¨èå‡ ä¸ªå ªç§°ç¥å™¨çš„å‘½ä»¤è¡Œè½¯ä»¶ï¼Œè®°å¾—çœ‹å®Œå“ˆï¼Œè¶Šåˆ°åé¢è¶Šç²¾å½©ï¼ WordGrinderï¼šå®ƒæ˜¯ä¸€æ¬¾ä½¿ç”¨èµ·æ¥å¾ˆç®€å•ï¼Œä½†æ‹¥æœ‰è¶³å¤Ÿçš„ç¼–å†™å’Œå‘å¸ƒåŠŸèƒ½çš„æ–‡å­—ç¼–è¾‘å™¨ã€‚å®ƒæ”¯æŒåŸºæœ¬çš„æ ¼å¼å’Œæ ·å¼ï¼Œå¹¶ä¸”ä½ å¯ä»¥å°†ä½ çš„æ–‡å­—ä»¥ Markdownã€ODTã€LaTeX æˆ–è€… HTML ç­‰æ ¼å¼å¯¼å‡ºï¼› \\2. Proselintï¼šå®ƒæ˜¯ä¸€æ¬¾å…¨èƒ½çš„å®æ—¶æ£€æŸ¥å·¥å…·ã€‚å®ƒä¼šæ‰¾å‡ºè¡Œè¯ã€å¤§è¯ã€ä¸æ­£ç¡®æ—¥æœŸå’Œæ—¶é—´æ ¼å¼ã€æ»¥ç”¨çš„æœ¯è¯­ç­‰ç­‰ã€‚å®ƒä¹Ÿå¾ˆå®¹æ˜“è¿è¡Œå¹¶å¿½ç•¥æ–‡æœ¬ä¸­çš„æ ‡è®°ï¼› \\3. GNU Aspellï¼šå®ƒèƒ½å¤Ÿäº¤äº’å¼åœ°æ£€æµ‹æ–‡æœ¬æ–‡æ¡£ï¼Œèƒ½é«˜äº®æ˜¾ç¤ºæ‹¼å†™é”™è¯¯ï¼Œè¿˜èƒ½åœ¨æ‹¼å†™é”™è¯¯çš„ä¸Šæ–¹æä¾›æ­£ç¡®çš„æ‹¼å†™å»ºè®®ã€‚Aspell åœ¨è¿›è¡Œæ‹¼å†™æ£€æŸ¥çš„æ—¶å€™ï¼ŒåŒæ ·èƒ½å¤Ÿå¿½ç•¥è®¸å¤šè¯­æ³•æ ‡è®°ï¼› \\4. tldrï¼šä½ èƒ½é€šè¿‡è¿™ä¸ªå·¥å…·ï¼Œå¿«é€ŸæŸ¥çœ‹æŸ¥çœ‹å„ç§å‘½ä»¤çš„å¸¸ç”¨å‘½ä»¤è¡Œä¾‹å­ï¼š \\5. Alexï¼šå®ƒæ˜¯ä¸€ä¸ªç®€å•ä½†å¾ˆæœ‰ç”¨çš„å°å·¥å…·ã€‚é€‚ç”¨äºæ˜æ–‡æ–‡æœ¬æˆ–è€…æ ¼å¼ä¸º Markdown æˆ– HTML çš„æ–‡æ¡£ã€‚Alex ä¼šå¯¹â€œæ€§åˆ«åå¥½ã€æç«¯ä¸»ä¹‰ã€ç§æ—ç›¸å…³ã€å®—æ•™ï¼Œæˆ–è€…æ–‡ç« ä¸­å…¶ä»–ä¸å¹³ç­‰çš„æªè¾â€äº§ç”Ÿè­¦å‘Šã€‚å¦‚æœä½ æƒ³è¦è¯•è¯•çœ‹ Alexï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªåœ¨çº¿ demoï¼› \\6. nmonï¼šå®ƒèƒ½å¤Ÿå¸®ä½ è¿›è¡Œç”µè„‘çš„æ€§èƒ½ç›‘æ§ï¼ŒåŒ…æ‹¬ CPUï¼Œå†…å­˜ï¼Œç£ç›˜ IOï¼Œç½‘ç»œ IOï¼Œå¹¶ä¸”ç•Œé¢å¾ˆç‚«é…·ï¼Œæ˜¯ä¸æ˜¯å¾ˆåƒé»‘å®¢ï¼Œå¿«å»è¯•è¯•å§ nmon for Linux | Main \\7. axelï¼šå¤šçº¿ç¨‹æ–­ç‚¹ä¸‹è½½å·¥å…·ï¼Œéå¸¸å¥½ç”¨ã€‚ä¾‹å¦‚ä¸‹å›¾ä¸­è¿™æ ·ï¼ŒæŒ‡å®šäº† 8 ä¸ªçº¿ç¨‹åŒæ—¶ä¸‹è½½ã€‚ \\8. SpaceVimï¼šè¿™æ˜¯ä¸€ä¸ª vim æ’ä»¶ï¼Œä½¿ä½ çš„ Vim å˜æˆå¸¦ä»£ç è‡ªåŠ¨è¡¥å…¨ç­‰åŠŸèƒ½çš„æ›´åŠ å¼ºå¤§çš„ä»£ç ç¼–è¾‘å™¨ï¼ \\9. thefuckï¼šä½  git branch æ‰“æˆ branch äº†ï¼Œç„¶åå‘½ä»¤è¡ŒæŠ¥é”™ï¼Œä½ æ˜¯ä¸æ˜¯å¿ƒé‡Œä¼šå†’å‡ºä¸€å¥ fuckï¼Ÿé‚£ä½ å°±åœ¨å‘½ä»¤è¡Œé‡Œè¾“å…¥ fuck ç„¶åå›è½¦ï¼å’¦ï¼ŒæˆåŠŸäº†ï¼ apt-get update æ‰“æˆ aptget update æŠ¥é”™ï¼Ÿè¾“å…¥ fuck ç„¶åå›è½¦ï¼å°±è§£å†³äº†ï¼çˆ½å§å“ˆå“ˆå“ˆ "},"linux/linux_GPU.html":{"url":"linux/linux_GPU.html","title":"Linuxæ˜¾å¡é©±åŠ¨ä¿®å¤","keywords":"","body":"nvidia-smiæŠ¥é”™ï¼ˆé‡è£…Nvidiaé©±åŠ¨ï¼‰ é‡åˆ°ä¸€ä¸ªè«åå…¶å¦™çš„é—®é¢˜ï¼š NVIDIA-SMI has failed because it couldnâ€™t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. è§£å†³æ–¹æ¡ˆï¼šé‡è£…NVIDIAé©±åŠ¨ï¼ˆécudaï¼‰ é¦–å…ˆåœ¨å®˜ç½‘ä¸‹è½½ä½ è‡ªå·±æ˜¾å¡å¯¹åº”çš„é©±åŠ¨NVIDIA-Linux-x86_64-xxx.xx.runï¼Œæ‹·è´åˆ°LinuxæŸä¸ªç›®å½•åå…ˆæ”¹æƒé™ chomod 777 NVIDIA-Linux-x86_64-xxx.xx.run 1 å¸è½½åŸé©±åŠ¨ sudo apt-get remove --purge nvidia* # æç¤ºæœ‰æ®‹ç•™å¯ä»¥æ¥ sudo apt autoremove 1 ä¸´æ—¶å…³é—­æ˜¾ç¤ºæœåŠ¡ sudo service lightdm stop 1 è¿è¡Œå®‰è£…ç¨‹åº sudo ./NVIDIA-Linux-x86_64-375.66.run å®‰è£…åå†é‡å¯æ˜¾ç¤º sudo service lightdm start "},"paper/":{"url":"paper/","title":"è®ºæ–‡","keywords":"","body":" è®ºæ–‡å†™ä½œ Image-to-Image çš„è®ºæ–‡æ±‡æ€»ï¼ˆå« GitHub ä»£ç ï¼‰ GNNç»¼è¿° Perceptual GAN for Small Object Detectioné˜…è¯»ç¬”è®° GANå˜ä½“-GMMN ç½‘ç»œ å›¾åƒè§†é¢‘å»å™ªä¸­çš„Deformable Kernels Isolating Sources of Disentanglement in VAEs Spectral Normalization è°±å½’ä¸€åŒ– ä¸å‡è¡¡æ ·æœ¬loss è®ºæ–‡ç¥ç»ç½‘ç»œç¤ºæ„å›¾ "},"paper/paper_write.html":{"url":"paper/paper_write.html","title":"è®ºæ–‡å†™ä½œ","keywords":"","body":"ç§‘å­¦å†™ä½œçš„æŠ€å·§â€”â€”ä»¥æœºå™¨å­¦ä¹ ä¸ºä¾‹ 2018 å¹´ 2 æœˆ 2 æ—¥ è®ºæ™º Zachary Lipton ä½œè€…ï¼šZachary Lipton ç¼–è¯‘ï¼šBing ç¼–è€…æŒ‰ï¼šæœ¬æ–‡åŸä½œè€…Zachary Chase Liptonï¼ˆhttp://zacklipton.com/ï¼‰æ›¾æ˜¯ç¾å›½åŠ å·å¤§å­¦åœ£è¿­æˆˆåˆ†æ ¡è®¡ç®—æœºç§‘å­¦å·¥ç¨‹ç³»çš„åšå£«ç”Ÿï¼Œç°åœ¨CMU Tepperå•†å­¦é™¢æ‹…ä»»åŠ©ç†æ•™æˆï¼ŒåŒæ—¶è¿˜åœ¨æœºå™¨å­¦ä¹ éƒ¨é—¨ä»»æ•™ï¼Œä¸“æ³¨äºæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç†è®ºå’Œå®è·µç ”ç©¶ã€‚æœ¬æ–‡ç¿»è¯‘è‡ªä»–çš„åšæ–‡Heuristics for Scientific Writing (a Machine Learning Perspective)ï¼Œä¸ºæˆ‘ä»¬ä»‹ç»äº†æœºå™¨å­¦ä¹ è®ºæ–‡å†™ä½œæ—¶çš„æ³¨æ„äº‹é¡¹ã€‚ åŸæ–‡åœ°å€ï¼šhttp://approximatelycorrect.com/2018/01/29/heuristics-technical-scientific-writing-machine-learning-perspective/ æ˜¥èŠ‚å°†è‡³ï¼Œå¤§å®¶è¿˜èƒ½é™ä¸‹å¿ƒæ¥å†™è®ºæ–‡å—ï¼Ÿéšç€æ–°å¹´çš„å¼€å¯ï¼Œå„å¤§é¡¶ä¼šçš„è®ºæ–‡æäº¤æˆªæ­¢æ—¥æœŸä¹Ÿå³å°†åˆ°æ¥ï¼šICMLæˆªç¨¿æ—¥æœŸä¸º2æœˆ9æ—¥ï¼ŒKDDæˆªæ­¢æ—¥æœŸä¸º2æœˆ11æ—¥ï¼Œåœ¨è¿™ä¹‹åè¿˜æœ‰ACLã€COLTã€ECMLã€UAIä»¥åŠNIPSâ€¦â€¦æ¯åœºå¤§ä¼šéƒ½ä¼šæ”¶åˆ°æ•°åƒä»½è®ºæ–‡ï¼Œ éšç€å¼€æºè½¯ä»¶ã€ç½‘ç»œè¯¾ç¨‹ä»¥åŠé¢„å°ç‰ˆæ–‡ç« çš„æ™®åŠï¼Œè¶Šæ¥è¶Šå¤šçš„äººå¼€å§‹å¯¹æœºå™¨å­¦ä¹ æ„Ÿå…´è¶£ï¼Œè™½ç„¶æˆæœä¸æ–­ä¸°å¯Œï¼Œä½†ä¸€ä¸ªæ— æ³•é¿å…çš„äº‹å®æ˜¯ï¼Œå¾ˆå¤šè®ºæ–‡ç”±äºä¹¦å†™çš„æ ¼å¼æˆ–æŠ€å·§çš„ä¸æ°å½“ï¼Œå¯¼è‡´å¯è¯»æ€§ä¸å¼ºï¼Œæœ€ç»ˆæœ‰å¯èƒ½å½±å“è¯„é€‰ç»“æœï¼Œç”šè‡³è¢«æ‹’ã€‚å³ä½¿åœ¨ä¸€äº›å…¬è®¤çš„æœ‰å½±å“åŠ›çš„è®ºæ–‡ä¸­ï¼Œç²—å¿ƒå¤§æ„çš„å†™ä½œä¹Ÿä¼šè¿·æƒ‘è¯»è€…ï¼Œç”šè‡³ä¼šè¢«è¯¯ä»¥ä¸ºæ˜¯ä¸ºäº†è’™éª—æŸäº›å¥–å­¦é‡‘è€Œç³Šå¼„çš„è®ºæ–‡ã€‚ ä½†æ˜¯ï¼Œåœ¨æˆ‘çš„å­¦æœ¯ç”Ÿæ¶¯ä¸­ï¼Œæˆ‘å¯¹è®ºæ–‡å†™ä½œå·²ç»æ€»ç»“äº†ä¸€å¥—ååˆ†è¯¦å°½çš„æ”»ç•¥ï¼ˆåœ¨æœ‰äº›åœ°æ–¹ä½ å¯èƒ½ä¼šæœ‰ä¸åŒæ„è§ï¼‰ã€‚åœ¨æˆ‘è¯»åšæœŸé—´ä»Charles Elkanæ•™æˆé‚£é‡Œå­¦åˆ°äº†å¾ˆå¤šå…³äºç§‘å­¦è®ºæ–‡å†™ä½œçš„é‡è¦å¯å‘å¼æ–¹æ³•ï¼Œæ¯ç§éƒ½èƒ½æç‚¼æˆç²¾ç‚¼çš„è¯­è¨€ã€‚ç°åœ¨ï¼Œå½“æˆ‘å’Œå¹´è½»çš„å­¦ç”Ÿä¸€èµ·å·¥ä½œï¼ŒæŒ‡å¯¼ä»–ä»¬å¦‚ä½•å†™å‡ºæ¸…æ™°æ˜äº†çš„è®ºæ–‡æ—¶ï¼Œæˆ‘å‘ç°è‡ªå·±ä»ç„¶åœ¨é‡å¤å½“å¹´çš„å†™ä½œæ–¹æ³•ï¼Œå¹¶ä¸”å¶å°”ä¼šæœ‰æ–°å‘ç°ã€‚ æ–‡ç« å‘¨çš„æ¯ä¸ªå»ºè®®éƒ½éå¸¸å¥½è®°ï¼Œéƒ½é™„æœ‰ç®€çŸ­çš„è§£é‡Šã€‚ä¸‹é¢å°±è®©æˆ‘ä»¬å¼€å§‹å§ï¼š ä»‹ç» æ‘˜è¦å°½é‡ç®€çŸ­ æ‘˜è¦ä¸å¯èƒ½åŒ…æ‹¬å…¨æ–‡æ‰€æœ‰å†…å®¹ï¼Œå®ƒåº”è¯¥æ˜¯èƒ½è®©äººä¸¤åˆ†é’Ÿå°±çœ‹å®Œçš„â€œå¹¿å‘Šâ€ï¼Œæ˜¯å¯¹æ•´ç¯‡è®ºæ–‡çš„ç²¾å‡†æç‚¼ã€‚è¯¦ç»†æ¥è¯´æœ‰å››æ¡åŸåˆ™ï¼š ç”¨ä¸€å¥è¯æˆ–ä¸€ä¸ªçŸ­è¯­æŠŠä½ çš„é—®é¢˜æè¿°æ¸…æ¥š æ˜ç¡®ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ è¡¨æ˜ä½ çš„ä¸»è¦æˆæœï¼ˆä¹Ÿå¯ä»¥åœ¨å¼€å¤´å†™æ˜ï¼‰ ç”¨ä¸¤ä¸‰å¥è¯è¯´è¯´ç»†èŠ‚å’Œä¸»è¦æ•°æ®ç­‰ ä¸‹é¢æ˜¯æˆ‘è¯»è¿‡çš„æœºå™¨å­¦ä¹ è®ºæ–‡ä¸­æœ€æ£’çš„æ‘˜è¦ï¼š Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. The algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user, with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians. -Sanjoy Dasgupta in â€œLearning Mixtures of Gaussiansâ€ å¦‚æœè¿™é‡ŒSanjoyæŠŠå¼€å¤´ä¸¤å¥è¯åˆå¹¶èµ·æ¥ä¼šä¸ä¼šæ›´ç´§å‡‘å‘¢ï¼Ÿâ€œCurrent techniques for learning mixtures of Gaussians from data are local search heuristics with weak performance guarantees.â€ ä¹Ÿè®¸æœ‰çš„äººè§‰å¾—ä¸é”™ï¼Œæœ‰çš„äººä¼šåå¯¹ï¼Œè®¤ä¸ºæœ¬æ–‡çš„å…³é”®è¯â€œMixture of Gaussiansâ€å°±ä¸èµ·çœ¼äº†ã€‚ åˆ«æƒ³è€è¯»è€… å¦‚æœè®ºæ–‡ç»“æœéœ€è¦å®šé‡è¡¨ç¤ºï¼Œé‚£ä¹ˆå°±åœ¨æ‘˜è¦å’Œä»‹ç»ä¸­ä½“ç°æ•°å­—ï¼›å¦‚æœè®ºæ–‡é‡Œå°±ä¸€ä¸ªç®€å•å…¬å¼ï¼Œé‚£å°±æŠŠå®ƒæ”¾åœ¨ä»‹ç»é‡Œå§ã€‚äººä»¬åªæœ‰æ„Ÿå…´è¶£æ‰ä¼šç»§ç»­è¯»ä¸‹å»ï¼Œä¸è¦æŠŠè¿™äº›ä¿¡æ¯éƒ½éšè—åœ¨è®ºæ–‡ä¸­é—´ã€‚ åˆ æ‰è€å¥—çš„å¼€åœºç™½ â€œThe last 10 years have witnessed tremendous growth in data and computers.â€ â€œDeep learning has had many successes at many thingsâ€.å¦‚æœä½ çš„å¼€å¤´æ˜¯è¿™ç§é€šç”¨å‹çš„ï¼Œå»ºè®®ç›´æ¥åˆ æ‰ã€‚ç¬¬ä¸€å°è±¡å¾ˆé‡è¦ï¼Œè®ºæ–‡çš„ç¬¬ä¸€å¥è¯å¾€å¾€æ˜¯æœ€å…³é”®çš„ï¼Œåƒä¸‡ä¸è¦æµªè´¹ã€‚ å…ˆæé—®å†å›ç­” å¦‚æœæ²¡æœ‰é—®é¢˜ç›´æ¥å†™å‡ºè§£å†³æ–¹æ³•å°†ä¼šéå¸¸æ— èŠï¼Œå¦‚æœä½ çš„è®ºæ–‡ç‰¹åˆ«æŠ½è±¡ï¼Œå®Œå…¨ä¸æ¥åœ°æ°”ï¼Œé‚£ä¹ˆåœ¨è¯»è€…çœ‹æ¥å°±åƒä¸€ç¯‡çº¯æ•°å­¦è®ºæ–‡ã€‚å¦‚æœå¯ä»¥çš„è¯ï¼Œç”¨å®é™…æ¡ˆä¾‹ä½œä¸ºå¼€åœºï¼Œå°†æŠ½è±¡çš„é—®é¢˜å…·ä½“åŒ–ï¼Œç„¶åç”¨å®éªŒä¸°å¯Œè¿™ä¸€è®ºæ–‡ã€‚ ä½ çš„æ–¹æ³•èƒ½åšä»€ä¹ˆï¼Œè€Œä¸æ˜¯ä¸èƒ½åšä»€ä¹ˆ æœ‰çš„æ—¶å€™å¯èƒ½éœ€è¦å»ºç«‹å¯¹ç…§ï¼Œä½†æ˜¯ä¸è¦é‡ç‚¹æè¿°åé¢çš„å¯¹è±¡ï¼Œå°¤å…¶æ˜¯ä½ è‡ªå·±çš„æƒ³æ³•ã€‚å½“ä½ å®¢è§‚åœ°æè¿°è®ºæ–‡æ—¶ï¼Œä¸¢æ‰é‚£äº›é—´æ¥çš„æè¿°ï¼Œç›´æ¥è¯´æ¸…æ¥šä½ çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Œä¸è¦è¯´æŸç‰©ä¸æ˜¯ä»€ä¹ˆã€‚ ç»“æ„ å±‚æ¬¡è¦åˆ†æ˜ ä¸€ç¯‡è®ºæ–‡åˆ†ä¸ºå¥½å‡ éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†åˆåŒ…æ‹¬å¥½å‡ æ®µï¼Œæ®µè½æ˜¯ç”±å¥å­æ„æˆçš„ï¼Œå¥å­åˆæ˜¯ç”±å•è¯æ„æˆçš„ã€‚æœ‰äº›è®ºæ–‡åªçœ‹ä¸€çœ¼ç»“æ„å°±çŸ¥é“è´¨é‡é«˜ä¸é«˜ã€‚æ¯ä¸€èŠ‚åº”è¯¥åƒPPTä¸Šçš„ç›®å½•ä¸€æ ·æ¸…æ™°åœ°æ’åˆ—ï¼Œè€Œä¸”å®ƒä»¬çš„åå­—åº”è¯¥å±äºåŒä¸€ç±»åˆ«ã€‚æœ‰æ—¶ä¸€æ®µè¯å¯ä»¥åªæœ‰ä¸¤ä¸ªå¥å­ï¼Œä½†æ˜¯æœ€å¥½ä¸å°‘äºä¸‰å¥ã€‚ æ•°æ®è¦æœ‰ä»£è¡¨æ€§ å³ä½¿ä¸€ä½â€œå°ç™½â€è¯»è€…ç•¥è¿‡äº†å›¾è¡¨ä¸­çš„ä¸€äº›æ•°å­—ï¼Œä»–ä¹Ÿåº”è¯¥æ˜ç¡®åœ°äº†è§£ä½ åœ¨è®²ä»€ä¹ˆã€‚ä»»ä½•å…³é”®çš„æ¨è®ºæˆ–æŠ€æœ¯ç»†èŠ‚éƒ½è¦ä½“ç°åœ¨æ­£æ–‡ä¸­ï¼Œå…¶ä¸­å¯ä»¥åˆ©ç”¨å›¾è¡¨å¢å¼ºå¯è§†åŒ–ã€‚ åŒæ ·çš„ï¼Œæ•°å­—ä¹Ÿè¦ä¸ä¸»é¢˜ç´§å¯†ç›¸å…³ã€‚å¦‚æœè¯»è€…ï¼ˆæˆ–å®¡ç¨¿äººï¼‰è·³è¿‡æ–‡å­—ç›´æ¥çœ‹å›¾è¡¨ï¼Œä»–ä»¬ä¹Ÿåº”è¯¥å¤§è‡´ç†è§£è®²äº†ä»€ä¹ˆï¼Œå¹¶äº†è§£ç ”ç©¶ç»“æœçš„æ„ä¹‰ã€‚å¦‚æœä¸æ˜ç™½yè½´çš„åˆ†æ•°æ˜¯è¶Šé«˜è¶Šå¥½è¿˜æ˜¯è¶Šä½è¶Šå¥½ï¼Œåˆ™åº”é…æœ‰è¯´æ˜æ–‡å­—ã€‚ ä½†æ˜¯ä¹Ÿä¸è¦å¤ªå¤¸å¼ ï¼Œè¯´æ˜æ–‡å­—ä¸èƒ½å¤ªé•¿ï¼Œæœ€å¥½åœ¨1åˆ°3è¡Œä¹‹é—´ã€‚æ³¨æ„ï¼Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è®ºæ–‡æœ‰æ—¶ä¸€æ•´é¡µéƒ½è¢«å›¾è¡¨å æ®ï¼Œåé¢ä¹Ÿæ²¡æœ‰è¯´æ˜ã€‚æˆ‘ä¸ªäººä¸å–œæ¬¢è¿™ç§é£æ ¼ï¼Œä½†æ˜¯ä¹Ÿè¦æ ¹æ®å®é™…æƒ…å†µå†³å®šã€‚ å¿«é€Ÿå±•ç¤ºè®ºæ–‡ç»“æœ è®ºæ–‡é“ºå«ä¸å®œå¤ªé•¿ï¼šï¼ˆ1ï¼‰å®¡ç¨¿äººåœ¨æ¯åœºä¼šè®®ä¸Šä¼šé˜…è¯»5è‡³10ç¯‡è®ºæ–‡ï¼Œä¸€å¹´å¤§æ¦‚è¦è¯»50è‡³100ç¯‡ç›¸ä¼¼é¢†åŸŸçš„è®ºæ–‡ã€‚é‡å¤çš„åŸºç¡€çŸ¥è¯†éƒ¨åˆ†ä¼šè®©ä»–ä»¬åŒçƒ¦ã€‚ï¼ˆ2ï¼‰å¦‚æœä½ çš„è®ºæ–‡ä¸€å…±æœ‰8é¡µï¼Œä¸»è¦æˆæœåˆ°ç¬¬5é¡µæ‰å±•ç¤ºå‡ºæ¥çš„è¯ï¼Œä¼°è®¡å®¡ç¨¿äººå·²ç»æ²¡æœ‰è€å¿ƒå†çœ‹ä¸‹å»äº†ã€‚ æ‰€ä»¥ï¼Œä¸€å®šè¦äº†è§£ä½ çš„è¯»è€…å’Œè®ºæ–‡çš„å®šä½ã€‚ä½ çš„æ‘˜è¦ã€ä»‹ç»ä»¥åŠæ•´ç¯‡è®ºæ–‡éƒ½åº”è¯¥æ¸…æ™°åœ°å™è¿°ä¸»é¢˜ã€‚ åœ¨è®ºæ–‡ä¸­è§£ç­”è¯»è€…çš„ç–‘é—® ä¸€ä¸ªå¥½çš„å®¡ç¨¿äººä¼šæå‡ºç›¸å…³è´¨ç–‘æ¥æŒ‘æˆ˜è®ºæ–‡ï¼Œæ¯”å¦‚ä¼šé—®ï¼šâ€œæœ‰æ²¡æœ‰å¯èƒ½è¿™ç§æ–¹æ³•ä»…ä»…æ˜¯å› ä¸ºXæ‰èƒ½ä½¿ç”¨ï¼Ÿâ€å¦‚æœä½ å›ç­”ï¼šâ€œæˆ‘ä¸çŸ¥é“â€æˆ–è€…â€œä¸æ˜¯â€çš„è¯ï¼Œä½ çš„è®ºæ–‡æœ‰å¯èƒ½ä¼šè¢«æ‹’ç»ã€‚å¦‚æœä½ èƒ½æå‰é¢„æ–™åˆ°ä¼šè¢«é—®å“ªäº›é—®é¢˜ï¼Œå°±å†™ä¸‹æ¥ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±åšä¸ªè¯•éªŒæ‰¾æ‰¾ç­”æ¡ˆã€‚å¸Œæœ›å¤§å®¶èƒ½æ„è¯†åˆ°ï¼Œåšç ”ç©¶å’Œå†™ä½œæ˜¯åˆ†ä¸å¼€çš„ã€‚ é£æ ¼ å¤šç”¨â€œæˆ‘ä»¬ï¼ˆweï¼‰â€ åœ¨ç§‘å­¦å†™ä½œä¸­ï¼Œè¦ç”¨â€œæˆ‘ä»¬ï¼ˆweï¼‰â€ä½œä¸ºå™è¿°ä¸»ä½“ã€‚è¿™é‡Œçš„â€œæˆ‘ä»¬â€ä»£è¡¨è¯»è€…å’Œä½œè€…åŒæ–¹ä¸€èµ·ã€‚æœ‰çš„æ—¶å€™ä½ å¯èƒ½éœ€è¦é˜è¿°è§‚ç‚¹ï¼Œæ‰€ä»¥è¦åœ¨æ–‡ä¸­è¡¨è¾¾æ¸…æ¥šè¿™äº›æƒ…å†µã€‚ åˆ«ç»™è‡ªå·±æŒ–å‘ ä½ å¿…é¡»ä¿è¯ï¼Œä»»ä½•æœ‰ç›¸å…³çŸ¥è¯†çš„è¯»è€…åœ¨è¯»å®Œä½ çš„æ•´ç¯‡è®ºæ–‡åï¼Œå³ä½¿ä¸è®¤åŒä½ çš„è§‚ç‚¹ã€æ–¹æ³•è®ºé€‰æ‹©æˆ–è€…ä»·å€¼è§‚ï¼Œä¹Ÿæ— æ³•æ‰¾å‡ºæŸä¸€å¥è¯åœ¨è¡¨è¿°ä¸Šçš„é”™è¯¯ã€‚æ¯”å¦‚ï¼šâ€œæˆ‘ä»¬çš„æ–¹æ³•Xåœ¨å¤§å¤šæ•°æ•°æ®ä¸Šæ¯”Yè¡¨ç°å¾—è¦å¥½ã€‚â€è¿™æ˜¯çœŸçš„å—ï¼Ÿâ€œå¤§å¤šæ•°â€ä»€ä¹ˆæ•°æ®é›†ï¼Ÿå®¡ç¨¿äººæ˜¯å¦èƒ½æ‰¾åˆ°ä¸€ä¸ªæ¨ç¿»è¿™ä¸€ç»“è®ºçš„æ•°æ®é›†ï¼Ÿæ‰€ä»¥ï¼Œæœ€å¥½æŠŠâ€œå¤§å¤šæ•°ï¼ˆmostï¼‰â€æ”¹ä¸ºâ€œå¾ˆå¤šï¼ˆmanyï¼‰â€ï¼Œè¿™æ ·ä¸ç®¡æ˜¯å®šä¹‰è¿˜æ˜¯åé©³éƒ½æ›´å®¹æ˜“ä¸€äº›ã€‚ å¤šä¸€å¥ä¸å¦‚å°‘ä¸€å¥ ä¸ä¸Šæ–‡ç±»ä¼¼ï¼Œå¦‚æœä½ å¯¹æŸä¸€ç»“è®ºå¹¶ä¸æ˜¯100%è‚¯å®šï¼Œé‚£å°±ä¸è¦è½»æ˜“åšå‡ºã€‚å°‘å†™ä¸€å¥ä¹Ÿè®¸ä¸ä¼šè¢«æ‹’ï¼Œä½†å¤šå†™ä¸€å¥å°±å¾ˆæœ‰å¯èƒ½è¢«å®¡ç¨¿äººpassæ‰ã€‚ å¦‚ä½•è¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ ä½ å¯èƒ½ä¼šé—®ï¼šâ€œå®¡ç¨¿äººä¹Ÿè®¸ä¸åŒæ„æˆ‘çš„è§‚ç‚¹ï¼Œæ˜¯ä¸æ˜¯å°±æ„å‘³ç€æˆ‘ä¸èƒ½åœ¨è®ºæ–‡ä¸­å†™å‡ºæ¥å‘¢ï¼Ÿâ€å¹¶éè¿™æ ·ï¼Œä¾‹å¦‚ä½ è®¤ä¸ºæœªæ¥GANsåœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢å¾ˆæœ‰å‰æ™¯ï¼Œä½†åœ¨æ–‡ç« ä¸­ä½ åº”è¯¥è¿™æ ·å†™ï¼šâ€œåœ¨æˆ‘çœ‹æ¥ï¼ˆin my opinionï¼‰ï¼ŒGANsâ€¦â€¦â€ è¯­è¨€ åˆ‡åˆ†é•¿å¥ ç»éªŒä¸è¶³çš„å†™ä½œè€…ä»¬æ€»ä¼šé”™è¯¯åœ°è®¤ä¸ºï¼Œé•¿å¥èƒ½åæ˜ å…¶é£è¯é€ å¥çš„èƒ½åŠ›ã€‚ä½†æ˜¯å‡ºè‰²çš„ç§‘å­¦è®ºæ–‡å¤§å¤šç”¨çŸ­å¥æ„æˆã€‚å¦‚æœä¸èƒ½æŠŠä½ çš„è§‚ç‚¹æµ“ç¼©åœ¨ä¸€å¥è¯é‡Œï¼Œå°±è¯•è¯•æŠŠå®ƒä»¬åˆ†å¼€ã€‚æŠ€æœ¯åä½œè¦å°½é‡ç®€æ´æ¸…æ¥šï¼Œç»“è®ºå¯ä»¥å¤æ‚ï¼Œä½†æ˜¯è¡¨è¿°ç»“è®ºçš„è¯ä¸èƒ½å¤æ‚ã€‚ åˆ é™¤å¼ºè°ƒè¯å’Œç©ºæ´çš„åŠ¨è¯ ä¾‹å¦‚ï¼šextremelyã€veryã€incrediblyã€completelyã€barelyã€essentiallyã€ratherã€quiteã€definitelyâ€¦â€¦ è¿™äº›å¼ºè°ƒè¯æœ‰ä¸¤ç‚¹ä¸å¥½ã€‚é¦–å…ˆï¼Œå®ƒä»¬æ”¹å˜äº†å¥å­çš„ç›®çš„ï¼šâ€œalgorithm X provides a tight approximationâ€è¿™å¬ä¸Šå»å¾ˆæœ‰ä¿¡å¿ƒã€‚å¦‚æœåŠ ä¸Šä¸€ä¸ªä¿®é¥°è¯­â€œveryâ€ï¼Œå°±å˜æˆï¼šâ€œalgorithm X provides a very tight approximationâ€æ€»è§‰å¾—æœ‰é‚£ä¹ˆä¸€ç‚¹ä¸ç¡®å®šæ€§ã€‚å¦å¤–ï¼Œå®ƒä»¬è¿˜èƒ½è¡¨è¾¾æ„è§ã€‚ä¾‹å¦‚ï¼šâ€œIs the algorithm betterï¼Ÿâ€æ˜¯çš„ã€‚å¦‚æœæ”¹æˆï¼šâ€œIs it much betterï¼Ÿâ€è¿™å°±æ˜¯ä¸€ç§æ„è§äº†ï¼Œä¹Ÿè®¸æ˜¯åœ¨ç»™è‡ªå·±æŒ–å‘ã€‚ ä¸»è¯­ã€åŠ¨è¯ã€ä¿®é¥°è¯­å¿…é¡»ä¿æŒä¸€è‡´ å†™ä½œä¸­å¸¸è§çš„é”™è¯¯æ˜¯å°†åŠ¨è¯å’Œä¿®é¥°è¯­æ”¾åˆ°é”™è¯¯çš„ä¸»è¯­ä¸Šã€‚ä¾‹å¦‚ï¼šâ€œthe algorithm tries to X, or the data is biased.â€è¿™å¥è¯ä¸­ï¼Œç®—æ³•ä¸èƒ½è‡ªå·±å°è¯•ï¼ˆtriesï¼‰ï¼Œä¸»è¯­åº”è¯¥æ˜¯æˆ‘ä»¬ï¼ˆweï¼‰å»ºæ¨¡è€…ï¼Œè€Œä¸æ˜¯ç®—æ³•ã€‚ æ¨è®ºï¼šæ¯ä¸ªåŠ¨è¯éƒ½åº”æœ‰æ‰€å±ã€‚æ²¡æœ‰ä¸»è¯­çš„åŠ¨è¯é€šå¸¸ç”¨äºè¢«åŠ¨ç»“æ„ä¸­ï¼ˆä¸»è¦åŠ¨è¯æ˜¯â€œto beâ€ï¼‰ã€‚æ¯”å¦‚ï¼šâ€œLSTMs are claimed to X, Y, Zâ€ï¼Œè¿™å¥è¯é‡Œâ€œclaimedâ€çš„ä¸»è¯­æ˜¯è°ï¼Ÿè¿™ä¸€ä¿¡æ¯æœ€å¥½åœ¨å…¶ä»–åœ°æ–¹äº¤ä»£æ¸…æ¥šï¼Œä¸€ç§æ–¹æ³•æ˜¯åœ¨åé¢åŠ æ‹¬å·ï¼Œé™„ä¸Šè§£é‡Šä¿¡æ¯ï¼›å¦ä¸€ç§æ–¹æ³•è¦ä½œè€…æ¸…æ¥šåœ°è¯´æ˜ã€‚ å‚è€ƒæ–‡çŒ® ä¸€èˆ¬å¼•ç”¨ ä½ å¼•ç”¨çš„æ–‡çŒ®çš„ä½œè€…æœ‰å¯èƒ½å°±æ˜¯ä½ çš„å®¡ç¨¿äººå“¦ã€‚å®¡ç¨¿äººé€šå¸¸éƒ½ä¼šé—®ä½ ä¸ºä»€ä¹ˆæ²¡æœ‰å¼•ç”¨æŸä¸€ä½œè€…çš„å¦å¤–å‡ ä¸ªä½œå“ã€‚å¦‚æœä¸ä½ çš„è®ºæ–‡ä¸ç›¸å…³ï¼Œé‚£å°±ä¸è¦å¼•ç”¨ã€‚å¦‚æœå®ƒä»¬æ˜¯ç›¸å…³çš„ï¼Œå¼•ç”¨ä¸€ä¸‹ä¹Ÿæ²¡ä»€ä¹ˆæŸå¤±ã€‚è¿™æ ·åšçš„è¯ï¼Œä½ çš„è®ºæ–‡ç»“æœä¸ä¼šå¤ªå·®ï¼Œå¹¶ä¸”å®¡ç¨¿äººä¹Ÿè®¸æ˜¯ä½ æœªæ¥æƒ³è¦ä¸€èµ·å·¥ä½œçš„äººï¼Œå¼•ç”¨ä»–ä»¬çš„ä½œå“ä¼šå¼•èµ·ä»–ä»¬çš„æ³¨æ„ã€‚ æ•´ä½“å¼•ç”¨ å®¡ç¨¿äººé€šå¸¸éƒ½æ¯”è¾ƒæ‡’ï¼Œè€Œä¸”æ²¡æœ‰é«˜å¼ºçš„è®°å¿†åŠ›ã€‚å¦‚æœä½ çš„ä½œå“æ˜¯å»ºç«‹åœ¨åˆ«äººçš„æˆæœä¹‹ä¸Šï¼Œé‚£ä¹ˆä¸è¦åªå¼•ç”¨ä¸ä½ ç›¸å…³çš„éƒ¨åˆ†â€”â€”é‚£ä¹ˆåªæ˜¯å¯¹ä½ å·¥ä½œå†…å®¹çš„æ€»ç»“ã€‚è€Œæ˜¯å½“ä½ åœ¨è¯»åˆ°ä¼˜äºä½ è‡ªå·±çš„æ–¹æ³•æ—¶å¼•ç”¨æ•´æ®µæ–‡å­—ã€‚å°¤å…¶æ˜¯è¿‘äº›å¹´ï¼ˆ5~10å¹´ï¼‰çš„ä½œå“ï¼Œå®ƒä»¬å¯èƒ½è¿˜ä¸å¸¸è§ï¼Œé€‚åˆå‡ºç°åœ¨å¼•ç”¨æœ€å¤šçš„â€œç›¸å…³ç ”ç©¶â€ç‰ˆå—é‡Œã€‚ å°½é‡å¤šå¼•ç”¨ è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„æ–¹æ³•ï¼Œé€‚ç”¨äºä¼šè®®å‡ºç‰ˆç‰©ï¼Œé€šå¸¸ä¼šè®®è®ºæ–‡ä¼šé™åˆ¶å‚è€ƒæ–‡çŒ®çš„é¡µæ•°ï¼ˆä¸€èˆ¬æ˜¯1æˆ–2é¡µï¼‰ã€‚å¦‚æœä½ æ¼äº†æœ€é‡è¦çš„å‚è€ƒæ–‡çŒ®ï¼Œå®¡ç¨¿äººæ˜¯ç»å¯¹ä¸ä¼šæ”¾è¿‡ä½ çš„ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ¼äº†ä¸€äº›ä¸å¤ªé‡è¦çš„æ–‡çŒ®ï¼Œä½ å¯ä»¥è¯´è¶…å‡ºæ•°ç›®é™åˆ¶äº†ï¼ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€Ÿå£ï¼‰ã€‚æ‰€ä»¥å¦‚æœä½ çš„å‚è€ƒæ–‡çŒ®å¤ªå°‘çš„è¯ï¼Œå¯å°±è¯´ä¸è¿‡å»äº†ã€‚ "},"paper/Image_to_Image.html":{"url":"paper/Image_to_Image.html","title":"Image-to-Image çš„è®ºæ–‡æ±‡æ€»ï¼ˆå« GitHub ä»£ç ï¼‰","keywords":"","body":"Image-to-Image çš„è®ºæ–‡æ±‡æ€»ï¼ˆå« GitHub ä»£ç ï¼‰ å›¾åƒç”Ÿæˆä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸéå¸¸æœ‰æ„æ€çš„æ–¹å‘ï¼Œå›¾åƒåˆ°å›¾åƒçš„å˜æ¢æ˜¯å…¶ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„åº”ç”¨ï¼Œä½¿ç”¨å›¾åƒåˆ°å›¾åƒçš„å˜æ¢ï¼Œå¯ä»¥å®Œæˆéå¸¸å¤šæœ‰è¶£çš„åº”ç”¨ï¼Œå¯ä»¥æŠŠé»‘ç†Šå˜æˆç†ŠçŒ«ï¼ŒæŠŠä½ çš„ç…§ç‰‡æ¢æˆåˆ«äººçš„è¡¨æƒ…ï¼Œè¿˜å¯ä»¥æŠŠæ™®é€šçš„ç…§ç‰‡å˜æˆæ¯•åŠ ç´¢é£æ ¼çš„æ²¹ç”»ï¼Œè‡ªä»GANæ¨ªç©ºå‡ºä¸–ä¹‹åï¼Œè¿™æ–¹é¢çš„åº”ç”¨ä¹Ÿè¶Šæ¥è¶Šå¤šï¼Œä¸‹é¢æ˜¯å¯¹è¿™ä¸ªé¢†åŸŸçš„ç›¸å…³è®ºæ–‡çš„ä¸€ä¸ªæ•´ç†ï¼Œè€Œä¸”å¤§éƒ¨åˆ†éƒ½æœ‰ä»£ç ï¼ githubåœ°å€ï¼šhttps://github.com/ExtremeMart/image-to-image-papers è¿™æ˜¯ä¸€ä¸ªå›¾åƒåˆ°å›¾åƒçš„è®ºæ–‡çš„æ±‡æ€»ã€‚è®ºæ–‡æŒ‰ç…§arXivä¸Šç¬¬ä¸€æ¬¡æäº¤æ—¶é—´æ’åºã€‚ ç›‘ç£å­¦ä¹  Note Model Paper Conference paper link code link pix2pix Image-to-Image Translation with Conditional Adversarial Networks CVPR 2017 1611.07004 junyanz/pytorch-CycleGAN-and-pix2pix texture guided TextureGAN TextureGAN: Controlling Deep Image Synthesis with Texture Patches CVPR 2018 1706.02823 janesjanes/Pytorch-TextureGAN Contextual GAN Image Generation from Sketch Constraint Using Contextual GAN ECCV 2018 1711.08972 pix2pix-HD High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs CVPR 2018 1711.11585 NVIDIA/pix2pixHD one-to-many BicycleGAN Toward Multimodal Image-to-Image Translation NIPS 2017 1711.11586 junyanz/BicycleGAN keypoint guided G2-GAN Geometry Guided Adversarial Facial Expression Synthesis MM 2018 1712.03474 contour2im Smart, Sparse Contours to Represent and Edit Images CVPR 2018 1712.08232 website disentangle Cross-domain disentanglement networks Image-to-image translation for cross-domain disentanglement NIPS 2018 1805.09730 video vid2vid Video-to-Video Synthesis NIPS 2018 1808.06601 NVIDIA/vid2vid video pix2pix-HD + Temporal Smoothing + faceGAN Everybody Dance Now ECCVW 2018 1808.07371 website éç›‘ç£å­¦ä¹  éç›‘ç£å­¦ä¹ -é€šç”¨ Note Model Paper Conference paper link code link DTN Unsupervised Cross-Domain Image Generation ICLR 2017 1611.02200 yunjey/domain-transfer-network (unofficial) UNIT Unsupervised image-to-image translation networks NIPS 2017 1703.00848 mingyuliutw/UNIT DiscoGAN Learning to Discover Cross-Domain Relations with Generative Adversarial Networks ICML 2017 1703.05192 SKTBrain/DiscoGAN CycleGAN Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks ICCV 2017 1703.10593 junyanz/pytorch-CycleGAN-and-pix2pix DualGAN DualGAN: Unsupervised Dual Learning for Image-to-Image Translation ICCV 2017 1704.02510 duxingren14/DualGAN DistanceGAN One-Sided Unsupervised Domain Mapping NIPS 2017 1706.00826 sagiebenaim/DistanceGAN semi supervised Triangle GAN Triangle Generative Adversarial Networks NIPS 2017 1709.06548 LiqunChen0606/Triangle-GAN CartoonGAN CartoonGAN: Generative Adversarial Networks for Photo Cartoonization CVPR 2018 thecvf unofficial test, unofficial pytorch non-adversarial NAM NAM: Non-Adversarial Unsupervised Domain Mapping ECCV 2018 1806.00804 facebookresearch/nam SCAN Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks ECCV 2018 1807.08536 dilated conv, improve shape deform. GANimorph Improved Shape Deformation in Unsupervised Image to Image Translation ECCV 2018 1808.04325 brownvc/ganimorph instance aware InstaGAN Instance-aware image-to-image translation ICLR 2019 (in review) openreview éç›‘ç£å­¦ä¹ -æ³¨æ„åŠ›æœºåˆ¶æˆ–è€…æ¨¡æ¿å¯¼å‘æœºåˆ¶ Note Model Paper Conference paper link code link mask ContrastGAN Generative Semantic Manipulation with Mask-Contrasting GAN ECCV 2018 1708.00315 attention DA-GAN DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks CVPR 2018 1802.06454 mask / attention Attention-GAN Attention-GAN for Object Transï¬guration in Wild Images 1803.06798 attention Attention guided GAN Unsupervised Attention-guided Image to Image Translation NIPS 2018 1806.02311 AlamiMejjati/Unsupervised-Attention-guided-Image-to-Image-Translation attention, one-sided Show, Attend and Translate: Unsupervised Image Translation with Self-Regularization and Attention 1806.06195 éç›‘ç£å­¦ä¹ -å¤šå¯¹å¤šï¼ˆå±æ€§ï¼‰ Note Model Paper Conference paper link code link Conditional CycleGAN Conditional CycleGAN for Attribute Guided Face Image Generation ECCV 2018 1705.09966 StarGAN StarGAN: Uniï¬ed Generative Adversarial Networks for Multi-Domain Image-to-Image Translation CVPR 2018 1711.09020 yunjey/StarGAN AttGAN AttGAN: Facial Attribute Editing by Only Changing What You Want 1711.10678 LynnHo/AttGAN-Tensorflow ComboGAN ComboGAN: Unrestrained Scalability for Image Domain Translation CVPRW 2018 1712.06909 AAnoosheh/ComboGAN AugCGAN (Augmented CycleGAN) Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data ICML 2018 1802.10151 aalmah/augmented_cyclegan sparsely grouped dataset SG-GAN Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation MM 2018 1805.07509 zhangqianhui/Sparsely-Grouped-GAN GANimation GANimation: Anatomically-aware Facial Animation from a Single Image ECCV 2018 (honorable mention) 1807.09251 albertpumarola/GANimation éç›‘ç£å­¦ä¹ -åˆ†ç¦»ï¼ˆä¸/æˆ–æ ·æœ¬å¯¼å‘ï¼‰ Note Model Paper Conference paper link code link XGAN XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings ICML 2018 1711.05139 dataset ELEGANT ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes ECCV 2018 1803.10562 Prinsphield/ELEGANT MUNIT Multimodal Unsupervised Image-to-Image Translation ECCV 2018 1804.04732 NVlabs/MUNIT cd-GAN (Conditional DualGAN) Conditional Image-to-Image Translation CVPR 2018 1805.00251 EG-UNIT Exemplar Guided Unsupervised Image-to-Image Translation 1805.11145 DRIT Diverse Image-to-Image Translation via Disentangled Representations ECCV 2018 1808.00948 HsinYingLee/DRIT non-disentangle, face makeup guided BeautyGAN BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network MM 2018 author UFDN A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation NIPS 2018 1809.01361 Alexander-H-Liu/UFDN "},"paper/GNN.html":{"url":"paper/GNN.html","title":"GNNç»¼è¿°","keywords":"","body":"GNN ç»¼è¿° æœ€è¿‘ï¼Œæ¸…åå¤§å­¦å­™èŒ‚æ¾æ•™æˆç»„åœ¨ arXiv å‘å¸ƒäº†è®ºæ–‡ Graph Neural Networks: A Review of Methods and Applicationsï¼Œä½œè€…å¯¹ç°æœ‰çš„GNNæ¨¡å‹åšäº†è¯¦å°½ä¸”å…¨é¢çš„ç»¼è¿°ã€‚ ä½œè€…ï¼šå‘¨ç•Œã€å´”æ·¦æ¸ ã€å¼ æ­£å½¦*ï¼Œæ¨æˆï¼Œåˆ˜çŸ¥è¿œï¼Œå­™èŒ‚æ¾ â€œå›¾ç¥ç»ç½‘ç»œæ˜¯è¿æ¥ä¸»ä¹‰ä¸ç¬¦å·ä¸»ä¹‰çš„æœ‰æœºç»“åˆï¼Œä¸ä»…ä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿåº”ç”¨åœ¨å›¾è¿™ç§éæ¬§å‡ é‡Œå¾·ç»“æ„ä¸Šï¼Œè¿˜ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹èµ‹äºˆäº†ä¸€å®šçš„å› æœæ¨ç†èƒ½åŠ›ã€‚â€è®ºæ–‡çš„å…±åŒç¬¬ä¸€ä½œè€…å‘¨ç•Œè¯´ã€‚ â€œåœ¨æ·±åº¦å­¦ä¹ æ–¹æ³•çš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§å—åˆ°è´¨ç–‘çš„ä»Šå¤©ï¼Œå›¾ç¥ç»ç½‘ç»œå¯èƒ½ä¸ºä»Šåäººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„æ–¹å‘ã€‚â€ GNNæœ€è¿‘åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¯¹äºæƒ³è¦å¿«é€Ÿäº†è§£è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶äººå‘˜æ¥è¯´ï¼Œå¯èƒ½ä¼šé¢ä¸´ç€æ¨¡å‹å¤æ‚ã€åº”ç”¨é—¨ç±»ä¼—å¤šçš„é—®é¢˜ã€‚ â€œæœ¬æ–‡å¸Œæœ›ä¸ºè¯»è€…æä¾›ä¸€ä¸ªæ›´é«˜å±‚æ¬¡çš„è§†è§’ï¼Œå¿«é€Ÿäº†è§£GNNé¢†åŸŸä¸åŒæ¨¡å‹çš„åŠ¨æœºä¸ä¼˜åŠ¿ã€‚â€å‘¨ç•Œå‘Šè¯‰æ–°æ™ºå…ƒï¼šâ€œåŒæ—¶ï¼Œé€šè¿‡å¯¹ä¸åŒçš„åº”ç”¨è¿›è¡Œåˆ†ç±»ï¼Œæ–¹ä¾¿ä¸åŒé¢†åŸŸçš„ç ”ç©¶è€…å¿«é€Ÿäº†è§£å°†GNNåº”ç”¨åˆ°ä¸åŒé¢†åŸŸçš„æ–‡çŒ®ã€‚â€ æ¯«ä¸å¤¸å¼ åœ°è¯´ï¼Œè®ºæ–‡ä¸­çš„å›¾è¡¨å¯¹äºæƒ³è¦äº†è§£å­¦ä¹ GNNä¹ƒè‡³å› æœæ¨ç†ç­‰æ–¹å‘çš„ç ”ç©¶è€…æ¥è¯´ï¼Œç®€ç›´åº”è¯¥é«˜æ¸…æ‰“å°è¿‡å¡‘ç„¶åè´´åœ¨å¢™ä¸Šä»¥ä½œå‚è€ƒâ€”â€” GNNçš„å„ç§å˜ä½“ï¼Œé€šè¿‡æ¯”å¯¹å„è‡ªçš„ aggregator & updaterï¼Œå°±èƒ½è½»æ¾åˆ†è¾¨ä¸åŒçš„GNNæ¨¡å‹ã€‚è¿™åªæ˜¯è¿™ç¯‡ç»¼è¿°å¼ºå¤§å›¾è¡¨çš„ä¸€ä¸ªç¤ºä¾‹ã€‚ æƒ³è¦å¿«é€Ÿäº†è§£GNNï¼Œçœ‹è¿™ç¯‡æ–‡ç« ç»å¯¹æ²¡é”™ åœ¨å†…å®¹ä¸Šï¼Œæ¨¡å‹æ–¹é¢ï¼Œæœ¬æ–‡ä»GNNåŸå§‹æ¨¡å‹çš„æ„å»ºæ–¹å¼ä¸å­˜åœ¨çš„é—®é¢˜å‡ºå‘ï¼Œä»‹ç»äº†å¯¹å…¶è¿›è¡Œä¸åŒæ”¹è¿›çš„GNNå˜ä½“ï¼ŒåŒ…æ‹¬å¦‚ä½•å¤„ç†ä¸åŒçš„å›¾çš„ç±»å‹ã€å¦‚ä½•è¿›è¡Œé«˜æ•ˆçš„ä¿¡æ¯ä¼ é€’ä»¥åŠå¦‚ä½•åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚æœ€åä»‹ç»äº†å‡ ä¸ªè¿‘å¹´æ¥æå‡ºçš„é€šç”¨æ¡†æ¶ï¼Œå®ƒä»¬æ€»ç»“æ¦‚æ‹¬äº†å¤šä¸ªç°æœ‰çš„æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚ åœ¨åº”ç”¨ä¸Šï¼Œæ–‡ç« å°†GNNçš„åº”ç”¨é¢†åŸŸåˆ†ä¸ºäº†ç»“æ„åŒ–åœºæ™¯ã€éç»“æ„åŒ–åœºæ™¯ä»¥åŠå…¶ä»–åœºæ™¯å¹¶ä»‹ç»äº†è¯¸å¦‚ç‰©ç†ã€åŒ–å­¦ã€å›¾åƒã€æ–‡æœ¬ã€å›¾ç”Ÿæˆæ¨¡å‹ã€ç»„åˆä¼˜åŒ–é—®é¢˜ç­‰ç»å…¸çš„GNNåº”ç”¨ã€‚ å…¸å‹åº”ç”¨åœºæ™¯ä»‹ç» æ–‡ç« æœ€åæå‡ºäº†å››ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å¦‚ä½•å¤„ç†å †å å¤šå±‚GNNé€ æˆçš„å¹³æ»‘é—®é¢˜ï¼Œå¦‚ä½•å¤„ç†åŠ¨æ€å˜åŒ–çš„å›¾ç»“æ„ï¼Œå¦‚ä½•ä½¿ç”¨é€šç”¨çš„æ–¹æ³•å¤„ç†éç»“æ„åŒ–çš„æ•°æ®ä»¥åŠå¦‚ä½•å°†å…¶æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„ç½‘ç»œä¸Šã€‚ ä½œè€…è¿˜æ•´ç†äº†ä¸€ä¸ªGNNè®ºæ–‡åˆ—è¡¨ï¼š https://github.com/thunlp/GNNPapers ä»¥ä¸‹æ˜¯æ–°æ™ºå…ƒå¯¹è¿™ç¯‡ç»¼è¿°çš„éƒ¨åˆ†æ‘˜è¯‘ï¼Œç‚¹å‡»é˜…è¯»åŸæ–‡æŸ¥çœ‹ arXiv è®ºæ–‡ã€‚ åŸå§‹GNNåŠå…¶å±€é™æ€§ GNNçš„æ¦‚å¿µé¦–å…ˆæ˜¯åœ¨F. Scarselliç­‰äººçš„è®ºæ–‡The graph neural network modelï¼ˆF. Scarselli et. al. 2009ï¼‰ä¸­æå‡ºçš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æè¿°äº†åŸå§‹çš„GNNï¼Œå¹¶åˆ—ä¸¾äº†åŸå§‹GNNåœ¨è¡¨ç¤ºèƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚ æ¥ç€ï¼Œæˆ‘ä»¬ä»‹ç»äº†å‡ ç§ä¸åŒçš„GNNå˜ä½“ï¼Œè¿™äº›å˜ä½“å…·æœ‰ä¸åŒçš„å›¾å½¢ç±»å‹ï¼Œåˆ©ç”¨ä¸åŒçš„ä¼ æ’­å‡½æ•°å’Œè®­ç»ƒæ–¹æ³•ã€‚ æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰ä¸ªé€šç”¨æ¡†æ¶ï¼Œåˆ†åˆ«æ˜¯message passing neural network (MPNN)ï¼Œnon-local neural network (NLNN)ï¼Œä»¥åŠgraph network(GN)ã€‚MPNNç»“åˆäº†å„ç§å›¾ç¥ç»ç½‘ç»œå’Œå›¾å·ç§¯ç½‘ç»œæ–¹æ³•ï¼›NLNNç»“åˆäº†å‡ ç§â€œself-attentionâ€ç±»å‹çš„æ–¹æ³•ï¼›è€Œå›¾ç½‘ç»œGNå¯ä»¥æ¦‚æ‹¬æœ¬æ–‡æåˆ°çš„å‡ ä¹æ‰€æœ‰å›¾ç¥ç»ç½‘ç»œå˜ä½“ã€‚ å›¾ç¥ç»ç½‘ç»œ å¦‚å‰æ‰€è¿°ï¼Œå›¾ç¥ç»ç½‘ç»œ(GNN)çš„æ¦‚å¿µæœ€æ—©æ˜¯Scarselliç­‰äººåœ¨2009å¹´æå‡ºçš„ï¼Œå®ƒæ‰©å±•äº†ç°æœ‰çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºå¤„ç†å›¾(graph)ä¸­è¡¨ç¤ºçš„æ•°æ®ã€‚åœ¨å›¾ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ç”±å…¶ç‰¹æ€§å’Œç›¸å…³èŠ‚ç‚¹å®šä¹‰çš„ã€‚ è™½ç„¶å®éªŒç»“æœè¡¨æ˜ï¼ŒGNNæ˜¯å»ºæ¨¡ç»“æ„åŒ–æ•°æ®çš„å¼ºå¤§æ¶æ„ï¼Œä½†åŸå§‹GNNä»å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ é¦–å…ˆï¼Œå¯¹äºå›ºå®šèŠ‚ç‚¹ï¼ŒåŸå§‹GNNè¿­ä»£æ›´æ–°èŠ‚ç‚¹çš„éšè—çŠ¶æ€æ˜¯ä½æ•ˆçš„ã€‚å¦‚æœæ”¾å®½äº†å›ºå®šç‚¹çš„å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªå¤šå±‚çš„GNNæ¥å¾—åˆ°èŠ‚ç‚¹åŠå…¶é‚»åŸŸçš„ç¨³å®šè¡¨ç¤ºã€‚ å…¶æ¬¡ï¼ŒGNNåœ¨è¿­ä»£ä¸­ä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼Œè€Œå¤§å¤šæ•°æµè¡Œçš„ç¥ç»ç½‘ç»œåœ¨ä¸åŒçš„å±‚ä¸­ä½¿ç”¨ä¸åŒçš„å‚æ•°ï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚ç‰¹å¾æå–æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒèŠ‚ç‚¹éšè—çŠ¶æ€çš„æ›´æ–°æ˜¯ä¸€ä¸ªé¡ºåºè¿‡ç¨‹ï¼Œå¯ä»¥ä»RNNå†…æ ¸(å¦‚GRU å’Œ LSTM)ä¸­è·ç›Šã€‚ ç¬¬ä¸‰ï¼Œåœ¨è¾¹ä¸Šä¹Ÿæœ‰ä¸€äº›æ— æ³•åœ¨åŸå§‹GNNä¸­å»ºæ¨¡çš„ä¿¡æ¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å­¦ä¹ è¾¹çš„éšè—çŠ¶æ€ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚ æœ€åï¼Œå¦‚æœæˆ‘ä»¬æŠŠç„¦ç‚¹æ”¾åœ¨èŠ‚ç‚¹çš„è¡¨ç¤ºä¸Šè€Œä¸æ˜¯å›¾å½¢ä¸Šï¼Œå°±ä¸é€‚åˆä½¿ç”¨å›ºå®šç‚¹ï¼Œå› ä¸ºåœ¨å›ºå®šç‚¹ä¸Šçš„è¡¨ç¤ºçš„åˆ†å¸ƒåœ¨æ•°å€¼ä¸Šæ˜¯å¹³æ»‘çš„ï¼ŒåŒºåˆ†æ¯ä¸ªèŠ‚ç‚¹çš„ä¿¡æ¯é‡ä¹Ÿæ¯”è¾ƒå°‘ã€‚ å›¾ç¥ç»ç½‘ç»œçš„å˜ä½“ åœ¨è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬æå‡ºå›¾ç¥ç»ç½‘ç»œçš„å‡ ç§å˜ä½“ã€‚é¦–å…ˆæ˜¯åœ¨ä¸åŒå›¾ç±»å‹ä¸Šè¿è¡Œçš„å˜ä½“ï¼Œè¿™äº›å˜ä½“æ‰©å±•äº†åŸå§‹æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†åœ¨ä¼ æ’­æ­¥éª¤è¿›è¡Œä¿®æ”¹ï¼ˆå·ç§¯ã€é—¨æœºåˆ¶ã€æ³¨æ„åŠ›æœºåˆ¶å’Œskip connectionï¼‰çš„å‡ ç§å˜ä½“ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥æ›´å¥½åœ°å­¦ä¹ è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬æè¿°äº†ä½¿ç”¨é«˜çº§è®­ç»ƒæ–¹æ³•çš„æ ‡é¢˜ï¼Œè¿™äº›æ–¹æ³•æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ å›¾2æ¦‚è¿°äº†GNNçš„ä¸åŒå˜ä½“ã€‚ ä¸€è§ˆGNNçš„ä¸åŒå˜ä½“ å›¾çš„ç±»å‹(Graph Types) åœ¨åŸå§‹GNNä¸­ï¼Œè¾“å…¥çš„å›¾ç”±å¸¦æœ‰æ ‡ç­¾ä¿¡æ¯çš„èŠ‚ç‚¹å’Œæ— å‘çš„è¾¹ç»„æˆï¼Œè¿™æ˜¯æœ€ç®€å•çš„å›¾å½¢æ ¼å¼ã€‚ç„¶è€Œï¼Œä¸–ç•Œä¸Šæœ‰è®¸å¤šä¸åŒçš„å›¾å½¢ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›ç”¨äºå»ºæ¨¡ä¸åŒç±»å‹å›¾å½¢çš„æ–¹æ³•ã€‚ å›¾ç±»å‹çš„å˜ä½“ æœ‰å‘å›¾(Directed Graphs ) å›¾å½¢çš„ç¬¬ä¸€ä¸ªå˜ä½“æ˜¯æœ‰å‘å›¾ã€‚æ— å‘è¾¹å¯ä»¥çœ‹ä½œæ˜¯ä¸¤ä¸ªæœ‰å‘è¾¹ï¼Œè¡¨æ˜ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´å­˜åœ¨ç€å…³ç³»ã€‚ç„¶è€Œï¼Œæœ‰å‘è¾¹æ¯”æ— å‘è¾¹èƒ½å¸¦æ¥æ›´å¤šçš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªçŸ¥è¯†å›¾ä¸­ï¼Œè¾¹ä»headå®ä½“å¼€å§‹åˆ°tailå®ä½“ç»“æŸï¼Œheadå®ä½“æ˜¯tailå®ä½“çš„çˆ¶ç±»ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬åº”è¯¥åŒºåˆ«å¯¹å¾…çˆ¶ç±»å’Œå­ç±»çš„ä¿¡æ¯ä¼ æ’­è¿‡ç¨‹ã€‚æœ‰å‘å›¾çš„å®ä¾‹æœ‰ADGPM (M. Kampffmeyer et. al. 2018)ã€‚ å¼‚æ„å›¾(Heterogeneous Graphs) å›¾çš„ç¬¬äºŒä¸ªå˜ä½“æ˜¯å¼‚æ„å›¾ï¼Œå¼‚æ„å›¾æœ‰å‡ ç§ç±»å‹çš„èŠ‚ç‚¹ã€‚å¤„ç†å¼‚æ„å›¾æœ€ç®€å•çš„æ–¹æ³•æ˜¯å°†æ¯ä¸ªèŠ‚ç‚¹çš„ç±»å‹è½¬æ¢ä¸ºä¸åŸå§‹ç‰¹å¾è¿æ¥çš„ä¸€ä¸ªone-hotç‰¹å¾å‘é‡ã€‚å¼‚æ„å›¾å¦‚GraphInceptionã€‚ å¸¦è¾¹ä¿¡æ¯çš„å›¾(Edge-informative Graph) å›¾çš„å¦å¤–ä¸€ä¸ªå˜ä½“æ˜¯ï¼Œæ¯æ¡è¾¹éƒ½æœ‰ä¿¡æ¯ï¼Œæ¯”å¦‚æƒå€¼æˆ–è¾¹çš„ç±»å‹ã€‚ä¾‹å¦‚G2Så’ŒR-GCNã€‚ ä½¿ç”¨ä¸åŒè®­ç»ƒæ–¹æ³•çš„å›¾å˜ä½“ è®­ç»ƒæ–¹æ³•å˜ä½“ åœ¨ä¼ æ’­æ­¥éª¤è¿›è¡Œä¿®æ”¹çš„GNNå˜ä½“ ä¼ æ’­æ­¥éª¤å˜ä½“ GNNçš„ä¸‰å¤§é€šç”¨æ¡†æ¶ é™¤äº†å›¾ç¥ç»ç½‘ç»œçš„ä¸åŒå˜ä½“ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å‡ ä¸ªé€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä¸åŒçš„æ¨¡å‹é›†æˆåˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚ J. Gilmerç­‰äºº(J. Gilmer et. al. 2017)æå‡ºäº†æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œ(message passing neural networkï¼Œ MPNN)ï¼Œç»Ÿä¸€äº†å„ç§å›¾ç¥ç»ç½‘ç»œå’Œå›¾å·ç§¯ç½‘ç»œæ–¹æ³•ã€‚ X. Wangç­‰äºº(X. Wang et. al. 2017)æå‡ºäº†éå±€éƒ¨ç¥ç»ç½‘ç»œ(non-local neural network, NLNN)ï¼Œå®ƒç»“åˆäº†å‡ ç§â€œself-attentionâ€é£æ ¼çš„æ–¹æ³•ã€‚ P. W. Battagliaç­‰äºº(P. W. Battaglia et. al. 2018)æå‡ºäº†å›¾ç½‘ç»œ(graph network, GN)ï¼Œå®ƒç»Ÿä¸€äº†ç»Ÿä¸€äº†MPNNå’ŒNLNNæ–¹æ³•ä»¥åŠè®¸å¤šå…¶ä»–å˜ä½“ï¼Œå¦‚äº¤äº’ç½‘ç»œ(Interaction Networks)ï¼Œç¥ç»ç‰©ç†å¼•æ“(Neural Physics Engine)ï¼ŒCommNetï¼Œstructure2vecï¼ŒGGNNï¼Œå…³ç³»ç½‘ç»œ(Relation Network)ï¼ŒDeep Setså’ŒPoint Netã€‚ å‡ ä¸ªå°šæœªè§£å†³çš„é—®é¢˜ å°½ç®¡GNNåœ¨ä¸åŒé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGNNæ¨¡å‹è¿˜ä¸èƒ½åœ¨ä»»ä½•æ¡ä»¶ä¸‹ï¼Œä¸ºä»»ä½•å›¾ä»»åŠ¡æä¾›ä»¤äººæ»¡æ„çš„è§£å†³æ–¹æ¡ˆã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†é™ˆè¿°ä¸€äº›å¼€æ”¾æ€§é—®é¢˜ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚ æµ…å±‚ç»“æ„ ä¼ ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œå¯ä»¥å †å æ•°ç™¾å±‚ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºæ›´æ·±çš„ç»“æ„å…·å¤‡æ›´å¤šçš„å‚æ•°ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒGNNæ€»æ˜¯å¾ˆæµ…ï¼Œå¤§å¤šæ•°ä¸è¶…è¿‡ä¸‰å±‚ã€‚ å®éªŒæ˜¾ç¤ºï¼Œå †å å¤šä¸ªGCNå±‚å°†å¯¼è‡´è¿‡åº¦å¹³æ»‘ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰é¡¶ç‚¹å°†æ”¶æ•›åˆ°ç›¸åŒçš„å€¼ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶äººå‘˜è®¾æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ä»ç„¶æ˜¯GNNçš„æœ€å¤§å±€é™æ‰€åœ¨ã€‚è®¾è®¡çœŸæ­£çš„æ·±åº¦GNNå¯¹äºæœªæ¥çš„ç ”ç©¶æ¥è¯´æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„æŒ‘æˆ˜ï¼Œå¹¶å°†å¯¹è¿›ä¸€æ­¥æ·±å…¥ç†è§£GNNåšå‡ºç›¸å½“å¤§çš„è´¡çŒ®ã€‚ åŠ¨æ€å›¾å½¢å¦ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ˜¯å¦‚ä½•å¤„ç†å…·æœ‰åŠ¨æ€ç»“æ„çš„å›¾å½¢ã€‚é™æ€å›¾æ€»æ˜¯ç¨³å®šçš„ï¼Œå› æ­¤å¯¹å…¶è¿›è¡Œå»ºæ¨¡æ˜¯å¯è¡Œçš„ï¼Œè€ŒåŠ¨æ€å›¾å¼•å…¥äº†å˜åŒ–çš„ç»“æ„ã€‚å½“è¾¹å’ŒèŠ‚ç‚¹å‡ºç°æˆ–æ¶ˆå¤±æ—¶ï¼ŒGNNä¸èƒ½è‡ªé€‚åº”åœ°åšå‡ºæ”¹å˜ã€‚ç›®å‰å¯¹åŠ¨æ€GNNçš„ç ”ç©¶ä¹Ÿåœ¨ç§¯æè¿›è¡Œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯ä¸€èˆ¬GNNçš„å…·å¤‡ç¨³å®šæ€§å’Œè‡ªé€‚åº”æ€§çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚ éç»“æ„æ€§åœºæ™¯ æˆ‘ä»¬è®¨è®ºäº†GNNåœ¨éç»“æ„åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°ä»åŸå§‹æ•°æ®ä¸­ç”Ÿæˆå›¾çš„æœ€ä½³æ–¹æ³•ã€‚åœ¨å›¾åƒåŸŸä¸­ï¼Œä¸€äº›ç ”ç©¶å¯ä»¥åˆ©ç”¨CNNè·å–ç‰¹å¾å›¾ï¼Œç„¶åå¯¹å…¶è¿›è¡Œä¸Šé‡‡æ ·ï¼Œå½¢æˆè¶…åƒç´ ä½œä¸ºèŠ‚ç‚¹ï¼Œè¿˜æœ‰çš„ç›´æ¥åˆ©ç”¨ä¸€äº›å¯¹è±¡æ£€æµ‹ç®—æ³•æ¥è·å–å¯¹è±¡èŠ‚ç‚¹ã€‚åœ¨æ–‡æœ¬åŸŸä¸­ï¼Œæœ‰äº›ç ”ç©¶ä½¿ç”¨å¥æ³•æ ‘ä½œä¸ºå¥æ³•å›¾ï¼Œè¿˜æœ‰çš„ç ”ç©¶é‡‡ç”¨å…¨è¿æ¥å›¾ã€‚å› æ­¤ï¼Œå…³é”®æ˜¯æ‰¾åˆ°å›¾ç”Ÿæˆçš„æœ€ä½³æ–¹æ³•ï¼Œä½¿GNNåœ¨æ›´å¹¿æ³›çš„é¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚ å¯æ‰©å±•æ€§é—®é¢˜ å¦‚ä½•å°†åµŒå…¥å¼ç®—æ³•åº”ç”¨äºç¤¾äº¤ç½‘ç»œæˆ–æ¨èç³»ç»Ÿè¿™ç±»å¤§è§„æ¨¡ç½‘ç»œç¯å¢ƒï¼Œæ˜¯å‡ ä¹æ‰€æœ‰å›¾å½¢åµŒå…¥ç®—æ³•é¢å¯¹çš„ä¸€ä¸ªè‡´å‘½é—®é¢˜ï¼ŒGNNä¹Ÿä¸ä¾‹å¤–ã€‚å¯¹GNNè¿›è¡Œæ‰©å±•æ˜¯å¾ˆå›°éš¾çš„ï¼Œå› ä¸ºæ¶‰åŠå…¶ä¸­çš„è®¸å¤šæ ¸å¿ƒæµç¨‹åœ¨å¤§æ•°æ®ç¯å¢ƒä¸­éƒ½è¦æ¶ˆè€—ç®—åŠ›ã€‚ è¿™ç§å›°éš¾ä½“ç°åœ¨å‡ ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œå›¾æ•°æ®å¹¶ä¸è§„åˆ™ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰è‡ªå·±çš„é‚»åŸŸç»“æ„ï¼Œå› æ­¤ä¸èƒ½æ‰¹é‡åŒ–å¤„ç†ã€‚å…¶æ¬¡ï¼Œå½“å­˜åœ¨çš„èŠ‚ç‚¹å’Œè¾¹æ•°é‡è¾¾åˆ°æ•°ç™¾ä¸‡æ—¶ï¼Œè®¡ç®—å›¾çš„æ‹‰æ™®æ‹‰æ–¯ç®—å­ä¹Ÿæ˜¯ä¸å¯è¡Œçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å‡ºï¼Œå¯æ‰©å±•æ€§çš„é«˜ä½ï¼Œå†³å®šäº†ç®—æ³•æ˜¯å¦èƒ½å¤Ÿåº”ç”¨äºå®é™…åœºæ™¯ã€‚ç›®å‰å·²ç»æœ‰ä¸€äº›ç ”ç©¶æå‡ºäº†è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•ï¼Œæˆ‘ä»¬æ­£åœ¨å¯†åˆ‡å…³æ³¨è¿™äº›æ–°è¿›å±•ã€‚ ç»“è®º åœ¨è¿‡å»å‡ å¹´ä¸­ï¼ŒGNNå·²ç»æˆä¸ºå›¾é¢†åŸŸæœºå™¨å­¦ä¹ ä»»åŠ¡çš„å¼ºå¤§è€Œå®ç”¨çš„å·¥å…·ã€‚è¿™ä¸€è¿›å±•æœ‰èµ–äºè¡¨ç°åŠ›ï¼Œæ¨¡å‹çµæ´»æ€§å’Œè®­ç»ƒç®—æ³•çš„è¿›æ­¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å›¾ç¥ç»ç½‘ç»œè¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚å¯¹äºGNNæ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒ‰å›¾ç±»å‹ã€ä¼ æ’­ç±»å‹å’Œè®­ç»ƒç±»å‹åˆ†ç±»çš„GNNå˜ä½“ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ€»ç»“äº†å‡ ä¸ªç»Ÿä¸€è¡¨ç¤ºä¸åŒGNNå˜ä½“çš„é€šç”¨æ¡†æ¶ã€‚åœ¨åº”ç”¨ç¨‹åºåˆ†ç±»æ–¹é¢ï¼Œæˆ‘ä»¬å°†GNNåº”ç”¨ç¨‹åºåˆ†ä¸ºç»“æ„åœºæ™¯ã€éç»“æ„åœºæ™¯å’Œå…¶ä»–18ä¸ªåœºæ™¯ï¼Œç„¶åå¯¹æ¯ä¸ªåœºæ™¯ä¸­çš„åº”ç”¨ç¨‹åºè¿›è¡Œè¯¦ç»†ä»‹ç»ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å››ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºäº†å›¾ç¥ç»ç½‘ç»œçš„ä¸»è¦æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ¨¡å‹æ·±åº¦ã€å¯æ‰©å±•æ€§ã€åŠ¨æ€å›¾å¤„ç†å’Œå¯¹éç»“æ„åœºæ™¯çš„å¤„ç†èƒ½åŠ›ã€‚ "},"paper/Perceptual_GAN_for_Small_Object_Detection.html":{"url":"paper/Perceptual_GAN_for_Small_Object_Detection.html","title":"Perceptual GAN for Small Object Detectioné˜…è¯»ç¬”è®°","keywords":"","body":"Perceptual GAN for Small Object Detectioné˜…è¯»ç¬”è®° ä½œè€…ï¼šå”é’ é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/83659247 æ¥æºï¼šçŸ¥ä¹ è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ Perceptual Generative Adversarial Networks for Small Object Detectionopenaccess.thecvf.com 1 æ‘˜è¦åŠä»‹ç»æ¦‚è¿° åŸºäºå°ç›®æ ‡ï¼Œæå‡ºäº†Perceptual GANç½‘ç»œæ¥ç”Ÿæˆå°ç›®æ ‡çš„è¶…åˆ†è¡¨è¾¾(super-resolved representation)ï¼ŒPerceptual GANåˆ©ç”¨å¤§å°ç›®æ ‡çš„ç»“æ„ç›¸å…³æ€§æ¥å¢å¼ºå°ç›®æ ‡çš„è¡¨è¾¾(represnetation)ï¼Œä½¿å…¶ä¸å…¶å¯¹åº”å¤§ç›®æ ‡çš„è¡¨è¾¾ç›¸ä¼¼ã€‚ Perceptual GANåˆ†ä¸ºä¸¤ä¸ªå­ç½‘ç»œï¼Œç”Ÿæˆå™¨(generator network) å’Œä¸€ä¸ªæ„ŸçŸ¥åˆ¤åˆ«å™¨( perceptual discriminator network)ã€‚åŸå§‹çš„GANçš„åˆ¤åˆ«å™¨ç”¨äºåˆ¤åˆ«ç”Ÿæˆå™¨ç”Ÿæˆçš„å›¾ç‰‡ä¸ºfake or tureï¼Œæœ¬æ–‡ä¸»è¦ä¿®æ”¹äº†åˆ¤åˆ«å™¨éƒ¨åˆ†ï¼Œä½¿åˆ¤åˆ«å™¨è¿˜èƒ½ä»ç”Ÿæˆçš„è¶…åˆ†å›¾å½¢ä¸­çš„æ£€æµ‹è·ç›Šé‡æ¥è®¡ç®—æŸå¤±å€¼(perceptual loss)ã€‚ ä½¿ç”¨çš„æ•°æ®åº“ä¸ºTsinghua-Tencent 100K(äº¤é€šæ ‡å¿—æ£€æµ‹) å’Œ the Caltech benchmark(è¡Œäººæ£€æµ‹) ã€‚ 2 Perceptual GANs Perceptual GANs å¯¹ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½è¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿ç”Ÿæˆå™¨èƒ½ç”Ÿæˆå°ç›®æ ‡çš„è¶…åˆ†è¡¨è¾¾ï¼Œåˆ¤åˆ«å™¨æ‹¥æœ‰ä¸¤ä¸ªloss(dversarial loss and perceptual loss )ã€‚ Figure 1.ã€€å…¬å¼ æ–‡ä¸­3.1ä¸­çš„å…¬å¼(æœ¬æ–‡ä¸­figure 1)è¡¨ç¤ºæœ¬æ–‡ç›®æ ‡æ˜¯è¦æœ€å°åŒ–Gå’Œæœ€å¤§åŒ–Dï¼Œå³ä½¿ç”¨Generatorç”Ÿæˆå°ç›®æ ‡çš„è¶…åˆ†å›¾(ä¸Groundtruthè¶Šè¿‘è¶Šå¥½)ï¼Œä½¿ç”¨Discriminatoråˆ¤åˆ«ç”Ÿæˆçš„è¶…åˆ†å›¾ä¸Groundtruth(ä½¿Discriminatorçº¦æ··æ·†è¶Šå¥½)ï¼Œè®©Discriminatorä¸èƒ½åˆ¤åˆ«ç”Ÿæˆçš„è¶…åˆ†å›¾æ˜¯åŸå›¾trueè¿˜æ˜¯ç”Ÿæˆå›¾fakeï¼Œä»è€Œè¾¾åˆ°ç”Ÿæˆå›¾å½¢ä¸åŸå›¾çš„æ— é™æ¥è¿‘ã€‚ Figure 2.ã€€å…¬å¼ æ–‡ä¸­ä½¿ç”¨å¤§ç›®æ ‡ ä½œä¸ºGroundtruthï¼Œå¸Œæœ›ä½¿ç”¨Generatorå°†å°ç›®æ ‡ç”Ÿæˆä¸ºå¤§ç›®æ ‡ã€‚åŸºäºæ­¤ï¼ŒFigure1å…¬å¼å˜ä¸ºFigure2å…¬å¼ã€‚ åˆ¤åˆ«å™¨è¾“å‡ºä¸ºä¸¤ä¸ªåˆ†æï¼š å¯¹æŠ—åˆ†æï¼šç”¨äºåŒºåˆ†ç”Ÿæˆçš„è¶…åˆ†è¡¨è¾¾ä¸å¤§ç›®æ ‡groudtruthå›¾ç‰‡ã€‚è®­ç»ƒåŠæ³•å’Œä¸€èˆ¬GANä¸­åˆ¤åˆ«å™¨ç±»ä¼¼ï¼Œlosså…¬å¼ä¸ºç« èŠ‚3.1å…¬å¼(3)ã€‚ æ„ŸçŸ¥åˆ†æï¼šç”¨äºjustifyç”Ÿæˆçš„è¡¨è¾¾å¯¹æ£€æµ‹ç²¾åº¦çš„è·ç›Šé‡ã€‚æ„ŸçŸ¥åˆ†æå…ˆéœ€æå‰ä½¿ç”¨å¤§ç›®æ ‡è¿›è¡Œè®­ç»ƒå¾—åˆ°pre-trainedæ¨¡å‹ï¼Œä½¿å…¶èƒ½äº§ç”Ÿç›¸å¯¹é«˜çš„æ£€æµ‹ç²¾åº¦ã€‚æ„ŸçŸ¥åˆ†æçš„è¾“å‡ºä¸ºä¸€ä¸ªå¤šä»»åŠ¡è¾“å‡º(classification+bounding-box regression)ã€‚ è®­ç»ƒæ—¶è½®æ¬¡è®­ç»ƒç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨ æŠŠæ‰€æœ‰çš„å®ä¾‹å¹³å‡åˆ†ä¸ºä¸¤ç±»ï¼šå¤§ç›®æ ‡å’Œå°ç›®æ ‡ã€‚é¦–å…ˆä½¿ç”¨å¤§ç›®æ ‡è®­ç»ƒåˆ¤åˆ«å™¨çš„æ„ŸçŸ¥åˆ†æï¼ŒåŸºäºå·²å­¦ä¹ è¿‡çš„æ„ŸçŸ¥åˆ†æï¼Œä½¿ç”¨å°ç›®æ ‡æ¥è®­ç»ƒç”Ÿæˆå™¨å’Œä½¿ç”¨å¤§å°ç›®æ ‡ä¸€èµ·æ¥è®­ç»ƒåˆ¤åˆ«å™¨çš„å¯¹æŠ—åˆ†æã€‚äº¤æ›¿åœ°æ‰§è¡Œç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç½‘ç»œçš„å¯¹æŠ—åˆ†æçš„è®­ç»ƒè¿‡ç¨‹ï¼Œç›´åˆ°æœ€ç»ˆè¾¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œå³ï¼šå¯ä»¥ä¸ºå°ç›®æ ‡ç”Ÿæˆç±»ä¼¼å¤§ç›®æ ‡çš„è¶…åˆ†ç‰¹å¾ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„æ£€æµ‹ç²¾åº¦ã€‚ Figure 3. æ•´ä¸ªPerceptual GAN ç»“æ„å›¾ã€‚ 2.1 Conditional Generator Network Architectureç”Ÿæˆå™¨ç½‘ç»œç»“æ„ è™šçº¿å·¦è¾¹æ˜¯ç”Ÿæˆå™¨ç»“æ„ï¼Œä¸Šé¢ä¸ºä¸€èˆ¬ConvNetç”¨äºç‰¹å¾æå–ï¼Œä¸‹é¢åŠ äº†ä¸€ç»„æ®‹å·®blockï¼Œç›®çš„æ˜¯è¾“å‡ºä¸€å¼ å›¾ï¼Œè®©å…¶å¯¹ä¸Šé¢ç½‘ç»œè¾“å‡ºçš„å°ç‰©ä½“ç‰¹å¾å›¾è¿›è¡Œè¡¥å……(Eltwise-Sum)ï¼Œç»„åˆç”ŸæˆSuper-Resolved Featureã€‚ 2.2 Discriminator Network Architecture åˆ¤åˆ«å™¨ç½‘ç»œç»“æ„ è™šçº¿å³è¾¹æ˜¯åˆ¤åˆ«å™¨ç»“æ„ï¼Œå…¶ä¸Šä¸‹ä¸¤éƒ¨åˆ†åˆ«ä¸ºå¯¹æŠ—åˆ†æä¸æ„ŸçŸ¥åˆ†æã€‚å¯¹æŠ—åˆ†æç»“æ„ä¸ä¸€èˆ¬classification(fake/true)ç½‘ç»œç±»ä¼¼ï¼Œæ„ŸçŸ¥åˆ†æä»å…¨è¿æ¥å¼€å§‹åˆåˆ†ä¸ºå¦å¤–ä¸¤ä¸ªåˆ†æï¼Œåˆ†åˆ«è¾“å‡ºclassificationå’ŒBounding boxã€‚åˆ¤åˆ«å™¨ç½‘ç»œçš„æŸå¤±ä¸ºä¸Šä¸‹ä¸¤æçš„æŸå¤±å„è‡ªä¹˜ä»¥æƒé‡åçš„å’Œï¼Œæœ¬æ–‡ä¸­è¿™ä¸¤æƒé‡éƒ½è®¾ä¸º1ã€‚ Figure 4ä¸ºåˆ¤åˆ«å™¨å¯¹æŠ—æŸå¤±ï¼Œæœ€å°åŒ– ï¼Œå³ï¼Œä½¿ )) æ¥è¿‘1ã€‚ Figure 4. å…¬å¼ åˆ¤åˆ«å™¨æ„ŸçŸ¥æŸå¤±å’Œä¸€èˆ¬detectionç»“æ„ç±»ä¼¼ã€‚ "},"paper/GMMN.html":{"url":"paper/GMMN.html","title":"GANå˜ä½“-GMMN ç½‘ç»œ","keywords":"","body":"å°±æ˜¯ä¸GANâ€”â€”ç”Ÿæˆå¼çŸ©(Moment)åŒ¹é…ç½‘ç»œGMMN é¾™è…¾ åœ¨GANå‘å±•çƒ­ç«æœå¤©çš„å½“ä¸‹ï¼Œç”Ÿæˆæ¨¡å‹çš„å…¶ä»–æ–¹å‘æ˜¾å¾—æœ‰äº›åŠ¿å•åŠ›è–„ã€‚GANçš„ä¸€ä¸ªè–„å¼±ç¯èŠ‚åœ¨äºMin-max Gameé—®é¢˜çš„ä¸æ˜“ä¼˜åŒ–ï¼Œè¯¥é—®é¢˜åœ¨WGANä¸­å¾—åˆ°äº†è¾ƒå¥½çš„è§£å†³ã€‚WGANæ ¸å¿ƒæ€æƒ³å³æŠŠåˆ¤åˆ«å™¨çš„Lossæ›¿æ¢ä¸ºè¡¡é‡çœŸå®æ•°æ®åˆ†å¸ƒ ä¸ç”Ÿæˆæ•°æ®åˆ†å¸ƒ çš„åˆ†å¸ƒå·®å¼‚â€”â€”Wassersteinè·ç¦»ã€‚ äº‹å®ä¸Šï¼Œè¿™ç§æ€è·¯å¹¶éé¦–å…ˆç”±WGANæå‡ºï¼Œåœ¨2015å¹´çš„ICMLä¸Šï¼Œå°±æœ‰äººç”¨ä¼˜åŒ–MMD(maximum mean discrepancy,ä¸€ç§åº¦é‡ä¸¤ä¸ªæ•°æ®é›†çš„å·®å¼‚çš„åº¦é‡)çš„æ–¹å¼æå‡ºäº†ä¸åŒäºGANçš„ç”Ÿæˆå¼æ¨¡å‹â€”â€”Generative Moment Matching Networksã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸€ä¸ª(å¤šå…ƒå‡åŒ€åˆ†å¸ƒä¸Šçš„)éšæœºé‡‡æ ·Sampleä½œä¸ºè¾“å…¥ï¼Œå°†ç»è¿‡è‹¥å¹²éçº¿æ€§å±‚ä¹‹åçš„è¾“å‡ºä½œä¸ºç”Ÿæˆçš„æ ·æœ¬ã€‚ æœ¬æ–‡çš„è´¡çŒ®æœ‰äºŒï¼š1.æå‡ºäº†åŸºäºMMDä¼˜åŒ–çš„GMMNï¼Œ2.é’ˆå¯¹GMMNå¯èƒ½å­˜åœ¨çš„é—®é¢˜(é«˜ç»´æ•°æ®éš¾ä»¥è¡¨ç°)ç»™å‡ºäº†ä¸€ä¸ªåŸºäºAuto-Encoderçš„æ”¹è¿›æ–¹æ¡ˆGMMN-AEã€‚å…¶ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š GMMNçš„è´¡çŒ®åœ¨äºï¼šä½¿ç”¨äº†MMDä½œä¸ºæŸå¤±å‡½æ•°æ˜“äºä¼˜åŒ–ï¼Œé¿å…äº†éš¾è§£çš„Min-maxé—®é¢˜ã€‚MMDè¡¡é‡ä¸¤ç»„æ ·æœ¬ å’Œ åœ¨æŸä¸ªç©ºé—´Sä¸­çš„åˆ†å¸ƒå·®å¼‚ã€‚å¦‚æœæŸä¸ªç©ºé—´Sä¸­åŒ…å«äº† å’Œ çš„åˆ†å¸ƒä¿¡æ¯ã€‚é‚£ä¹ˆæœ€å°åŒ–MMDå°±ç­‰ä»·äºæ‹Ÿåˆä¸¤ä¸ªåˆ†å¸ƒã€‚ æœ¬æ–‡ä½¿ç”¨MMDçš„å¹³æ–¹ä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œå¦‚ä¸‹å¼(1)æ‰€ç¤ºï¼š (è¿™é‡Œçš„ æ˜¯ä¸€ä¸ªå†ç”Ÿæ ¸å˜æ¢ï¼Œå°†æ ·æœ¬ä»åŸç©ºé—´æ˜ å°„åˆ°æŸä¸ªç©ºé—´Sï¼Œæˆ‘ä»¬é€‰æ‹©çš„å‡½æ•° å†³å®šäº†ç›®æ ‡ç©ºé—´Sã€‚å†ç”Ÿæ ¸å˜æ¢çš„ç†è®ºæ€§è´¨å‚è§è§†é¢‘è¯¾ç¨‹ï¼šå†ç”Ÿæ ¸å˜æ¢ä¸æ ¸æŠ€å·§) ä½¿ç”¨MMDçš„å¹³æ–¹è€ŒéMMDæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦åˆ©ç”¨æ ¸æŠ€å·§ã€‚ï¼ˆæ ¸æŠ€å·§ï¼šæœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œæ ¸æŠ€å·§ä½¿å¾—æˆ‘ä»¬æˆ‘ä»¬ä¸ç”¨æ˜¾å¼åœ°çŸ¥é“å˜æ¢åçš„æ ·æœ¬ çš„å½¢å¼ï¼Œç”šè‡³ä¸éœ€è¦çŸ¥é“ çš„å½¢å¼ã€‚å› è€Œ å…·æœ‰å¾ˆé«˜çµæ´»æ€§ã€‚ä½¿ç”¨æ ¸æŠ€å·§åªéœ€è¦çŸ¥é“ç›®æ ‡ç©ºé—´ä¸­æ ·æœ¬çš„å†…ç§¯ ï¼Œè€Œæ— éœ€çŸ¥é“ç›®æ ‡ç©ºé—´çš„å…¶ä»–æ€§è´¨ã€‚ï¼‰ åŸºäºæ ¸æŠ€å·§ï¼Œ(1)å¼å±•å¼€ä¸ºï¼š è€ƒè™‘ æ˜¯æ’ç­‰å˜æ¢çš„æƒ…å†µ ä¸éš¾å‘ç°(1)å¼è®¡ç®—çš„æ˜¯æ ·æœ¬å‡å€¼ã€‚å½“ æ˜¯äºŒæ¬¡å˜æ¢æ—¶ï¼Œ(1)å¼åˆ™è®¡ç®—äº† å’Œ çš„äºŒé˜¶çŸ©ã€‚å½“ åŒ…å«å„æ¬¡é¡¹çš„æ—¶å€™ï¼Œ(1)å¼å°±è®¡ç®—äº†å„é˜¶çŸ©ï¼Œæ­¤æ—¶ï¼Œæœ€å°åŒ–(1)å¼å°±ç›¸å½“äºåŒ¹é… å’Œ çš„å„é˜¶çŸ©ï¼Œå› è€Œç­‰æ•ˆäºåŒ¹é…äº†ä¸¤ä¸ªåˆ†å¸ƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ¬æ–‡è¢«ç§°ä½œçŸ©åŒ¹é…ç½‘ç»œã€‚ æœ¬æ–‡ä½¿ç”¨ çš„ æ˜¯é«˜æ–¯å˜æ¢ï¼š é«˜æ–¯å‡½æ•°èƒ½å¤Ÿå±•å¼€ä¸ºæ— ç©·çº§æ•°ï¼Œå› æ­¤å¯ä»¥æ»¡è¶³(1)å¼è®¡ç®—å„é˜¶çŸ©çš„éœ€æ±‚ã€‚ æ ‡è®°ç”Ÿæˆæ ·æœ¬ä¸º ï¼Œæ•°æ®æ ·æœ¬ä¸º ï¼Œæˆ‘ä»¬å®¹æ˜“è®¡ç®—(1)å¯¹ç”Ÿæˆæ ·æœ¬çš„ç¬¬ ç»´çš„æ¢¯åº¦ åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œæ¢¯åº¦å¯ä»¥ä¸€ç›´æ²¿ç€ç½‘ç»œåå‘ä¼ æ’­ã€‚å› æ­¤ç½‘ç»œæ˜¯å¯è®­ç»ƒçš„ã€‚ GMMNå­˜åœ¨ä¸€ä¸ªéš¾ç‚¹ï¼šç›´æ¥ç”Ÿæˆé«˜ç»´æ•°æ®è¾ƒä¸ºå›°éš¾ï¼Œå› ä¸ºï¼š é«˜ç»´çš„å›¾åƒå®é™…ä¸Šå­˜åœ¨ä½ç»´æµå½¢ã€‚ é«˜ç»´æ•°æ®ä¸æ˜“å¤„ç† å› æ­¤ï¼Œæœ¬æ–‡æå‡ºGMMN+AEè¿›è¡Œæ”¹è¿›ã€‚è¯¥æ¨¡å‹å°†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹æ‰“ç ´ä¸ºä¸¤éƒ¨åˆ†ï¼ŒAEè´Ÿè´£ç”Ÿæˆé«˜ç»´æ•°æ®çš„ä½ç»´æµå½¢ï¼Œå³ä¸­é—´è¡¨ç¤ºå±‚representationï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå›¾åƒé‡æ„ã€‚è€ŒGMMNç”Ÿæˆçš„å†…å®¹æ˜¯representationï¼Œè€Œéå›¾åƒï¼Œè¿™ä¸ªè¿‡ç¨‹å¦‚å›¾1.(b)æ‰€ç¤ºã€‚ å®é™…åº”ç”¨ä¸­ï¼ŒGMMN+AEçš„è®­ç»ƒåˆ†æˆä¸‰æ­¥å®Œæˆï¼š 1.é€å±‚é¢„è®­ç»ƒAE 2.Finetune AE 3.è®­ç»ƒGMMN åœ¨åé¢çš„å®éªŒéƒ¨åˆ†æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŠ å…¥AEæå¤§çš„æå‡äº†GMMNçš„è¡¨ç°ã€‚ æœ¬æ–‡è¿˜æŒ‡å‡ºäº†ä¸€äº›GMMNå¸¸è§çš„è®­ç»ƒæŠ€å·§ï¼ŒåŒ…æ‹¬ï¼š å¯¹AEçš„å„ä¸ªEncoderå±‚æ–½åŠ Drop-outã€‚ ä½¿ç”¨é›†æˆæ–¹æ³•æ¥é›†æˆä¸åŒ çš„é«˜æ–¯å‡½æ•°(å®éªŒä¸­MNISTå– [2,5,10,20,40,80]å…­ä¸ªæ ¸å¹³å‡ï¼ŒTFDå–[5,10,20,40,80,160]å…­ä¸ªæ ¸å¹³å‡)ï¼š \\3. ä½¿ç”¨MMDçš„å¹³æ–¹æ ¹æ¥ç›‘ç£è®­ç»ƒï¼Œä»¥è·å¾—è‡ªé€‚åº”å­¦ä¹ ç‡çš„æ•ˆæœ(MMDè¾ƒå°æ—¶æ¢¯åº¦ä¸æ¶ˆå¤±), \\4. Mini-batchè®­ç»ƒï¼šæ ¸æŠ€å·§çš„å¼±ç‚¹æ˜¯ï¼šéœ€è¦æ„é€ æ ¸çŸ©é˜µ(å³ç›®æ ‡ç©ºé—´çš„ä¸­æ ·æœ¬çš„ä¸¤ä¸¤ä¹‹é—´çš„å†…ç§¯)ï¼Œå› æ­¤ï¼Œæ ¸çŸ©é˜µçš„è§„æ¨¡æ˜¯n^2ã€‚æ‰€ä»¥ä¸èƒ½å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ã€‚GMMNä½¿ç”¨Mini-batchçš„æ–¹å¼æ¥è§„é¿æ ¸çŸ©é˜µè¿‡å¤§çš„é—®é¢˜ã€‚å…¶ç†ç”±æ˜¯â€”â€”ä¸€ä¸ªBatchçš„æ•°æ®èƒ½å¤Ÿè¿‘ä¼¼æ•°æ®æ•´ä½“çš„åˆ†å¸ƒã€‚å®é™…å®ç°ä¸­ä½¿ç”¨çš„Batch-sizeä¸º1000ã€‚ æ ·æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼šåœ¨GMMNä¸­ï¼Œä»å¤šå…ƒå‡åŒ€åˆ†å¸ƒé‡‡æ ·ä¸€ä¸ªæ ·æœ¬ä½œä¸ºç½‘ç»œè¾“å…¥ï¼Œè¿›è¡Œä¸€è½®å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°çš„è¾“å‡ºå³ä¸ºç”Ÿæˆå›¾åƒæ ·æœ¬ã€‚åœ¨GMMN-Aä¸­ï¼Œä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·ä¸€ä¸ªæ ·æœ¬ä½œä¸ºç½‘ç»œè¾“å…¥ï¼Œè¿›è¡Œä¸€è½®å‰å‘ä¼ æ’­å¾—åˆ°çš„æ˜¯å›¾åƒçš„representationã€‚è¯¥representationå†ç»è¿‡AEçš„è§£ç å±‚ç”Ÿæˆå›¾åƒã€‚ å®éªŒï¼šç”Ÿæˆæ¨¡å‹çš„ç ”ç©¶ä¸­ï¼Œå¸¸ç”¨çš„è¡¡é‡æ ‡å‡†æ˜¯ï¼šä½¿ç”¨ç”Ÿæˆåˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„log-likelihoodåˆ¤å®šç”Ÿæˆæ¨¡å‹æ•ˆæœï¼Œæœ¬æ–‡å¯¹æ¯”æ–¹æ³•åŒ…æ‹¬: æ·±åº¦ä¿¡å¿µç½‘ç»œDBN Contractive stacked auto-encoder Deep Generative Stochastic Network GAN GMMN GMMN+AE ä½¿ç”¨çš„æ•°æ®é›†ä¸ºï¼šMNIST å’Œ TFDï¼ˆå¤šä¼¦å¤šäººè„¸ï¼‰ å…¶å®šé‡çš„ç»“æœä¸ºï¼š å®šæ€§çš„ç»“æœå¦‚ä¸‹å›¾ï¼š(a)-(d)ç”Ÿæˆçš„ç»“æœ (e)-(f)ç”Ÿæˆæ•°æ®ä»¥åŠç”Ÿæˆçš„å›¾åƒåœ¨çœŸå®æ•°æ®ä¸­çš„æœ€è¿‘é‚» ç”Ÿæˆæ¨¡å‹çš„ä¸€ä¸ªé£é™©æ˜¯ï¼Œä»…ä»…è®°ä½äº†æ•°æ®é›†è€ŒéçœŸæ­£çš„å­¦ä¼šäº†æ•°æ®åˆ†å¸ƒï¼Œä¸ºæ­¤ä½œè€…ç»™å‡ºçš„ä¸¤ç‚¹è¯æ®ï¼š ä¸Šå›¾(e)-(h)å¯ä»¥çœ‹å‡ºç”Ÿæˆæ•°æ®å’ŒçœŸå®æ•°æ®é›†ä¸­çš„æœ€è¿‘é‚»æœ‰ç•¥å¾®å·®å¼‚ã€‚ ç”¨å‡åŒ€åˆ†å¸ƒä¸­çš„5ä¸ªç‚¹ç”Ÿæˆçš„5å¼ å›¾åƒï¼ˆä¸‹å›¾çº¢æ¡†ï¼‰ï¼›å†ä»¥å‡åŒ€åˆ†å¸ƒä¸Šï¼Œè¿™5ä¸ªç‚¹ä¹‹é—´çš„å…¶ä»–ä½ç½®é‡‡æ ·åˆ°çš„ç‚¹ï¼ˆé‡‡æ ·æ–¹å¼ä¸ºçº¿æ€§æ’å€¼ï¼‰ä½œä¸ºè¾“å…¥ç”Ÿæˆçš„å›¾åƒã€‚ æœ¬æ–‡çš„å¯ç¤ºï¼š æœ¬æ–‡å¯¹Related Workçš„æ¢è®¨éå¸¸å…¨é¢ï¼Œåˆ†æäº†å…¶ä»–ç”Ÿæˆæ¨¡å‹çš„ç‰¹ç‚¹å’Œä¸è¶³ï¼š Boltzmanç³»åˆ—çš„æ¨¡å‹ï¼Œä¸»è¦é—®é¢˜æ˜¯MCMCè¿‡ç¨‹éš¾ä»¥ä¼˜åŒ– ä¿¡å¿µç½‘ç»œç³»åˆ—æ¨¡å‹ï¼Œä¸»è¦é—®é¢˜æ˜¯å¯¹éšå˜é‡é¡ºåºæœ‰æ‰€å‡è®¾ï¼Œä¸çœŸå®ä¸–ç•Œä¸ç¬¦ åŸºäºAuto Encoderé‡æ„æ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹ï¼Œä¹Ÿéœ€è¦MCMCä¼˜åŒ–ã€‚ å˜åˆ†ç½‘ç»œç³»åˆ—ï¼Œéœ€è¦é¢å¤–ä¿¡æ¯ã€‚ \\2. æœ¬æ–‡çš„å†™ä½œå¾ˆå€¼å¾—å­¦ä¹ ï¼Œä½œè€…åœ¨å„ä¸ªç« èŠ‚éƒ½åå¤å¼ºè°ƒäº†MMDå¯¹äºMini-maxçš„ä¼˜ç‚¹ï¼Œä½¿å¾—å„ä¸ªSectionç›¸å¯¹ç‹¬ç«‹ï¼Œé™ä½äº†å®¡ç¨¿äººé˜…è¯»çš„éš¾åº¦ã€‚ \\3. æœ¬æ–‡çš„æœ«å°¾æ¢è®¨äº†è®¸å¤šåŸºäºMMDçš„å¯èƒ½çš„æ”¹è¿›æ–¹å‘ã€‚ \\4. æœ¬æ–‡çš„å¼€æºä»£ç å’ŒTensorflowç‰ˆæœ¬ \\4. æœ¬æ–‡çš„ä¸è¶³ï¼šBatch_size 1000çš„æ—¶å€™æ‰èƒ½åšåˆ°ä¸€ä¸ªbatchèƒ½å¤Ÿè¿‘ä¼¼æ•°æ®åˆ†å¸ƒï¼Œè¿™æ˜¯å¯¹äºMNISTå’ŒTFDè¿™æ ·çš„å°æ•°æ®é›†è€Œè¨€çš„ï¼Œå¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚ "},"paper/Deformable_Kernels.html":{"url":"paper/Deformable_Kernels.html","title":"å›¾åƒè§†é¢‘å»å™ªä¸­çš„Deformable Kernels","keywords":"","body":"ã€è®ºæ–‡è§£è¯»ã€‘å›¾åƒ/è§†é¢‘å»å™ªä¸­çš„Deformable Kernels CVè·¯ä¸Šä¸€åç ”ç©¶åƒ§ ä¸œå—å¤§å­¦ ç”µå­ä¸é€šä¿¡å·¥ç¨‹ç¡•å£«åœ¨è¯» 1. ç®€ä»‹ è¿™æ˜¯ä¸€ç¯‡æºè‡ªå•†æ±¤(SenseTime)çš„è®ºæ–‡ï¼Œæ–‡ç« é¢˜ç›®\"Learning Deformable Kernels for Image and Video Denoising\"ã€‚ä¸KPNã€MKPN(ä¼ é€é—¨)ç›¸ä¼¼ï¼Œä¹Ÿæ˜¯ä¸€ç§åŸºäºæ ¸é¢„æµ‹çš„å»å™ªæ¨¡å‹ï¼Œæ•´ä½“ä¸Šï¼Œç½‘ç»œç»“æ„ä¹Ÿæ¯”è¾ƒç›¸ä¼¼ï¼Œä¸åŒç‚¹åœ¨äºï¼Œé¢„æµ‹å‡ºçš„Kernelså¯¹äºå‘¨å›´åƒç´ ç‚¹çš„æƒé‡ï¼Œä»¥åŠå‘¨å›´åƒç´ ç‚¹çš„é€‰æ‹©æ–¹å¼ã€‚æœ¬æ–‡æºç å·²å¼€æºè‡³Githubï¼Œæ¬¢è¿Start/Fork~ https://github.com/z-bingo/Deformable-Kernels-For-Video-Denoisinggithub.com ä»BM3Dè¿™ç±»ç»å…¸çš„ä¼ ç»Ÿå›¾åƒå»å™ªæ–¹å¼èµ·ï¼Œä¸åŒæ–¹æ³•ä¹‹é—´çš„æœ€å¤§åŒºåˆ«å°±åœ¨äºå¦‚ä½•ç”¨ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼é€‰æ‹©åˆé€‚çš„åƒç´ ç‚¹ã€ä»¥åŠå¦‚ä½•å®šä¹‰è¿™äº›åƒç´ ç‚¹çš„æƒé‡ï¼Œè¿›è€Œå¯¹è¿™äº›åƒç´ ç‚¹åŠ æƒå¹³å‡å¯å¾—åˆ°è¿‘ä¼¼â€œå¹²å‡€â€çš„å›¾åƒï¼Œä¹Ÿå°±è¾¾åˆ°äº†å›¾åƒå»å™ªçš„ç›®çš„ã€‚ å°±KPNå’ŒMKPNè€Œè¨€ï¼Œç½‘ç»œçš„è¾“å‡ºæ˜¯per-pixelçš„è‡ªé€‚åº”å·ç§¯æ ¸ï¼ŒKPNä»…é¢„æµ‹å•ä¸€å°ºå¯¸çš„å·ç§¯æ ¸ï¼›è€ŒMKPNå¯ä»¥åŒæ—¶é¢„æµ‹å¤šä¸ªå°ºå¯¸çš„å·ç§¯æ ¸ï¼Œè¿›è€Œé€‚åº”å«å™ªå›¾åƒä¸­ç‰©ä½“çš„ä¸åŒå°ºåº¦ï¼Œè¾¾åˆ°æ—¢å¯ä»¥è¾ƒå¥½åœ°ä¿ç•™ç»†èŠ‚ã€åˆå¯ä»¥å¹³æ»‘å¹³å¦åŒºåŸŸçš„ç›®çš„ã€‚è¿™ç¯‡æ–‡ç« æå‡ºçš„DKPNï¼ˆæš‚ä¸”ç§°ä¹‹ä¸ºDKPNï¼‰ï¼Œäº¦æ˜¯å¦‚æ­¤ï¼Œé¢„æµ‹å‡ºper-pixelçš„è‡ªé€‚åº”å·ç§¯æ ¸ï¼Œä½†æ˜¯å¯¹äºå¯å˜å½¢å·ç§¯æ¥è¯´ï¼Œä¸ä»…åŒ…å«weightså’Œbiasä¸¤éƒ¨åˆ†ï¼Œè¿˜ä¼šåŒ…å«offsetsï¼Œç”¨äºæŒ‡ç¤ºâ€œå‘¨å›´åƒç´ â€æ˜¯æŒ‡å“ªäº›åƒç´ ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿå·ç§¯è¿ç®—å¯¹äºæ–¹å½¢é¢†åŸŸçš„å®šä¹‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨é‡‡æ ·ç‚¹æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå°½å¯èƒ½åœ°å¢å¤§kernelsçš„æ„Ÿå—é‡ï¼Œæœ‰åˆ©äºé«˜æ•ˆåœ°åˆ©ç”¨å›¾åƒä¸­çš„ä¿¡æ¯ã€‚Deformable Kernelså’Œä¼ ç»Ÿçš„Kernelså¼‚åŒå¯é€šè¿‡ä¸‹å›¾ä½“ç°ï¼š â€‹ Deformable Kernels 2. Deformable Convolution æœ¬èŠ‚é€šè¿‡å°½å¯èƒ½ç®€çŸ­ã€ç®€æ´çš„è¯­è¨€æ¥ä»‹ç»2Dä»¥åŠ3Då¯å˜å½¢å·ç§¯çš„åŸç†ï¼Œä»¥åŠå·¥ä½œæ–¹å¼ã€‚ 2.1 2D Deformable Convolution å¯¹äºä¸€ä¸ªå·²çŸ¥çš„å›¾åƒXï¼Œåœ¨å»å™ªä»»åŠ¡ä¸­å¯è®¤ä¸ºæ˜¯å«å™ªå£°çš„å›¾åƒï¼Œé‚£ä¹ˆï¼Œå·ç§¯æ ¸åœ¨è¯¥å›¾åƒä¸Šå·ç§¯å¯è¡¨ç¤ºä¸º å…¶ä¸­ï¼Œ è¡¨ç¤ºåœ¨å›¾åƒä¸­çš„åæ ‡ï¼Œ è¡¨ç¤ºå·ç§¯æ ¸çš„ä¸ªæ•°ï¼Œ è¡¨ç¤ºå·ç§¯æ ¸çš„é‡‡æ ·åç§»ç‚¹ï¼Œå¯¹äºä¼ ç»Ÿçš„æ–¹å½¢é¢†åŸŸï¼Œç”± æ„æˆçš„é›†åˆä¸ºï¼ˆä»¥ çš„å·ç§¯æ ¸ä¸ºä¾‹)ï¼š åŸºäºä¼ ç»Ÿå·ç§¯ç†è®ºï¼Œå¯å˜å½¢å·ç§¯åœ¨é‡‡æ ·åç§»ç‚¹ä¸Šé¢å¤–å åŠ äº†ä¸€ä¸ªå¯ä»¥å­¦ä¹ çš„offsetså‚æ•°ï¼Œå°†è§„åˆ™é‡‡æ ·é—®é¢˜å˜ä¸ºäº†ä¸€ä¸ªä¸è§„åˆ™é‡‡æ ·é—®é¢˜ï¼Œè‹¥å°†offestsè¡¨ç¤ºä¸º ,2Då¯å˜æ€§å·ç§¯å¯è¡¨ç¤ºä¸ºï¼š åœ¨æ­¤ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜éœ€è¦æ³¨æ„ï¼Œç”±äºoffsetsæ˜¯å­¦ä¹ å¾—åˆ°çš„åç§»é‡ï¼Œå› æ­¤ï¼Œå…¶ä¸€èˆ¬ä¸ä¼šæ˜¯æ•´æ•°ï¼Œè€Œæ˜¯å°æ•°ï¼Œæ„å‘³ç€è¦é‡‡æ ·çš„ç‚¹ä¸å¤„äºè§„åˆ™çš„åƒç´ ä¸Šã€‚æ­¤æ—¶ï¼Œå°±è¦é€šè¿‡åŒçº¿æ€§æ’å€¼ç­‰æ’å€¼ç®—æ³•æ ¹æ®è§„åˆ™çš„åƒç´ ç‚¹è¿›è¡Œæ’å€¼ï¼Œå¾—åˆ°æƒ³è¦åæ ‡ç‚¹çš„åƒç´ å€¼ã€‚ 2.2 3D Deformable Convolution ä¸2Då¯å˜å½¢å·ç§¯ç›¸ä¼¼ï¼Œ3Då¯å˜æ€§å·ç§¯å°±æ˜¯3Då·ç§¯çš„æ‹“å±•ï¼Œåœ¨3Då·ç§¯çš„åŸºç¡€ä¸Šæ·»åŠ ä¸‰ä¸ªå¯å­¦ä¹ çš„offsetsã€‚ä¼ ç»Ÿ3Då·ç§¯ä¸€èˆ¬å¯ç”¨äºVolumeæ•°æ®çš„å¤„ç†ä¸­ï¼Œå¯¹åº”äºå»å™ªé¢†åŸŸï¼Œå³å¤šå¸§å›¾åƒå»å™ªæˆ–è§†é¢‘å»å™ªï¼Œæ­¤æ—¶ç½‘ç»œçš„è¾“å…¥å¯ä»¥çœ‹åš ï¼Œåˆ†åˆ«è¡¨ç¤ºbatch_sizeï¼Œè¾“å…¥å›¾åƒçš„æ•°é‡æˆ–è§†é¢‘çš„å¸§æ•°ï¼Œé¢œè‰²é€šé“æ•°ä»¥åŠé•¿å’Œå®½ï¼Œ3Då·ç§¯ä¼šä½œç”¨äºé¢œè‰²é€šé“ã€é•¿å’Œå®½ä¸‰ä¸ªç»´åº¦ã€‚é‚£ä¹ˆï¼Œå¯¹äº3Då¯å˜æ€§å·ç§¯æ¥è¯´ï¼Œé™¤äº†é•¿å’Œå®½ä¸¤ä¸ªç»´åº¦çš„offsetsä¹‹å¤–ï¼Œç¬¬ä¸‰ä¸ªoffsetså°±æˆäº†å¸§ä¸å¸§ä¹‹é—´çš„åç§»ï¼Œè¿™æ ·å°±ä¼šæ›´æœ‰åˆ©ä¸ç½‘ç»œåœ¨ä¸åŒçš„å¸§ä¹‹é—´æå–æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¯¹äºå……åˆ†åˆ©ç”¨è¿ç»­çš„è§†é¢‘å¸§ä¹‹é—´çš„å†—ä½™ä¿¡æ¯æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚3Då¯å˜å½¢å·ç§¯å¯è¡¨ç¤ºä¸ºï¼š å¯¹äºéè§„åˆ™çš„é‡‡æ ·ç‚¹ï¼Œåœ¨ä¸‰ç»´ç©ºé—´ä¸­é€šå¸¸é€šè¿‡ä¸‰çº¿æ€§æ’å€¼æ¥å®ç°ã€‚ 3. DKPNç½‘ç»œç»“æ„ DKPNç½‘ç»œç»“æ„å›¾ ä¸Šå›¾æ˜¯DKPNç½‘ç»œç»“æ„å›¾ï¼Œå…¶ä¸»å¹²ç½‘ç»œä¹Ÿæ˜¯ä¸€ä¸ªåŸºäºU-Netçš„Encoder-Decoderç»“æ„ï¼Œç”±äºå¯å˜æ€§å·ç§¯å¤šäº†é¢å¤–çš„offsetså‚æ•°éœ€å­¦ä¹ ï¼Œä¸KPNä¸åŒï¼ŒU-Netçš„è¾“å‡ºä¸å†æ˜¯è‡ªé€‚åº”å·ç§¯æ ¸çš„æƒé‡ï¼Œå¯æ˜¯offsetsï¼›offsetsç»è¿‡Sampleré‡‡æ ·åï¼Œä¸è¾“å…¥çš„å¤šå¸§å«å™ªå£°å›¾åƒconcatåˆ°ä¸€èµ·ï¼Œå†ç»è¿‡å‡ ä¸ªå·ç§¯å±‚åå¯å¾—åˆ°è‡ªé€‚åº”å·ç§¯æ ¸çš„weightsã€‚æ­¤æ—¶ï¼Œå°†åˆšåˆšé‡‡æ ·å¾—åˆ°çš„å›¾åƒä¸weigthsç›¸ä¹˜ä¾¿å¯å¾—åˆ°å»å™ªåå›¾åƒã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“å»å™ªä»»åŠ¡ä¸ºå•å¸§å›¾åƒå»å™ªæ—¶ï¼Œæ¯ä¸ªåƒç´ ç‚¹æœ‰ä¸¤ä¸ªéœ€è¦éœ€è¦çš„offsetï¼Œè€Œè§†é¢‘å»å™ªæ—¶æœ‰ä¸‰ä¸ªoffsetséœ€è¦å­¦ä¹ ã€‚ KPNä¸­ï¼Œä½œè€…é‡‡ç”¨äº†é€€ç«é¡¹ä½œä¸ºLosså‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œå•ç‹¬ä¸ºæ¯å¸§è¾“å…¥å›¾åƒé¢„æµ‹å»å™ªåçš„å›¾åƒï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢ç½‘ç»œå¾ˆå¿«æ”¶æ•›è‡³ä¸€ä¸ªå±€éƒ¨æå°å€¼ï¼Œä½¿å¾—å‚è€ƒå¸§ä¹‹å¤–çš„å›¾åƒå¸§åœ¨è¾“å‡ºå»å™ªå›¾åƒä¸­å‡ ä¹ä¸èµ·ä½œç”¨ã€‚ç›¸ä¼¼çš„æ€æƒ³åœ¨DKPNä¸­ä¹Ÿå¾—åˆ°äº†ä½¿ç”¨ï¼Œå¯¹äºè¾“å‡ºçš„Nä¸ªé‡‡æ ·ç‚¹ï¼ŒDKPNä¼šå°†å…¶åˆ†ä¸ºsä¸ªç»„ï¼Œæ¯ä¸ªç»„ç›¸äº’ç‹¬ç«‹åœ°é¢„æµ‹å»å™ªåçš„å¹²å‡€å›¾åƒï¼Œå¹¶ä½œä¸ºlosså‡½æ•°çš„é€€ç«é¡¹ï¼Œè¿™æ ·å°±å¯ä»¥æœ‰æ•ˆé˜²æ­¢ç½‘ç»œå¾ˆå¿«æ”¶æ•›è‡³å±€éƒ¨æœ€å°å€¼ã€‚å°†è¾“å…¥å›¾åƒåºåˆ—ç»è¿‡Sampleråçš„å›¾åƒåˆ†ä¸ºsç»„ï¼Œæ¯ç»„å¹³å‡åŒ…å« ä¸ªé‡‡æ ·ç‚¹ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º ï¼Œé‚£ä¹ˆï¼Œç›¸äº’ç‹¬ç«‹çš„å»å™ªåå›¾åƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š losså‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š 4. å®éªŒç»“æœ æ–‡ä¸­éƒ¨åˆ†å®éªŒç»“æœå¦‚ä¸‹ï¼š 5.ä¸€äº›æ€è€ƒ ä¸ºäº†é¿å…ç½‘ç»œè¿…é€Ÿæ”¶æ•›è‡³ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£ï¼Œå¯¹äºå¾—åˆ°çš„Deformable Kernelsé‡‡æ ·ç‚¹ï¼Œä¼šå°†å…¶åˆ†ä¸ºsç»„ï¼Œæ¯ç»„è´Ÿè´£é¢„æµ‹ä¸€ä¸ªå»å™ªåçš„å›¾åƒå°½å¯èƒ½ä¸ground truthç›¸ä¼¼...åŒæ—¶ï¼Œè¿™ç§åšæ³•ä¹Ÿä¼šå¢å¼ºç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ã€‚ é‚£ä¹ˆï¼Œå¦‚ä½•åˆ†ç»„æˆ–è®¸ä¹Ÿæ˜¯ä¸€ä¸ªæ¯”è¾ƒé‡ç‚¹çš„åœ°æ–¹ï¼Œåšä¸»æƒ³åˆ°äº†ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š (1) è§„åˆ™åˆ†ç»„ï¼Œå¯¹äºNä¸ªé‡‡æ ·ç‚¹ï¼Œå‡åŒ€åœ°åˆ’åˆ†ä¸ºsç»„ï¼Œè¿™ä¹Ÿæ˜¯æœ€å¸¸è§„çš„æ–¹å¼æ— éœ€ä»”ç»†è®¨è®ºï¼› (2) éšæœºåˆ†ç»„ï¼Œå¯¹Nä¸ªé‡‡æ ·ç‚¹éšæœºåˆ†ä¸ºsç»„ï¼Œè€Œä¸é‡‡ç”¨è§„åˆ™çš„åˆ†ç»„ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—æ¯ä¸ªé‡‡æ ·ç‚¹éƒ½èƒ½å‘æŒ¥è¿‘ä¼¼ç›¸ç­‰çš„åœ°ä½ï¼Œèƒ½å¦è¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Ÿ (3) å…¶å®ï¼Œåˆ†ç»„çš„åšæ³•å¯ä»¥è¡ç”Ÿè‡³dropoutå±‚çš„åŸç†ï¼Œä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒæŒ‰ç…§ä¸€å®šæ¦‚ç‡æ–­å¼€ä¸€äº›é“¾æ¥ã€‚é‚£ä¹ˆï¼Œä¹Ÿå¯ä»¥åƒdropoutå±‚ä¸€æ ·ï¼Œè€Œä¸æ˜¯é‡‡ç”¨åˆ†ç»„çš„æ–¹å¼ï¼Œæ¯æ¬¡æŒ‰ç…§æ¦‚ç‡péšæœºå–ä¸€äº›é‡‡æ ·ç‚¹é¢„æµ‹å»å™ªåçš„å›¾åƒï¼Œè¿™æ ·èƒ½å¤Ÿæé«˜æ³›åŒ–èƒ½åŠ›ï¼Ÿ æ¬¢è¿å¤§å®¶ä¸æˆ‘ä¸€èµ·è®¨è®º~ Reference [1] Learning Deformable Kernels for Image and Video Denoising "},"paper/Isolating_Sources_of_Disentanglement_in_VAEs.html":{"url":"paper/Isolating_Sources_of_Disentanglement_in_VAEs.html","title":"Isolating Sources of Disentanglement in VAEs","keywords":"","body":"Isolating Sources of Disentanglement in VAEs è®ºæ–‡åœ°å€:https://arxiv.org/abs/1802.04942 ä½œè€…:Ricky T. Q. Chen, Xuechen Li, Roger Grosse, David Duvenaud University of Toronto, Vector Institute æˆ‘ä»¬åˆ†è§£äº†å˜åˆ†ä¸‹ç•Œï¼Œå±•ç¤ºäº†æ½œå˜é‡ä¸­çš„å…¨ç›¸å…³å€¼(Total Correlation)çš„å­˜åœ¨ï¼Œå¹¶è®¾è®¡äº†Î²-TCVAEç®—æ³•ã€‚Î²-TCVAEç®—æ³•æ˜¯ä¸€ä¸ªç²¾ç‚¼çš„ï¼Œå¯ä»¥æ›¿ä»£Î²-VAEæ¥å®Œæˆè§£çº ç¼ ï¼ˆDisentanglementï¼‰è¡¨ç¤ºç®—æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬å¹¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¸éœ€è¦åˆ†ç±»å™¨çš„è§£çº ç¼ åº¦é‡æ–¹æ³•ï¼Œå«åšäº’ä¿¡æ¯é—´éš”ï¼ˆMutual Information Gapï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¤§é‡çš„å’Œé«˜è´¨é‡çš„å®éªŒï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€äº›é™åˆ¶æ¡ä»¶å’Œéçº¿æ€§æ¡ä»¶ä¸‹çš„è®¾ç½®ä¸­ï¼Œè¯æ˜äº†å…¨ç›¸å…³å’Œè§£çº ç¼ ä¹‹é—´é‡è¦çš„å…³ç³»ã€‚ "},"paper/Spectral_Normalization.html":{"url":"paper/Spectral_Normalization.html","title":"Spectral Normalization è°±å½’ä¸€åŒ–","keywords":"","body":"Spectral Normalization è°±å½’ä¸€åŒ– æœ¬æ–‡ä¸»è¦ä»‹ç»è°±å½’ä¸€åŒ–è¿™é¡¹æŠ€æœ¯ï¼Œè¯¦ç»†è®ºæ–‡å‚è€ƒä¸‹é¢çš„é“¾æ¥ã€‚ Spectral Normalization For GANsarxiv.org æœ¬æ–‡ä¸»è¦å¯¹è®ºæ–‡ä¸­çš„åŸºç¡€çŸ¥è¯†å’Œé—æ¼çš„ç»†èŠ‚åšå‡ºè¡¥å……ï¼Œä»¥ä¾¿äºæ›´å¥½åœ°ç†è§£è°±å½’ä¸€åŒ–ã€‚éƒ¨åˆ†å†…å®¹å‚è€ƒå¹¶æ•´åˆäº†å¦‚ä¸‹ä¸¤ç¯‡åšå®¢ï¼šSpectral Normalization Explainedchristiancosgrove.com GAN çš„è°±å½’ä¸€åŒ–(Spectral Norm)å’ŒçŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decompostion)kaiz.xyz Lipschitz continuity åœ¨ GAN ä¸­ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ¤åˆ«å™¨ ï¼Œå…¶ä¸­ æ˜¯å›¾åƒç©ºé—´ã€‚å¦‚æœåˆ¤åˆ«å™¨æ˜¯ K-Lipschitz continuous çš„ï¼Œé‚£ä¹ˆå¯¹å›¾åƒç©ºé—´ä¸­çš„ä»»æ„ x å’Œ yï¼Œæœ‰ï¼š å…¶ä¸­ ä¸º normï¼Œå¦‚æœ K å–åˆ°æœ€å°å€¼ï¼Œé‚£ä¹ˆ K è¢«ç§°ä¸º Lipschitz constantã€‚ ç›´è§‚åœ°æ¥è¯´ï¼ŒLipschitz æ¡ä»¶é™åˆ¶äº†å‡½æ•°å˜åŒ–çš„å‰§çƒˆç¨‹åº¦ï¼Œå³å‡½æ•°çš„æ¢¯åº¦ã€‚åœ¨ä¸€ç»´ç©ºé—´ä¸­ï¼Œå¾ˆå®¹æ˜“çœ‹å‡º y=sin(x) æ˜¯ 1-Lipschitzçš„ï¼Œå®ƒçš„æœ€å¤§æ–œç‡æ˜¯ 1ã€‚ é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦ä½¿åˆ¤åˆ«å™¨å‡½æ•°å…·æœ‰ Lipschitz continuity å‘¢ï¼ŸWasserstein GAN æå‡ºäº†ç”¨ wasserstein è·ç¦»å–ä»£ä¹‹å‰çš„ KL æ•£åº¦æˆ–è€… JS æ•£åº¦ï¼Œä½œä¸º GAN åˆ¤åˆ«å™¨çš„æŸå¤±å‡½æ•°ï¼š å…¶ä¸­ åˆ†åˆ«ä¸ºçœŸå®æ•°æ®å’Œç”Ÿæˆçš„æ•°æ®çš„åˆ†å¸ƒå‡½æ•°ï¼Œwasserstein è·ç¦»è¡¡é‡äº†è¿™ä¸¤ä¸ªåˆ†å¸ƒå‡½æ•°çš„å·®å¼‚æ€§ã€‚ç›´è§‚åœ°ç†è§£ï¼Œå°±æ˜¯æ ¹æ®è¿™ä¸¤ä¸ªåˆ†å¸ƒå‡½æ•°åˆ†åˆ«ç”Ÿæˆä¸€å †æ•°æ® å’Œå¦ä¸€å †æ•°æ® , ç„¶åè®¡ç®—è¿™ä¸¤å †æ•°æ®ä¹‹é—´çš„è·ç¦»ã€‚è·ç¦»çš„ç®—æ³•æ˜¯æ‰¾åˆ°ä¸€ç§ä¸€ä¸€å¯¹åº”çš„é…å¯¹æ–¹æ¡ˆ ï¼ŒæŠŠ ç§»åŠ¨åˆ° ï¼Œæ±‚æ€»ç§»åŠ¨è·ç¦»çš„æœ€å°å€¼ã€‚ç”±äºåœ¨ GAN ä¸­ï¼Œ å’Œ éƒ½æ²¡æœ‰æ˜¾å¼çš„è¡¨è¾¾å¼ï¼Œåªèƒ½æ˜¯ä»é‡Œé¢ä¸åœåœ°é‡‡æ ·ï¼Œæ‰€ä»¥ä¸å¯èƒ½æ‰¾åˆ°è¿™æ ·çš„ ï¼Œæ— æ³•ç›´æ¥ä¼˜åŒ–å…¬å¼ (2) ã€‚W-GAN çš„åšæ³•æ˜¯æ ¹æ® Kantorovich-Rubinstein dualityï¼Œå°†å…¬å¼ (2) è½¬åŒ–æˆå…¬å¼ (3)ï¼Œè¿‡ç¨‹è¯¦è§ï¼š Wasserstein GAN and the Kantorovich-Rubinstein Dualityvincentherrmann.github.io å…¶ä¸­ å³ä¸ºåˆ¤åˆ«å™¨å‡½æ•°ã€‚åªæœ‰å½“åˆ¤åˆ«å™¨å‡½æ•°æ»¡è¶³ 1-Lipschitz çº¦æŸæ—¶ï¼Œ(2) æ‰èƒ½è½¬åŒ–ä¸º (3)ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ­£å¦‚ä¸Šæ–‡æ‰€è¯´ï¼ŒLipschitz continuous çš„å‡½æ•°çš„æ¢¯åº¦ä¸Šç•Œè¢«é™åˆ¶ï¼Œå› æ­¤å‡½æ•°æ›´å¹³æ»‘ï¼Œåœ¨ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå‚æ•°å˜åŒ–ä¹Ÿä¼šæ›´ç¨³å®šï¼Œä¸å®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸ï¼Œå› æ­¤Lipschitz continuity æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ€§è´¨ã€‚ ä¸ºäº†è®©åˆ¤åˆ«å™¨å‡½æ•°æ»¡è¶³ 1-Lipschitz continuityï¼ŒW-GAN å’Œä¹‹åçš„ W-GAN GP åˆ†åˆ«é‡‡ç”¨äº† weight-clipping å’Œ gradient penalty æ¥çº¦æŸåˆ¤åˆ«å™¨å‚æ•°ã€‚è¿™é‡Œçš„è°±å½’ä¸€åŒ–ï¼Œåˆ™æ˜¯å¦ä¸€ç§è®©å‡½æ•°æ»¡è¶³ 1-Lipschitz continuity çš„æ–¹å¼ã€‚ çŸ©é˜µçš„ Lipschitz continuity ä¼—æ‰€å‘¨çŸ¥ï¼ŒçŸ©é˜µçš„ä¹˜æ³•æ˜¯çº¿æ€§æ˜ å°„ã€‚å¯¹çº¿æ€§æ˜ å°„æ¥è¯´ï¼Œå¦‚æœå®ƒåœ¨é›¶ç‚¹å¤„æ˜¯ K-Lipschitz çš„ï¼Œé‚£ä¹ˆå®ƒåœ¨æ•´ä¸ªå®šä¹‰åŸŸä¸Šéƒ½æ˜¯ K-Lipschitz çš„ã€‚æƒ³è±¡ä¸€æ¡è¿‡é›¶ç‚¹çš„ç›´çº¿ï¼Œå®ƒçš„æ–œç‡æ˜¯å›ºå®šçš„ï¼Œåªè¦å®ƒä¸Šé¢ä»»ä½•ä¸€ç‚¹æ˜¯ K-Lipschitz çš„ï¼Œé‚£ä¹ˆå®ƒä¸Šé¢æ‰€æœ‰ç‚¹éƒ½æ˜¯ K-Lipschitz çš„ã€‚å› æ­¤ï¼Œå¯¹çŸ©é˜µ æ¥è¯´ï¼Œå®ƒæ»¡è¶³ K-Lipschitz çš„å……è¦æ¡ä»¶æ˜¯ï¼š å¯¹å…¶åšå¦‚ä¸‹å˜æ¢ï¼š å…¶ä¸­ è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„å†…ç§¯ã€‚ç”±äºçŸ©é˜µ æ˜¯åŠæ­£å®šçŸ©é˜µï¼Œå®ƒçš„æ‰€æœ‰ç‰¹å¾å€¼å‡ä¸ºéè´Ÿã€‚æˆ‘ä»¬å‡è®¾å®ƒçš„ç‰¹å¾å‘é‡æ„æˆçš„åŸºåº•ä¸º ï¼Œå¯¹åº”çš„ç‰¹å¾å€¼ä¸º ï¼Œä»¤ ã€‚é‚£ä¹ˆï¼Œå…¬å¼ (5) çš„å·¦åŠéƒ¨åˆ†å¯ä»¥è½¬åŒ–ä¸ºï¼š è¦ä½¿å…¬å¼ (6) å¯¹ä»»æ„ æ’æˆç«‹ï¼Œä¸” éè´Ÿï¼Œåˆ™å¿…æœ‰ ã€‚è‹¥ ä¸ºæœ€å¤§çš„ç‰¹å¾å€¼ï¼Œåªéœ€è¦æ»¡è¶³ ï¼Œè¿™é‡Œ å³ä¸ºçŸ©é˜µ çš„ spectral normã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œæ˜ å°„ æ»¡è¶³ K-Lipschitz è¿ç»­ï¼ŒK çš„æœ€å°å€¼ä¸º ã€‚é‚£ä¹ˆï¼Œè¦æƒ³è®©çŸ©é˜µ æ»¡è¶³ 1-Lipschitz è¿ç»­ï¼Œåªéœ€è¦åœ¨Açš„æ‰€æœ‰å…ƒç´ ä¸ŠåŒæ—¶é™¤ä»¥ å³å¯ï¼ˆè§‚å¯Ÿå…¬å¼ (4)å·¦ä¾§æ˜¯çº¿æ€§æ˜ å°„ï¼‰ã€‚ é€šè¿‡ä¸Šé¢çš„è®¨è®ºï¼Œæˆ‘ä»¬å¾—å‡ºäº†è¿™æ ·çš„ç»“è®ºï¼šçŸ©é˜µ é™¤ä»¥å®ƒçš„ spectral normï¼ˆ æœ€å¤§ç‰¹å¾å€¼çš„å¼€æ ¹å· ï¼‰å¯ä»¥ä½¿å…¶å…·æœ‰ 1-Lipschitz continuityã€‚ çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ ä¸Šæ–‡æåˆ°çš„çŸ©é˜µçš„ spectral norm çš„å¦ä¸€ä¸ªç§°å‘¼æ˜¯çŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼ã€‚å›é¡¾çŸ©é˜µçš„ SVD åˆ†è§£ï¼š çŸ©é˜µ å­˜åœ¨è¿™æ ·çš„ä¸€ç§åˆ†è§£ï¼š å…¶ä¸­ï¼š U æ˜¯ä¸€ä¸ª çš„å•ä½æ­£äº¤åŸºçŸ©é˜µ æ˜¯ä¸€ä¸ª çš„å¯¹è§’é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸ºå¥‡å¼‚å€¼ï¼Œéå¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸º0 V æ˜¯ä¸€ä¸ª çš„å•ä½æ­£äº¤åŸºçŸ©é˜µ SVD åˆ†è§£ ï¼ˆä¸Šå›¾æ¥è‡ªï¼šå¥‡å¼‚å€¼åˆ†è§£(SVD)åŸç†ä¸åœ¨é™ç»´ä¸­çš„åº”ç”¨ - åˆ˜å»ºå¹³Pinard - åšå®¢å›­ï¼‰ ç”±äº U å’Œ V éƒ½æ˜¯å•ä½æ­£äº¤åŸºï¼Œå› æ­¤å¯ä»¥æŠŠçŸ©é˜µä¹˜ä»¥å‘é‡åˆ†æˆä¸‰æ­¥ï¼šæ—‹è½¬ï¼Œæ‹‰ä¼¸ï¼Œæ—‹è½¬ã€‚ä¸€å‰ä¸€åçš„ä¸¤æ­¥æ—‹è½¬ä¸æ”¹å˜å‘é‡çš„æ¨¡é•¿ï¼Œå”¯ä¸€æ”¹å˜å‘é‡æ¨¡é•¿çš„æ˜¯ä¸­é—´çš„æ‹‰ä¼¸ï¼Œå³ä¸ ç›¸ä¹˜çš„é‚£ä¸€æ­¥ã€‚è€ŒçŸ©é˜µçš„ Lipschitz continuity å…³å¿ƒçš„æ­£æ˜¯çŸ©é˜µå¯¹å‘é‡æ¨¡é•¿çš„æ”¹å˜ï¼Œä¸å…³å¿ƒæ—‹è½¬ã€‚å› æ­¤ï¼Œåªéœ€è¦ç ”ç©¶ä¸­é—´çš„ å³å¯ã€‚è€Œ åˆæ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå› æ­¤ï¼Œå®ƒå¯¹å‘é‡çš„æ¨¡é•¿æ‹‰é•¿çš„æœ€å¤§å€¼ï¼Œå°±æ˜¯å¯¹è§’çº¿ä¸Šçš„å…ƒç´ çš„æœ€å¤§å€¼ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒçŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼å³ä¸ºå®ƒçš„ spectral normã€‚ æ ¹æ®å…¬å¼ (7) ï¼Œä»¥åŠ SVD åˆ†è§£ä¸­ U V éƒ½æ˜¯å•ä½æ­£äº¤åŸºï¼Œå•ä½æ­£äº¤åŸºçš„è½¬ç½®ä¹˜ä»¥å®ƒæœ¬èº«ä¸ºå•ä½çŸ©é˜µï¼Œæœ‰ï¼š å› æ­¤ï¼Œåªéœ€è¦æ±‚å‡º çš„æœ€å¤§ç‰¹å¾å€¼ï¼Œå†å¼€æ ¹å· ï¼Œå°±æ±‚å‡ºäº†çŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼ï¼Œä¹Ÿå°±æ˜¯çŸ©é˜µçš„ spectral normï¼Œå’Œä¸Šä¸€å°èŠ‚çš„æ¨å¯¼æ®Šé€”åŒå½’ã€‚ ç¥ç»ç½‘ç»œçš„ Spectral Normalization å¯¹äºå¤åˆå‡½æ•°ï¼Œæˆ‘ä»¬æœ‰è¿™æ ·çš„å®šç†ï¼š è€Œå¤šå±‚ç¥ç»ç½‘ç»œï¼Œæ­£æ˜¯å¤šä¸ªå¤åˆå‡½æ•°åµŒå¥—çš„æ“ä½œã€‚æœ€å¸¸è§çš„åµŒå¥—æ˜¯ï¼šä¸€å±‚å·ç§¯ï¼Œä¸€å±‚æ¿€æ´»å‡½æ•°ï¼Œå†ä¸€å±‚å·ç§¯ï¼Œå†ä¸€å±‚æ¿€æ´»å‡½æ•°ï¼Œè¿™æ ·å±‚å±‚åŒ…è£¹èµ·æ¥ã€‚è€Œæ¿€æ´»å‡½æ•°é€šå¸¸é€‰å–çš„ ReLUï¼ŒLeaky ReLU éƒ½æ˜¯ 1-Lipschitz çš„ï¼Œå¸¦å…¥åˆ° (9) ä¸­ç›¸ä¹˜ä¸å½±å“æ€»ä½“çš„ Lipschitz constantï¼Œæˆ‘ä»¬åªéœ€è¦ä¿è¯å·ç§¯çš„éƒ¨åˆ†æ˜¯ 1-Lipschitz continuous çš„ï¼Œå°±å¯ä»¥ä¿è¯æ•´ä¸ªç¥ç»ç½‘ç»œéƒ½æ˜¯ 1-Lipschitz continuous çš„ã€‚ è€Œåœ¨å›¾åƒä¸Šæ¯ä¸ªä½ç½®çš„å·ç§¯æ“ä½œï¼Œæ­£å¥½å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦çº¦æŸå„å±‚å·ç§¯æ ¸çš„å‚æ•° ï¼Œä½¿å®ƒæ˜¯ 1-Lipschitz continuous çš„ï¼Œå°±å¯ä»¥æ»¡è¶³æ•´ä¸ªç¥ç»ç½‘ç»œçš„ 1-Lipschitz continuityã€‚è€Œæˆ‘ä»¬å·²ç»çŸ¥é“ï¼Œæƒ³è®©çŸ©é˜µæ»¡è¶³ 1-Lipschitz continuousï¼Œåªéœ€è¦è®©å®ƒæ‰€æœ‰å…ƒç´ åŒæ—¶é™¤ä»¥å®ƒçš„æœ€å¤§å¥‡å¼‚å€¼ï¼Œæˆ–è€…è¯´æ˜¯å®ƒçš„ spectural normã€‚å› æ­¤ï¼Œä¸‹ä¸€æ­¥çš„é—®é¢˜æ˜¯å¦‚ä½•è®¡ç®— çš„æœ€å¤§å¥‡å¼‚å€¼ã€‚ å¯¹å¤§çŸ©é˜µåš SVD åˆ†è§£è¿ç®—é‡å¾ˆå¤§ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›åœ¨ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥éƒ½å¯¹å·ç§¯æ ¸çŸ©é˜µåšä¸€æ¬¡ SVD åˆ†è§£ã€‚ä¸€ä¸ªè¿‘ä¼¼çš„è§£å†³æ–¹æ¡ˆæ˜¯ power iteration ç®—æ³•ã€‚ Power Iteration Power iteration æ˜¯ç”¨æ¥è¿‘ä¼¼è®¡ç®—çŸ©é˜µæœ€å¤§çš„ç‰¹å¾å€¼ï¼ˆdominant eigenvalue ä¸»ç‰¹å¾å€¼ï¼‰å’Œå…¶å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼ˆä¸»ç‰¹å¾å‘é‡ï¼‰çš„ã€‚ å‡è®¾çŸ©é˜µ A æ˜¯ä¸€ä¸ª çš„æ»¡ç§©çš„æ–¹é˜µï¼Œå®ƒçš„å•ä½ç‰¹å¾å‘é‡ä¸º ï¼Œå¯¹åº”çš„ç‰¹å¾å€¼ä¸º ã€‚é‚£ä¹ˆå¯¹ä»»æ„å‘é‡ ï¼Œæœ‰ï¼š æˆ‘ä»¬ç»è¿‡ k æ¬¡è¿­ä»£ï¼š ç”±äº \\lambda_2>...>\\lambda_n\" alt=\"å›¾ç‰‡\"> (ä¸è€ƒè™‘ä¸¤ä¸ªç‰¹å¾å€¼ç›¸ç­‰çš„æƒ…å†µï¼Œå› ä¸ºå¤ªå°‘è§äº†)ã€‚å¯çŸ¥ï¼Œç»è¿‡ k æ¬¡è¿­ä»£å ( )ã€‚å› æ­¤ï¼š ä¹Ÿå°±æ˜¯è¯´ï¼Œç»è¿‡ k æ¬¡è¿­ä»£åï¼Œæˆ‘ä»¬å°†å¾—åˆ°çŸ©é˜µä¸»ç‰¹å¾å‘é‡çš„çº¿æ€§æ”¾ç¼©ï¼Œåªè¦æŠŠè¿™ä¸ªå‘é‡å½’ä¸€åŒ–ï¼Œå°±å¾—åˆ°äº†è¯¥çŸ©é˜µçš„å•ä½ä¸»ç‰¹å¾å‘é‡ï¼Œè¿›è€Œå¯ä»¥è§£å‡ºçŸ©é˜µçš„ä¸»ç‰¹å¾å€¼ã€‚ è€Œæˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæƒ³æ±‚çš„æ˜¯æƒé‡çŸ©é˜µ çš„æœ€å¤§å¥‡å¼‚å€¼ï¼Œæ ¹æ®ä¸Šé¢å‡ èŠ‚çš„æ¨å¯¼ï¼ŒçŸ¥é“è¿™ä¸ªå¥‡å¼‚å€¼æ­£æ˜¯ æœ€å¤§ç‰¹å¾å€¼çš„å¼€æ–¹ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ power iteration çš„æ–¹å¼æ±‚è§£ çš„å•ä½ä¸»ç‰¹å¾å‘é‡ï¼Œè¿›è€Œæ±‚å‡ºæœ€å¤§ç‰¹å¾å€¼ ã€‚è®ºæ–‡ä¸­ç»™å‡ºçš„ç®—æ³•æ˜¯è¿™æ ·çš„ï¼š å¦‚æœå•çº¯çœ‹åˆ†å­ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸¤æ­¥åˆèµ·æ¥å°±æ˜¯ ï¼Œåå¤è¿­ä»£ (13) ä¸­ä¸Šä¸‹ä¸¤ä¸ªå¼å­ ï¼Œå³å¯å¾—åˆ°çŸ©é˜µ çš„å•ä½ä¸»ç‰¹å¾å‘é‡ ã€‚åªä¸è¿‡ (13) çš„æ¯ç®—â€œåŠâ€æ­¥éƒ½å½’ä¸€åŒ–ä¸€æ¬¡ã€‚å…¶å®è¿™ç§å½’ä¸€åŒ–å¹¶ä¸å½±å“å‘é‡ çš„æ–¹å‘æ”¶æ•›åˆ°ä¸»ç‰¹å¾å‘é‡çš„æ–¹å‘ï¼Œè€Œåªå½±å“ç‰¹å¾å‘é‡å‰é¢çš„ç³»æ•°ã€‚è€Œæ¯æ­¥å½’ä¸€åŒ–ä¸€æ¬¡çš„å¥½å¤„æ˜¯ï¼Œæ¯æ­¥éƒ½å¯ä»¥å¾—åˆ°å•ä½ä¸»ç‰¹å¾å‘é‡çš„è¿‘ä¼¼è§£ã€‚ é‚£ä¹ˆï¼ŒçŸ¥é“ çš„å•ä½ä¸»ç‰¹å¾å‘é‡ åï¼Œå¦‚ä½•æ±‚å‡ºæœ€å¤§ç‰¹å¾å€¼ å‘¢ï¼Ÿ è€Œå°†å…¬å¼ (13) çš„ç¬¬äºŒä¸ªå¼å­ä¸¤è¾¹åŒæ—¶å·¦ä¹˜ : æœ€ç»ˆï¼Œ(15) å³ä¸ºè®ºæ–‡ä¸­æå‡ºçš„æƒé‡çŸ©é˜µ çš„ spectral norm å…¬å¼ã€‚ è€Œåœ¨å…·ä½“çš„ä»£ç å®ç°è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥éšæœºåˆå§‹åŒ–ä¸€ä¸ªå™ªå£°å‘é‡ ä»£å…¥å…¬å¼ (13) ã€‚ç”±äºæ¯æ¬¡æ›´æ–°å‚æ•°çš„ step size å¾ˆå°ï¼ŒçŸ©é˜µ çš„å‚æ•°å˜åŒ–éƒ½å¾ˆå°ï¼ŒçŸ©é˜µå¯ä»¥é•¿æ—¶é—´ç»´æŒä¸å˜ã€‚å› æ­¤ï¼Œå¯ä»¥æŠŠå‚æ•°æ›´æ–°çš„ step å’Œæ±‚çŸ©é˜µæœ€å¤§å¥‡å¼‚å€¼çš„ step èåˆåœ¨ä¸€èµ·ï¼Œå³æ¯æ›´æ–°ä¸€æ¬¡æƒé‡ ï¼Œæ›´æ–°ä¸€æ¬¡ å’Œ ï¼Œå¹¶å°†çŸ©é˜µå½’ä¸€åŒ–ä¸€æ¬¡ï¼ˆé™¤ä»¥å…¬å¼ (15) è¿‘ä¼¼ç®—å‡ºæ¥çš„ spectral normï¼‰ã€‚ å…·ä½“ä»£ç è§ï¼š christiancosgrove/pytorch-spectral-normalization-gangithub.com ç¼–è¾‘äº 2019-01-23 ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ è®¡ç®—æœºè§†è§‰ å¯¹è§’å ä¼˜çŸ©é˜µ ã€€ã€€å¯¹è§’å ä¼˜çŸ©é˜µæ˜¯è®¡ç®—æ•°å­¦ä¸­åº”ç”¨éå¸¸å¹¿æ³›çš„çŸ©é˜µç±»ï¼Œå®ƒè¾ƒå¤šå‡ºç°äºç»æµä»·å€¼æ¨¡å‹å’Œåç½‘ç»œç³»ç»Ÿçš„ç³»æ•°çŸ©é˜µåŠè§£æŸäº›ç¡®å®šå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼è§£æ³•ä¸­ï¼Œåœ¨ä¿¡æ¯è®ºã€ç³»ç»Ÿè®ºã€ç°ä»£ç»æµå­¦ã€ç½‘ç»œã€ç®—æ³•å’Œç¨‹åºè®¾è®¡ç­‰ä¼—å¤šé¢†åŸŸéƒ½æœ‰ç€ååˆ†é‡è¦çš„åº”ç”¨ã€‚ã€€ã€€å®šä¹‰ï¼šné˜¶æ–¹é˜µAï¼Œå¦‚æœå…¶ä¸»å¯¹è§’çº¿å…ƒç´ çš„ç»å¯¹å€¼å¤§äºåŒè¡Œå…¶ä»–å…ƒç´ ç»å¯¹å€¼ä¹‹å’Œï¼Œåˆ™ç§°Aæ˜¯å¯¹è§’å ä¼˜çš„ã€‚ å¦‚æœAçš„æ¯ä¸ªå¯¹è§’å…ƒçš„ç»å¯¹å€¼éƒ½æ¯”æ‰€åœ¨è¡Œçš„éå¯¹è§’å…ƒçš„ç»å¯¹å€¼çš„å’Œè¦å¤§ï¼Œå³ |a_ii|>sum{j!=i}|a_ij| å¯¹æ‰€æœ‰çš„iæˆç«‹ï¼Œé‚£ä¹ˆç§°Aæ˜¯ï¼ˆè¡Œï¼‰ä¸¥æ ¼å¯¹è§’å ä¼˜é˜µã€‚ å¦‚æœA'æ˜¯è¡Œä¸¥æ ¼å¯¹è§’å ä¼˜é˜µï¼Œé‚£ä¹ˆç§°Aæ˜¯åˆ—ä¸¥æ ¼å¯¹è§’å ä¼˜é˜µã€‚ ä¹ æƒ¯ä¸Šå¦‚æœä¸æŒ‡æ˜å“ªç§ç±»å‹çš„è¯å°±è®¤ä¸ºæ˜¯è¡Œå¯¹è§’å ä¼˜ã€‚ ä¸¥æ ¼å¯¹è§’å ä¼˜çŸ©é˜µä¸€å®šæ­£å®šå—ï¼Ÿ ä¸ä¸€å®šï¼Œæ¯”å¦‚ è´Ÿä¸‰é˜¶å•ä½çŸ©é˜µ å®å¯¹ç§°çŸ©é˜µæ˜¯é«˜ç­‰ä»£æ•°ä¸­ä¸€ä¸ªé‡è¦çš„å†…å®¹, æ‰€è°“å®šå‹å® å¯¹ç§°çŸ©é˜µæ˜¯æŒ‡æ­£å®šã€è´Ÿå®šã€åŠæ­£å®šå’ŒåŠè´Ÿå®šçŸ©é˜µ, æˆ‘ä»¬é¦–å…ˆå› é¡¾ä¸€ä¸‹æœ¬æ–‡å°†ç”¨åˆ°çš„æœ‰å…³å®å¯¹ç§°çŸ©é˜µçš„ä¸€äº›ç»“è®º: æ€§è´¨1: ä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µAæ­£å®šçš„å……è¦æ¡ä»¶æ˜¯å­˜åœ¨å¯é€†æ–¹ é˜µC, ä½¿å¾—A=Câ€²Cã€‚ æ€§è´¨2: ä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µAåŠæ­£å®šçš„å……è¦æ¡ä»¶æ˜¯å®ƒçš„æ‰€æœ‰ ä¸»å­å¼éƒ½å¤§äºç­‰äºé›¶ã€‚ æ€§è´¨3: ä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µAè´Ÿå®š( åŠè´Ÿå®š) çš„å……è¦æ¡ä»¶æ˜¯- A ä¸ºæ­£å®š( åŠæ­£å®š) ã€‚ æ€§è´¨4: nç»´æ¬§æ°ç©ºé—´ä¸­, ä¸€ç»„åŸºÎµ1,Îµ2, â‹¯,Îµn çš„åº¦é‡çŸ©é˜µA= (aij), å…¶ä¸­aij=(Îµi,Îµj)ä¸ºå®å¯¹ç§°çŸ©é˜µ, è€Œä¸”çŸ©é˜µAæ˜¯æ­£å®šçš„ã€‚ æ€§è´¨5: nç»´æ¬§æ°ç©ºé—´ä¸­, ä¸¤ç»„åŸºÎµ1,Îµ2, â‹¯,Îµn å’ŒÎ·1 ,Î·2, â‹¯,Î·n çš„åº¦é‡çŸ©é˜µåˆ†åˆ«ä¸ºAå’ŒB, é‚£ä¹ˆAå’ŒBæ˜¯åˆåŒçš„, å³è‹¥(Î·1,Î·2 , â‹¯,Î·n ) =(Îµ1,Îµ2, â‹¯,Îµn)C, åˆ™æœ‰B=Câ€²ACã€‚ æœ¬æ–‡è¦è¯æ˜çš„ä¸»è¦å®šç†ä¸º: å®šç†1: A=(aij)ä¸ºné˜¶æ­£å®šçŸ©é˜µ, åˆ™æœ‰detAâ‰¤ n k=1 âˆakk å…³äºå®šå‹å®å¯¹ç§°çŸ©é˜µçš„è¡Œåˆ—å¼çš„ä¸€ä¸ªç»“è®º( é•¿æ±Ÿå¸ˆèŒƒå­¦é™¢æ•°å­¦ç³», é‡åº†408100)æ¨ä¸–æ˜¾ å¯¹ç§°æ­£å®šçŸ©é˜µå¯¹è§’çº¿ä¸Šçš„å…ƒç´ å¿…é¡»ç›¸åŒå—ï¼Ÿ ä¸å¿…é¡»ï¼Œä¾‹å¦‚æ‰€æœ‰æ»¡è¶³å¯¹è§’çº¿å…ƒç´ éƒ½æ˜¯æ­£æ•°çš„å¯¹è§’çŸ©é˜µéƒ½æ˜¯å¯¹ç§°æ­£å®šçš„ã€‚ ä¸ºä»€ä¹ˆhermiteæ­£å®šçŸ©é˜µçš„æ¨¡æœ€å¤§çš„å…ƒç´ ä¸€å®šä½äºä¸»å¯¹è§’çº¿ä¸Š? åˆ©ç”¨æ­£å®šçŸ©é˜µçš„ä»»ä½•ä¸»å­é˜µæ­£å®šã€‚ å¦‚æœæ¨¡æœ€å¤§çš„å…ƒç´ A(i,j)ä¸åœ¨å¯¹è§’çº¿ä¸Šï¼Œé‚£ä¹ˆäºŒé˜¶ä¸»å­é˜µ A(i,i) A(i,j) A(j,i) A(j,j) ä¸æ˜¯æ­£å®šçš„ã€‚ å¯¹ç§°æ­£å®šçŸ©é˜µçš„ç»å¯¹å€¼æœ€å¤§å…ƒä¸ºä»€ä¹ˆæ˜¯å¯¹è§’å…ƒï¼Ÿ æ­£å®šçŸ©é˜µçš„æ‰€æœ‰ä¸»å­å¼å¤§äº0 åˆ™ aij=aji =0 ï¼Œä¸»å¯¹è§’å…ƒæ˜¯ä¸€é˜¶ä¸»å­å¼>=0ï¼Œä½†å…¶ä»–ä¸»å­å¼ä¸ä¸€å®š>=0ï¼Œæ•…ä¸ä¸€å®šã€‚ æœ‰ï¼Œé›¶çŸ©é˜µå°±æ˜¯åŠæ­£å®šçš„ã€‚ æ­£å®šçŸ©é˜µå¯¹è§’çº¿çš„å„å…ƒç´ éƒ½å¤§äº0å—ï¼Ÿ ç›´æ¥ç”¨æ­£å®šçš„å®šä¹‰å°±å¯ä»¥äº†ã€‚ å–x=(0,0,...,1,...,0)'ï¼Œå³ç¬¬iä¸ªå…ƒç´ ä¸º1ï¼Œå…¶ä½™ä¸º0çš„åˆ—å‘é‡ï¼Œé‚£ä¹ˆ x'Ax=a_{ii}>0ã€‚ éƒ½æ˜¯éè´Ÿæ•°ï¼Œå¯ä»¥æœ‰0ï¼Œä½†æ˜¯ä¸èƒ½å…¨éƒ¨æ˜¯0ã€‚ åº”è¯¥æ˜¯æ­£ç¡®çš„ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ•°å­¦å½’çº³æ³•æ¥è¯æ˜è¿™ä¸ªç»“è®ºã€‚ é¦–å…ˆï¼Œnï¼1æ—¶ï¼Œæ˜¯æ˜¾ç„¶æˆç«‹çš„ã€‚ å‡è®¾ï¼Œnï¼kæ—¶æˆç«‹ã€‚ åˆ™ï¼Œå½“nï¼k+1æ—¶ã€‚åˆ™è€ƒè™‘å…¶ä¸€ä¸ªné˜¶ä¸»å­å¼ï¼Œå…¶ä¹Ÿæ˜¯æ­£å®šçš„ã€‚å…¶å¯¹è§’å…ƒçš„å…ƒç´ ä¹‹å’Œå…¨éƒ½å¤§äº0ã€‚å†è€ƒå¯Ÿå¦ä¸€ä¸ªné˜¶ä¸»å­å¼ï¼Œåˆ™å…¶å¯¹è§’å…ƒçš„å…ƒç´ ä¹Ÿå…¨å¤§äºé›¶ã€‚ç»¼ä¸ŠçŸ¥ï¼Œå…¶æ‰€æœ‰çš„å¯¹è§’å…ƒçš„å…ƒç´ éƒ½å¤§äº0ã€‚ ç»¼ä¸ŠçŸ¥ï¼Œå‘½é¢˜å¾—è¯ã€‚ æ­£å®šçŸ©é˜µå¯¹è§’çº¿çš„å…ƒç´ aiiéƒ½å¤§äº0å—ï¼Ÿ å–xä¸ºå•ä½é˜µçš„ç¬¬iåˆ—ï¼Œç”±x'Ax>0å³å¾—ã€‚ çº¯é‡é˜µå°±æ˜¯A=aE å…¶ä¸­aä¸ºå¸¸æ•°ï¼ŒEä¸ºå•ä½çŸ©é˜µ æ­£å®šçŸ©é˜µçš„æ‰€æœ‰çš„ç‰¹å¾å€¼éƒ½æ˜¯å¤§äºé›¶çš„ï¼Œ è€ŒçŸ©é˜µçš„è¿¹(å³ï¼šä¸»å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ)=æ‰€æœ‰ç‰¹å¾å€¼çš„å’Œ>0 å¯¹è§’çº¿å…ƒç´ å‡ä¸º0çš„å¯¹ç§°çŸ©é˜µï¼Œå®ƒæ˜¯åŠæ­£å®šçš„å—ï¼Ÿ åŠæ­£å®šçŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ æ˜¯éè´Ÿçš„ï¼Œè€Œä¸”é›¶çŸ©é˜µæ˜¯ä¸€ä¸ªç‰¹æ®ŠçŸ©é˜µã€‚è¯·é—®å¯¹è§’çº¿å…ƒç´ ä¸º0ä¸€å®šèƒ½æ¨å‡ºæ˜¯åŠæ­£å®šå—ï¼Ÿ çº¿æ€§ä»£æ•°ä¸­ä»€ä¹ˆå«çº¯é‡ï¼Ÿä¸ºä»€ä¹ˆæ­£å®šçŸ©é˜µçš„ä¸»å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½å¤§äº0ï¼Ÿ åŠæ­£å®šï¼Œç­‰ä»·äºæ‰€æœ‰ä¸»å­å¼>=0 ï¼Œä¸»å¯¹è§’å…ƒæ˜¯ä¸€é˜¶ä¸»å­å¼>=0ï¼Œä½†å…¶ä»–ä¸»å­å¼ä¸ä¸€å®š>=0ï¼Œæ•…ä¸ä¸€å®šã€‚ æœ‰ï¼Œé›¶çŸ©é˜µå°±æ˜¯åŠæ­£å®šçš„ã€‚ å¯¹è§’çº¿å…ƒç´ ä¸º0ã€éå¯¹è§’çº¿å…ƒç´ å¤§äºç­‰äº0çš„å¯¹ç§°çŸ©é˜µï¼Œå®ƒæ˜¯åŠæ­£å®šçš„å—ï¼Ÿ ä¸ä¸€å®šæ˜¯ï¼Œæœ€ç®€å•çš„ï¼ŒäºŒé˜¶çŸ©é˜µï¼Œè§’çº¿å…ƒç´ ä¸º0ã€éå¯¹è§’çº¿å…ƒç´ ä¸º1ã€‚è€Œå®ƒçš„è¡Œåˆ—å¼ä¸º-1ã€‚ è¯·é—®ï¼Œå¯¹è§’çº¿å…ƒç´ ä¸º0ã€éå¯¹è§’çº¿å…ƒç´ å¤§äºç­‰äº0çš„å¯¹ç§°çŸ©é˜µï¼Œå®ƒæ˜¯åŠæ­£å®šçš„å—ï¼Ÿ ä¸ä¸€å®šæ˜¯ï¼Œæœ€ç®€å•çš„ï¼ŒäºŒé˜¶çŸ©é˜µï¼Œè§’çº¿å…ƒç´ ä¸º0ã€éå¯¹è§’çº¿å…ƒç´ ä¸º1ã€‚è€Œå®ƒçš„è¡Œåˆ—å¼ä¸º-1 ä¸ºä»€ä¹ˆè¯´åŠæ­£å®šçŸ©é˜µçš„è¡Œåˆ—å¼å¤§äºç­‰äº0? å› ä¸ºåŠæ­£å®šçŸ©é˜µçš„ç‰¹å¾å€¼>=0 ç‰¹åˆ«é™„åŠ å®šä¹‰ï¼ŒåŠæ­£å®šçŸ©é˜µä¸ºå¯¹ç§°çŸ©é˜µ æ‰€ä»¥å¯ä»¥å¯¹è§’åŒ–(å®šç†) A=P*B*P^-1 |A|=|B|>=0 "},"paper/Unbalanced_sample_loss.html":{"url":"paper/Unbalanced_sample_loss.html","title":"ä¸å‡è¡¡æ ·æœ¬loss","keywords":"","body":"æœ¬æ–‡æ˜¯è°·æ­Œå¯¹CVPR ' 19ä¸Šå‘è¡¨çš„ä¸€ç¯‡æ–‡ç« çš„ç»¼è¿°ï¼Œæ–‡ç« çš„æ ‡é¢˜æ˜¯Class-Balanced Loss Based on Effective Number of Samplesã€‚ å®ƒä¸ºæœ€å¸¸ç”¨çš„æŸè€—(softmax-cross-entropyã€focal lossç­‰)æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æ¯ä¸ªç±»åˆ«çš„é‡æ–°åŠ æƒæ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¿«é€Ÿæé«˜ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åº¦ç±»ä¸å¹³è¡¡çš„æ•°æ®æ—¶ã€‚ è®ºæ–‡çš„PyTorchå®ç°æºç ï¼šhttps://github.com/vandit15/Class-balanced-loss-pytorch æ ·æœ¬çš„æœ‰æ•ˆæ•°é‡ åœ¨å¤„ç†é•¿å°¾æ•°æ®é›†(å…¶ä¸­å¤§éƒ¨åˆ†æ ·æœ¬å±äºå¾ˆå°‘çš„ç±»ï¼Œè€Œè®¸å¤šå…¶ä»–ç±»çš„æ ·æœ¬éå¸¸å°‘)çš„æ—¶å€™ï¼Œå¦‚ä½•å¯¹ä¸åŒç±»çš„æŸå¤±è¿›è¡ŒåŠ æƒå¯èƒ½æ¯”è¾ƒæ£˜æ‰‹ã€‚é€šå¸¸ï¼Œæƒé‡è®¾ç½®ä¸ºç±»æ ·æœ¬çš„å€’æ•°æˆ–ç±»æ ·æœ¬çš„å¹³æ–¹æ ¹çš„å€’æ•°ã€‚ ä¼ ç»Ÿçš„æƒé‡è°ƒæ•´ä¸è¿™é‡Œæå‡ºçš„æƒé‡è°ƒæ•´ ç„¶è€Œï¼Œæ­£å¦‚ä¸Šé¢çš„å›¾æ‰€ç¤ºï¼Œè¿™ä¸€è¿‡æ¸¡æ˜¯å› ä¸ºéšç€æ ·æœ¬æ•°é‡çš„å¢åŠ ï¼Œæ–°æ•°æ®ç‚¹çš„å¸¦æ¥çš„å¥½å¤„ä¼šå‡å°‘ã€‚æ–°æ·»åŠ çš„æ ·æœ¬ææœ‰å¯èƒ½æ˜¯ç°æœ‰æ ·æœ¬çš„è¿‘ä¼¼å‰¯æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ä½¿ç”¨å¤§é‡æ•°æ®å¢å¼º(å¦‚é‡æ–°ç¼©æ”¾ã€éšæœºè£å‰ªã€ç¿»è½¬ç­‰)çš„æ—¶å€™ï¼Œå¾ˆå¤šéƒ½æ˜¯è¿™æ ·çš„æ ·æœ¬ã€‚ç”¨æœ‰æ•ˆæ ·æœ¬æ•°é‡æ–°åŠ æƒå¯ä»¥å¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚ æœ‰æ•ˆæ ·æœ¬æ•°å¯ä»¥æƒ³è±¡ä¸ºnä¸ªæ ·æœ¬æ‰€è¦†ç›–çš„å®é™…ä½“ç§¯ï¼Œå…¶ä¸­æ€»ä½“ç§¯Nç”±æ€»æ ·æœ¬è¡¨ç¤ºã€‚ æœ‰æ•ˆæ ·æœ¬æ•°é‡ æˆ‘ä»¬å†™æˆï¼š æœ‰æ•ˆæ ·æœ¬æ•°é‡ æˆ‘ä»¬è¿˜å¯ä»¥å†™æˆä¸‹é¢è¿™æ ·ï¼š æ¯ä¸ªæ ·æœ¬çš„è´¡çŒ® è¿™æ„å‘³ç€ç¬¬jä¸ªæ ·æœ¬å¯¹æœ‰æ•ˆæ ·æœ¬æ•°çš„è´¡çŒ®ä¸ºÎ²j-1ã€‚ ä¸Šå¼çš„å¦ä¸€ä¸ªå«ä¹‰æ˜¯ï¼Œå¦‚æœÎ²=0ï¼Œåˆ™En=1ã€‚åŒæ ·ï¼Œå½“Î²â†’1çš„æ—¶å€™Enâ†’nã€‚åè€…å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨æ´›å¿…è¾¾æ³•åˆ™è¯æ˜ã€‚è¿™æ„å‘³ç€å½“Nå¾ˆå¤§æ—¶ï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ä¸æ ·æœ¬æ•°Nç›¸åŒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå”¯ä¸€åŸå‹æ•°Nå¾ˆå¤§ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯å”¯ä¸€çš„ã€‚ç„¶è€Œï¼Œå¦‚æœN=1ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥ç”¨ä¸€ä¸ªåŸå‹è¡¨ç¤ºã€‚ ç±»åˆ«å‡è¡¡æŸå¤± å¦‚æœæ²¡æœ‰é¢å¤–çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¸èƒ½ä¸ºæ¯ä¸ªç±»è®¾ç½®å•ç‹¬çš„Betaå€¼ï¼Œå› æ­¤ï¼Œä½¿ç”¨æ•´ä¸ªæ•°æ®çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸ºä¸€ä¸ªç‰¹å®šçš„å€¼(é€šå¸¸è®¾ç½®ä¸º0.9ã€0.99ã€0.999ã€0.9999ä¸­çš„ä¸€ä¸ª)ã€‚ å› æ­¤ï¼Œç±»åˆ«å‡è¡¡æŸå¤±å¯è¡¨ç¤ºä¸ºï¼š è¿™é‡Œï¼Œ L(p,y) å¯ä»¥æ˜¯ä»»æ„çš„æŸå¤±ã€‚ ç±»åˆ«å‡è¡¡Focal Loss åŸå§‹ç‰ˆæœ¬çš„focal lossæœ‰ä¸€ä¸ªÎ±å¹³è¡¡å˜é‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¯ä¸ªç±»çš„æœ‰æ•ˆæ ·æœ¬æ•°å¯¹å…¶é‡æ–°åŠ æƒã€‚ ç±»ä¼¼åœ°ï¼Œè¿™æ ·ä¸€ä¸ªé‡æ–°åŠ æƒçš„é¡¹ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–è‘—åçš„æŸå¤±(sigmod -cross-entropy, softmax-cross-entropyç­‰)ã€‚ å®ç° åœ¨å¼€å§‹å®ç°ä¹‹å‰ï¼Œéœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨ä½¿ç”¨åŸºäºsigmoidçš„æŸå¤±è¿›è¡Œè®­ç»ƒæ—¶ï¼Œä½¿ç”¨b=-log(C-1)åˆå§‹åŒ–æœ€åä¸€å±‚çš„åå·®ï¼Œå…¶ä¸­Cæ˜¯ç±»çš„æ•°é‡ï¼Œè€Œä¸æ˜¯0ã€‚è¿™æ˜¯å› ä¸ºè®¾ç½®b=0ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶é€ æˆå·¨å¤§çš„æŸå¤±ï¼Œå› ä¸ºæ¯ä¸ªç±»çš„è¾“å‡ºæ¦‚ç‡æ¥è¿‘0.5ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å…ˆéªŒç±»æ˜¯1/Cï¼Œå¹¶ç›¸åº”åœ°è®¾ç½®bçš„å€¼ã€‚ æ¯ä¸ªç±»çš„æƒå€¼çš„è®¡ç®— è®¡ç®—å½’ä¸€åŒ–çš„æƒå€¼ ä¸Šé¢çš„ä»£ç è¡Œæ˜¯è·å–æƒé‡å¹¶å°†å…¶æ ‡å‡†åŒ–çš„ç®€å•å®ç°ã€‚ å¾—åˆ°æ ‡ç­¾çš„onehotå¼ é‡ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¾—åˆ°æƒé‡çš„ç‹¬çƒ­å€¼ï¼Œè¿™æ ·å®ƒä»¬å°±å¯ä»¥åˆ†åˆ«ä¸æ¯ä¸ªç±»çš„æŸå¤±å€¼ç›¸ä¹˜ã€‚ å®éªŒ ç±»å¹³è¡¡æä¾›äº†æ˜¾è‘—çš„æ”¶ç›Šï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®é›†é«˜åº¦ä¸å¹³è¡¡æ—¶(ä¸å¹³è¡¡= 200,100)ã€‚ ç»“è®º åˆ©ç”¨æœ‰æ•ˆæ ·æœ¬æ•°çš„æ¦‚å¿µï¼Œå¯ä»¥è§£å†³æ•°æ®é‡å é—®é¢˜ã€‚ç”±äºæˆ‘ä»¬æ²¡æœ‰å¯¹æ•°æ®é›†æœ¬èº«åšä»»ä½•å‡è®¾ï¼Œå› æ­¤é‡æ–°åŠ æƒé€šå¸¸é€‚ç”¨äºå¤šä¸ªæ•°æ®é›†å’Œå¤šä¸ªæŸå¤±å‡½æ•°ã€‚å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨æ›´åˆé€‚çš„ç»“æ„æ¥å¤„ç†ç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå› ä¸ºå¤§å¤šæ•°å®é™…æ•°æ®é›†éƒ½å­˜åœ¨å¤§é‡çš„æ•°æ®ä¸å¹³è¡¡ã€‚ "},"paper/NN.html":{"url":"paper/NN.html","title":"è®ºæ–‡ç¥ç»ç½‘ç»œç¤ºæ„å›¾","keywords":"","body":"ç¤ºæ„å›¾ NN SVG â˜…â˜…â˜…â˜… æä¾› ä¸‰ç§å…¸å‹ çš„ç¥ç»ç½‘ç»œç»˜å›¾é£æ ¼ï¼Œä¸ªæ€§åŒ–å‚æ•°å¤š äº¤äº’å¼ç»˜å›¾ jettan/tikz_cnn â˜…â˜† åŸºäºtikzçš„texçš„å®æŒ‡ä»¤ç»˜åˆ¶ ç»˜åˆ¶è„šæœ¬ç¹æ‚ PlotNeuralNet â˜…â˜…â˜…â˜† åº•å±‚åŸºäºtikzçš„texçš„å®æŒ‡ä»¤ç»˜åˆ¶ ä¸Šå±‚æä¾›åŸºäºpythonçš„æè¿°æ¡†æ¶ï¼Œç»˜åˆ¶è„šæœ¬ç®€å• å¯ç»˜åˆ¶å¤æ‚çš„ç½‘ç»œç»“æ„ ConvNetDraw â˜…â˜… åŸºäºjavascriptå’Œcssç»˜åˆ¶ ä»…æ”¯æŒåŸºæœ¬Layerç±»å‹ gwding/draw_convnet â˜…â˜…â˜† ç®€å•æ˜“ç”¨ åº•å±‚åŸºäºmatplotlibç»˜åˆ¶ ajtulloch/dnngraph ï¼ˆä¾¿æ·æ€§ä¸å¥½è¯„ä»·ï¼Œæš‚ä¸æ‰“åˆ†ï¼‰ åŸºäºHaskellè¯­è¨€ è®¡ç®—å›¾ lutzroeder/netron â˜…â˜…â˜…â˜…â˜… ï¼ˆ2019.4.30æ–°å¢ï¼Œæƒ­æ„§ï¼‰ æ”¯æŒå·¥å…·: ONNX, Keras, CoreML, Caffe2, MXNet, TensorFlow Lite, Caffe, PyTorch, Torch, CNTK, PaddlePaddle, Darknet, scikit-learn, TensorFlow.js, TensorFlow. æä¾›è¿è¡Œæ–¹å¼: æµè§ˆå™¨, Python Server; macOS, Linux, Windows draw_net.py â˜…â˜…â˜† caffeè‡ªå¸¦çš„ç”»å›¾å·¥å…· Netscope â˜…â˜…â˜… éå¸¸æ˜“ç”¨ ä»…æ”¯æŒç½‘é¡µç‰ˆ Netscope-dgschwend â˜…â˜…â˜…â˜† åŸºäºNetscopeäºŒæ¬¡å¼€å‘ ç»™å‡ºç½‘ç»œçš„å„ç§è®¡ç®—æ“ä½œæ¬¡æ•°ï¼ˆéå¸¸æ–¹ä¾¿ï¼ï¼‰ TFLearn â˜…â˜…â˜…â˜† å…¶å­å·¥å…·æä¾›äº†æ¨¡å‹å¯è§†åŒ–çš„åŠŸèƒ½ Tensorboard â˜…â˜…â˜… å…¶å­å·¥å…·æä¾›äº†æ¨¡å‹å¯è§†åŒ–çš„åŠŸèƒ½ å¦‚æœè¿˜æœ‰é‡åˆ°ï¼Œæˆ‘ä¼šç»§ç»­è¡¥å……çš„ã€‚ 2019å¹´3æœˆ24æ—¥æ›´æ–° NN SVG HarisIqbal88/PlotNeuralNet jettan/tikz_cnn https://cbovar.github.io/ConvNetDraw/ 2017.11.04æ›´æ–° Tensorboard 2017.10.13æ›´æ–° TFLearn 2016.9.10æ›´æ–° Netscope Netscope-dgschwend æœ€æ—©å›ç­” ajtulloch/dnngraph caffe/draw_net.py gwding/draw_convnet "},"super_resolution/":{"url":"super_resolution/","title":"è¶…åˆ†è¾¨ç‡","keywords":"","body":" è¶…åˆ†è¾¨ç‡æ–¹å‘ç»¼è¿° è¶…åˆ†è¾¨ç‡æŠ€æœ¯ è¶…åˆ†è¾¨ç‡ä»£ç æ•°æ®é›†åˆé›† è¶…åˆ†è¾¨ç‡baseline è¶…åˆ†è¾¨ç‡çš„æŸå¤±å‡½æ•°æ€»ç»“ "},"super_resolution/SR_summarize.html":{"url":"super_resolution/SR_summarize.html","title":"è¶…åˆ†è¾¨ç‡æ–¹å‘ç»¼è¿°","keywords":"","body":"åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡å›¾åƒæŠ€æœ¯ä¸€è§ˆ è¿‘å¹´æ¥ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å–å¾—äº†æ˜¾ç€è¿›æ­¥ã€‚ä¸€èˆ¬å¯ä»¥å°†ç°æœ‰çš„SRæŠ€æœ¯ç ”ç©¶å¤§è‡´åˆ†ä¸ºä¸‰å¤§ç±»ï¼šç›‘ç£SRï¼Œæ— ç›‘ç£SRå’Œç‰¹å®šé¢†åŸŸSRï¼ˆäººè„¸ï¼‰ã€‚ å…ˆè¯´ç›‘ç£SR å¦‚ä»Šå·²ç»æœ‰å„ç§æ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¾èµ–äºæœ‰ç›‘ç£çš„è¶…åˆ†è¾¨ç‡ï¼Œå³ç”¨LRå›¾åƒå’Œç›¸åº”çš„åŸºç¡€äº‹å®ï¼ˆGTï¼‰HRå›¾åƒè®­ç»ƒã€‚è™½ç„¶è¿™äº›æ¨¡å‹ä¹‹é—´çš„å·®å¼‚éå¸¸å¤§ï¼Œä½†å®ƒä»¬æœ¬è´¨ä¸Šæ˜¯ä¸€ç»„ç»„ä»¶çš„ç»„åˆï¼Œä¾‹å¦‚æ¨¡å‹æ¡†æ¶ï¼Œä¸Šé‡‡æ ·æ–¹æ³•ï¼Œç½‘ç»œè®¾è®¡å’Œå­¦ä¹ ç­–ç•¥ç­‰ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œç ”ç©¶äººå‘˜å°†è¿™äº›ç»„ä»¶ç»„åˆèµ·æ¥æ„å»ºä¸€ä¸ªç”¨äºæ‹Ÿåˆç‰¹å®šä»»åŠ¡çš„é›†æˆSRæ¨¡å‹ã€‚ ç”±äºå›¾åƒè¶…åˆ†è¾¨ç‡æ˜¯ä¸€ä¸ªç—…æ€é—®é¢˜ï¼Œå¦‚ä½•è¿›è¡Œä¸Šé‡‡æ ·ï¼ˆå³ä»ä½åˆ†è¾¨ç‡äº§ç”Ÿé«˜åˆ†è¾¨ç‡ï¼‰æ˜¯å…³é”®é—®é¢˜ã€‚åŸºäºé‡‡ç”¨çš„ä¸Šé‡‡æ ·æ“ä½œåŠå…¶åœ¨æ¨¡å‹ä¸­çš„ä½ç½®ï¼ŒSRæ¨¡å‹å¯å½’å› äºå››ç§æ¨¡å‹æ¡†æ¶ï¼šé¢„å…ˆé‡‡æ ·SRï¼Œåä¸Šé‡‡æ ·SRï¼Œæ¸è¿›ä¸Šé‡‡æ ·SRå’Œè¿­ä»£ä¸Šä¸‹é‡‡æ ·SRï¼Œå¦‚å›¾æ‰€ç¤ºã€‚ é™¤äº†åœ¨æ¨¡å‹ä¸­çš„ä½ç½®ä¹‹å¤–ï¼Œä¸Šé‡‡æ ·æ“ä½œå¦‚ä½•å®ç°å®ƒä»¬ä¹Ÿéå¸¸é‡è¦ã€‚ä¸ºäº†å…‹æœæ’å€¼æ³•çš„ç¼ºç‚¹ï¼Œå¹¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ ä¸Šé‡‡æ ·æ“ä½œï¼Œè½¬ç½®å·ç§¯å±‚ï¼ˆTransposed Convolution Layerï¼‰å’Œäºšåƒç´ å±‚ï¼ˆSub-pixel Layerï¼‰å¯ä»¥å¼•å…¥åˆ°è¶…åˆ†è¾¨ç‡ä¸­ã€‚ è½¬ç½®å·ç§¯å±‚ï¼Œå³åå·ç§¯å±‚ï¼ŒåŸºäºå°ºå¯¸ç±»ä¼¼äºå·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾å›¾æ¥é¢„æµ‹å¯èƒ½çš„è¾“å…¥ã€‚å…·ä½“åœ°è¯´ï¼Œå®ƒé€šè¿‡æ’å…¥é›¶å€¼å¹¶æ‰§è¡Œå·ç§¯æ¥æ‰©å±•å›¾åƒï¼Œä»è€Œæé«˜äº†å›¾åƒåˆ†è¾¨ç‡ã€‚ä¸ºäº†ç®€æ´èµ·è§ï¼Œä»¥3Ã—3å†…æ ¸æ‰§è¡Œ2æ¬¡ä¸Šé‡‡æ ·ä¸ºä¾‹ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚é¦–å…ˆï¼Œè¾“å…¥æ‰©å±•åˆ°åŸå§‹å¤§å°çš„ä¸¤å€ï¼Œå…¶ä¸­æ–°æ·»åŠ çš„åƒç´ å€¼è¢«è®¾ç½®ä¸º0ï¼ˆbï¼‰ã€‚ç„¶ååº”ç”¨å¤§å°ä¸º3Ã—3ã€æ­¥é•¿1å’Œå¡«å……1çš„å†…æ ¸å·ç§¯ï¼ˆcï¼‰ã€‚è¿™æ ·è¾“å…¥ç‰¹å¾å›¾å®ç°å› å­ä¸º2çš„ä¸Šé‡‡æ ·ï¼Œè€Œæ„Ÿå—é‡æœ€å¤šä¸º2Ã—2ã€‚ ç”±äºè½¬ç½®å·ç§¯å±‚å¯ä»¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æ”¾å¤§å›¾åƒå¤§å°ï¼ŒåŒæ—¶ä¿æŒä¸vanillaå·ç§¯å…¼å®¹çš„è¿æ¥æ¨¡å¼ï¼Œå› æ­¤å®ƒè¢«å¹¿æ³›ç”¨ä½œSRæ¨¡å‹çš„ä¸Šé‡‡æ ·å±‚ã€‚ç„¶è€Œï¼Œå®ƒå¾ˆå®¹æ˜“åœ¨æ¯ä¸ªè½´ä¸Šäº§ç”Ÿâ€œä¸å‡åŒ€é‡å ï¼ˆuneven overlappingï¼‰â€ï¼Œå¹¶ä¸”åœ¨ä¸¤ä¸ªè½´çš„ä¹˜æ³•è¿›ä¸€æ­¥äº§ç”Ÿäº†ç‰¹æœ‰çš„ä¸åŒå¹…åº¦æ£‹ç›˜çŠ¶å›¾æ¡ˆï¼Œä»è€ŒæŸå®³äº†SRæ€§èƒ½ã€‚ äºšåƒç´ å±‚ä¹Ÿæ˜¯ç«¯åˆ°ç«¯å­¦ä¹ çš„ä¸Šé‡‡æ ·å±‚ï¼Œé€šè¿‡å·ç§¯ç”Ÿæˆå¤šä¸ªé€šé“ç„¶åé‡æ–°æ•´å½¢ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚é¦–å…ˆå·ç§¯äº§ç”Ÿå…·æœ‰s2å€é€šé“çš„è¾“å‡ºï¼Œå…¶ä¸­sæ˜¯ä¸Šé‡‡æ ·å› å­ï¼ˆbï¼‰ã€‚å‡è®¾è¾“å…¥å¤§å°ä¸ºhÃ—wÃ—cï¼Œåˆ™è¾“å‡ºå¤§å°ä¸ºhÃ—wÃ—s2cã€‚ä¹‹åï¼Œæ‰§è¡Œæ•´å½¢ï¼ˆshuffleï¼‰æ“ä½œäº§ç”Ÿå¤§å°ä¸ºshÃ—swÃ—cçš„è¾“å‡ºï¼ˆcï¼‰ã€‚æ„Ÿå—é‡å¤§å°å¯ä»¥è¾¾åˆ°3Ã—3ã€‚ ç”±äºç«¯åˆ°ç«¯çš„ä¸Šé‡‡æ ·æ–¹å¼ï¼Œäºšåƒç´ å±‚ä¹Ÿè¢«SRæ¨¡å‹å¹¿æ³›ä½¿ç”¨ã€‚ä¸è½¬ç½®å·ç§¯å±‚ç›¸æ¯”ï¼Œäºšåƒç´ å±‚çš„æœ€å¤§ä¼˜åŠ¿æ˜¯å…·æœ‰è¾ƒå¤§çš„æ„ŸçŸ¥åœºï¼Œæä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œèƒ½å¸®åŠ©ç”Ÿæˆæ›´å‡†ç¡®çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œäºšåƒç´ å±‚çš„æ„Ÿå—é‡çš„åˆ†å¸ƒæ˜¯ä¸å‡åŒ€çš„ï¼Œå—çŠ¶åŒºåŸŸå®é™…ä¸Šå…±äº«ç›¸åŒçš„æ„Ÿå—é‡ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨å—è¾¹ç•Œé™„è¿‘çš„ä¸€äº›ç•¸å˜ã€‚ å„ç§æ·±åº¦å­¦ä¹ çš„æ¨¡å‹å·²ç»è¢«ç”¨äºSRï¼Œå¦‚å›¾æ‰€ç¤ºã€‚ ResNetå­¦ä¹ æ®‹å·®è€Œä¸æ˜¯å½»åº•çš„æ˜ å°„ï¼Œå·²è¢«SRæ¨¡å‹å¹¿æ³›é‡‡ç”¨ï¼Œå¦‚ä¸Šå›¾ï¼ˆaï¼‰æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œæ®‹å·®å­¦ä¹ ç­–ç•¥å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼Œå³å…¨å±€å’Œå±€éƒ¨æ®‹å·®å­¦ä¹ ã€‚ ç”±äºè¶…åˆ†è¾¨ç‡æ˜¯å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ï¼Œå…¶ä¸­è¾“å…¥å›¾åƒä¸ç›®æ ‡å›¾åƒé«˜åº¦ç›¸å…³ï¼Œå…¨å±€æ®‹å·®å­¦ä¹ ä»…å­¦ä¹ ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„æ®‹å·®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒé¿å…å­¦ä¹ ä»å®Œæ•´å›¾åƒåˆ°å¦ä¸€ä¸ªå›¾åƒçš„å¤æ‚è½¬æ¢ï¼Œè€Œåªéœ€è¦å­¦ä¹ æ®‹å·®å›¾æ¥æ¢å¤ä¸¢å¤±çš„é«˜é¢‘ç»†èŠ‚ã€‚ç”±äºå¤§å¤šæ•°åŒºåŸŸæ®‹å·®æ¥è¿‘äºé›¶ï¼Œæ¨¡å‹çš„å¤æ‚æ€§å’Œå­¦ä¹ éš¾åº¦éƒ½å¤§å¤§é™ä½ã€‚è¿™ç§æ–¹æ³•åœ¨é¢„ä¸Šé‡‡æ ·çš„SRæ¡†æ¶æ™®éé‡‡ç”¨ã€‚ å±€éƒ¨æ®‹å·®å­¦ä¹ ç±»ä¼¼äºResNetçš„æ®‹å·®å­¦ä¹ ï¼Œç”¨äºç¼“è§£ä¸æ–­å¢åŠ çš„ç½‘ç»œæ·±åº¦å¼•èµ·çš„é€€åŒ–é—®é¢˜å¹¶æé«˜å­¦ä¹ èƒ½åŠ›ã€‚ å®è·µä¸­ï¼Œä¸Šè¿°æ–¹æ³•éƒ½æ˜¯é€šè¿‡å¿«æ·è¿æ¥ï¼ˆé€šå¸¸æœ‰å°å¸¸æ•°å› å­çš„ç¼©æ”¾ï¼‰å’Œé€å…ƒç´ åŠ æ³•æ“ä½œå®ç°çš„ã€‚åŒºåˆ«åœ¨äºï¼Œå‰è€…ç›´æ¥è¿æ¥è¾“å…¥å›¾åƒå’Œè¾“å‡ºå›¾åƒï¼Œè€Œåè€…é€šå¸¸åœ¨ä¸åŒæ·±åº¦çš„ç½‘ç»œä¸­å±‚ä¹‹é—´æ·»åŠ å¤šä¸ªå¿«æ·æ–¹å¼ã€‚ â€¢ é€’å½’å­¦ä¹  é€’å½’å­¦ä¹ ï¼ˆä»¥é€’å½’æ–¹å¼å¤šæ¬¡åº”ç”¨ç›¸åŒæ¨¡å—ï¼‰ä¹Ÿè¢«è¶…åˆ†è¾¨ç‡é‡‡ç”¨ï¼Œå¦‚ä¸Šå›¾ ï¼ˆbï¼‰æ‰€ç¤ºã€‚åœ¨å®è·µä¸­ï¼Œé€’å½’å­¦ä¹ å›ºæœ‰åœ°å¸¦æ¥äº†æ¶ˆå¤±ï¼ˆvanishingï¼‰æˆ–çˆ†æ¶¨ï¼ˆexplodingï¼‰æ¢¯åº¦é—®é¢˜ï¼Œå› æ­¤æ®‹å·®å­¦ä¹ å’Œå¤šä¿¡å·ç›‘ç£ç­‰ä¸€äº›æŠ€æœ¯é€šå¸¸ä¸é€’å½’å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥å‡è½»è¿™äº›é—®é¢˜ã€‚ â€¢ é€šé“å…³æ³¨ è€ƒè™‘åˆ°ä¸åŒé€šé“ä¹‹é—´ç‰¹å¾è¡¨å¾çš„ç›¸äº’ä¾èµ–å’Œä½œç”¨ï¼Œä¸€ç§â€œæŒ¤å‹-æ¿€å‘ï¼ˆSAEï¼Œsqueeze-and-excitationï¼‰â€æ¨¡å—æ˜ç¡®å¯¹é€šé“ç›¸äº’ä¾èµ–æ€§å»ºæ¨¡ï¼Œæ¥æé«˜è¡¨ç¤ºèƒ½åŠ›ï¼Œå¦‚ä¸Šå›¾ï¼ˆcï¼‰æ‰€ç¤ºã€‚å…¶ä¸­ç”¨å…¨å±€å¹³å‡æ± åŒ–å°†æ¯ä¸ªè¾“å…¥é€šé“å‹ç¼©åˆ°é€šé“æè¿°å­ï¼ˆå³ä¸€ä¸ªå¸¸æ•°ï¼‰ä¸­ï¼Œç„¶åå°†è¿™äº›æè¿°å­é¦ˆé€åˆ°ä¸¤ä¸ªå…¨è¿æ¥å±‚äº§ç”Ÿé€šé“å°ºåº¦å› å­ã€‚åŸºäºé€šé“ä¹˜æ³•ï¼Œç”¨å°ºåº¦å› å­é‡æ–°ç¼©æ”¾è¾“å…¥é€šé“å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚ â€¢ è‡´å¯†è¿æ¥ è‡´å¯†è¿æ¥åœ¨è§†è§‰ä»»åŠ¡ä¸­å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚åœ¨è‡´å¯†å—çš„æ¯ä¸ªå±‚ï¼Œæ‰€æœ‰å‰å±‚çš„ç‰¹å¾å›¾ç”¨ä½œè¾“å…¥ï¼Œå¹¶ä¸”å…¶è‡ªèº«ç‰¹å¾å›¾ç”¨ä½œæ‰€æœ‰åç»­å±‚çš„è¾“å…¥ï¼Œåœ¨ä¸€ä¸ªæœ‰lå±‚è‡´å¯†å—ä¸­å¸¦æ¥lÂ·ï¼ˆl - 1ï¼‰/ 2ä¸ªè¿æ¥ã€‚è‡´å¯†è¿æ¥ï¼Œä¸ä»…æœ‰åŠ©äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€å¢å¼ºä¿¡å·çš„ä¼ æ’­å¹¶ä¿ƒè¿›ç‰¹å¾é‡ç”¨ï¼Œè€Œä¸”åœ¨è¿æ¥ä¹‹åé‡‡ç”¨å°å¢é•¿ç‡ï¼ˆå³è‡´å¯†å—çš„é€šé“æ•°ï¼‰å’Œé€šé“ç¼©å‡æ¥å¤§å¤§å‡å°‘å‚æ•°é‡ã€‚ ä¸ºäº†èåˆä½çº§å’Œé«˜çº§ç‰¹å¾ä»¥æä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯æ¥é‡å»ºé«˜è´¨é‡çš„ç»†èŠ‚ï¼Œè‡´å¯†è¿æ¥è¢«å¼•å…¥SRé¢†åŸŸï¼Œå¦‚ä¸Šå›¾ï¼ˆdï¼‰æ‰€ç¤ºã€‚ â€¢ å¤šè·¯å¾„å­¦ä¹  å¤šè·¯å¾„å­¦ä¹ æŒ‡æ¨¡å‹å­˜åœ¨å¤šä¸ªè·¯å¾„ä¼ é€’ç‰¹å¾ï¼Œè¿™äº›è·¯å¾„æ‰§è¡Œä¸åŒçš„æ“ä½œä»¥æä¾›æ›´å¥½çš„å»ºæ¨¡åŠŸèƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒå¯ä»¥åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼šå…¨å±€æ³•ã€å±€éƒ¨æ³•å’Œç‰¹å®šå°ºåº¦æ³•ã€‚ å…¨å±€å¤šè·¯å¾„å­¦ä¹ æ˜¯æŒ‡ç”¨å¤šä¸ªè·¯å¾„æå–å›¾åƒä¸åŒæ–¹é¢çš„ç‰¹å¾ã€‚è¿™äº›è·¯å¾„å¯ä»¥åœ¨ä¼ æ’­ä¸­ç›¸äº’äº¤å‰ï¼Œä»è€Œå¤§å¤§å¢å¼ºäº†ç‰¹å¾æå–çš„èƒ½åŠ›ã€‚ æœ¬åœ°å¤šè·¯å¾„å­¦ä¹ ç”¨æ–°å—è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œå¦‚ä¸Šå›¾ï¼ˆeï¼‰æ‰€ç¤ºã€‚è¯¥å—é‡‡ç”¨ä¸åŒå†…æ ¸å¤§å°çš„å·ç§¯åŒæ—¶æå–ç‰¹å¾ï¼Œç„¶åå°†è¾“å‡ºè¿æ¥èµ·æ¥å¹¶å†æ¬¡è¿›è¡Œç›¸åŒçš„æ“ä½œã€‚å¿«æ·æ–¹å¼é€šè¿‡é€å…ƒç´ æ·»åŠ æ¥è¿æ¥è¯¥å—çš„è¾“å‡ºå’Œè¾“å…¥ã€‚é€šè¿‡è¿™ç§å±€éƒ¨å¤šè·¯å¾„å­¦ä¹ ï¼ŒSRæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ä»å¤šä¸ªå°ºåº¦æå–å›¾åƒç‰¹å¾ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚ ç‰¹å®šå°ºåº¦å¤šè·¯å¾„å­¦ä¹ å…±äº«æ¨¡å‹çš„ä¸»è¦éƒ¨åˆ†ï¼ˆå³ç‰¹å¾æå–çš„ä¸­é—´éƒ¨åˆ†ï¼‰ï¼Œå¹¶åˆ†åˆ«åœ¨ç½‘ç»œçš„å¼€å¤´å’Œç»“å°¾é™„åŠ ç‰¹å®šå°ºåº¦çš„é¢„å¤„ç†è·¯å¾„å’Œä¸Šé‡‡æ ·è·¯å¾„ï¼Œå¦‚ä¸Šå›¾ï¼ˆfï¼‰æ‰€ç¤ºã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œä»…å¯ç”¨å’Œæ›´æ–°ä¸æ‰€é€‰å°ºåº¦å¯¹åº”çš„è·¯å¾„ã€‚è¿™æ ·å¤§å¤šæ•°å‚æ•°åœ¨ä¸åŒå°ºåº¦ä¸Šå…±äº«ã€‚ â€¢ é«˜çº§å·ç§¯ å·ç§¯è¿ç®—æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œçš„åŸºç¡€ï¼Œæ”¹è¿›å·ç§¯è¿ç®—å¯è·å¾—æ›´å¥½çš„æ€§èƒ½æˆ–æ›´å¿«çš„é€Ÿåº¦ã€‚è¿™é‡Œç»™å‡ºä¸¤ä¸ªæ–¹æ³•ï¼šæ‰©å¼ å·ç§¯ï¼ˆDilated Convolutionï¼‰å’Œç¾¤å·ç§¯ï¼ˆGroup Convolutionï¼‰ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰åŠ©äºåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ç”Ÿæˆé€¼çœŸçš„ç»†èŠ‚ã€‚æ‰©å¼ å·ç§¯èƒ½å°†æ„Ÿå—é‡å¢åŠ ä¸¤å€ï¼Œæœ€ç»ˆå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚ç¾¤å·ç§¯ä»¥å¾ˆå°‘çš„æ€§èƒ½æŸå¤±å¯å‡å°‘å¤§é‡çš„å‚æ•°å’Œæ“ä½œï¼Œå¦‚ä¸Šå›¾ï¼ˆgï¼‰æ‰€ç¤ºã€‚ â€¢ åƒç´ é€’å½’å­¦ä¹  å¤§å¤šæ•°SRæ¨¡å‹è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸åƒç´ æ— å…³çš„ä»»åŠ¡ï¼Œå› æ­¤æ— æ³•æ­£ç¡®åœ°ç¡®å®šç”Ÿæˆåƒç´ ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚åœ¨äººæ³¨æ„åŠ›è½¬ç§»æœºåˆ¶æ¨åŠ¨ä¸‹ï¼Œä¸€ç§é€’æ¨ç½‘ç»œå¯ä¾æ¬¡å‘ç°å‚ä¸çš„è¡¥ä¸å¹¶è¿›è¡Œå±€éƒ¨å¢å¼ºã€‚ä»¥è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå›¾åƒè‡ªèº«ç‰¹æ€§è‡ªé€‚åº”åœ°ä¸ªæ€§åŒ–æœ€ä½³æœç´¢è·¯å¾„ï¼Œä»è€Œå……åˆ†åˆ©ç”¨å›¾åƒå…¨å±€çš„å†…ä¾èµ–æ€§ï¼ˆintra-dependenceï¼‰ã€‚ä¸è¿‡ï¼Œéœ€è¦é•¿ä¼ æ’­è·¯å¾„çš„é€’å½’è¿‡ç¨‹ï¼Œç‰¹åˆ«å¯¹è¶…åˆ†è¾¨ç‡çš„HRå›¾åƒï¼Œå¤§å¤§å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œè®­ç»ƒéš¾åº¦ã€‚ â€¢ é‡‘å­—å¡”æ± åŒ– é‡‘å­—å¡”æ± åŒ–æ¨¡å—æ›´å¥½åœ°åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚ä¸Šå›¾ï¼ˆhï¼‰æ‰€ç¤ºã€‚å…·ä½“åœ°ï¼Œå¯¹äºå°ºå¯¸ä¸ºhÃ—wÃ—cçš„ç‰¹å¾å›¾ï¼Œæ¯ä¸ªç‰¹å¾å›¾è¢«åˆ’åˆ†ä¸ºMÃ—Mä¸ªåŒºé—´ï¼Œå¹¶ç»å†å…¨å±€å¹³å‡æ± åŒ–äº§ç”ŸMÃ—MÃ—cä¸ªè¾“å‡ºã€‚ç„¶åï¼Œæ‰§è¡Œ1Ã—1å·ç§¯è¾“å‡ºå‹ç¼©åˆ°ä¸€ä¸ªå•ä¿¡é“ã€‚ä¹‹åï¼Œé€šè¿‡åŒçº¿æ€§æ’å€¼å°†ä½ç»´ç‰¹å¾å›¾ä¸Šé‡‡æ ·åˆ°ä¸åŸå§‹ç‰¹å¾å›¾ç›¸åŒçš„å¤§å°ã€‚ä½¿ç”¨ä¸åŒçš„Mï¼Œè¯¥æ¨¡å—å¯ä»¥æœ‰æ•ˆåœ°æ•´åˆå…¨å±€å’Œå±€éƒ¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ â€¢ å°æ³¢å˜æ¢ ä¼—æ‰€å‘¨çŸ¥ï¼Œå°æ³¢å˜æ¢ï¼ˆWTï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„å›¾åƒè¡¨ç¤ºï¼Œå°†å›¾åƒä¿¡å·åˆ†è§£ä¸ºè¡¨ç¤ºçº¹ç†ç»†èŠ‚çš„é«˜é¢‘å°æ³¢å’ŒåŒ…å«å…¨å±€æ‹“æ‰‘ä¿¡æ¯çš„ä½é¢‘å°æ³¢ã€‚å°†WTä¸åŸºäºæ·±åº¦å­¦ä¹ çš„SRæ¨¡å‹ç›¸ç»“åˆï¼Œè¿™æ ·æ’å€¼LRå°æ³¢çš„å­å¸¦ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹ç›¸åº”HRå­å¸¦çš„æ®‹å·®ã€‚WTå’Œé€†WTåˆ†åˆ«ç”¨äºåˆ†è§£LRè¾“å…¥å’Œé‡å»ºHRè¾“å‡ºã€‚ å¦å¤–å­¦ä¹ ç­–ç•¥é—®é¢˜ï¼Œæ¶‰åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼ˆåŒ…æ‹¬åƒç´ æŸå¤±ï¼Œå†…å®¹æŸå¤±ï¼Œçº¹ç†æŸå¤±ï¼Œå¯¹æŠ—æŸå¤±å’Œå‘¨æœŸè¿ç»­æŸå¤±ï¼‰ã€æ‰¹å¤„ç†å½’ä¸€åŒ–ï¼ˆBNï¼‰ã€è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰å’Œå¤šä¿¡å·ç›‘ç£ï¼ˆMulti-supervisionï¼‰ç­‰ç­‰ã€‚ å†è¯´æ— ç›‘ç£SR ç°æœ‰çš„è¶…åˆ†è¾¨ç‡å·¥ä½œä¸»è¦é›†ä¸­åœ¨ç›‘ç£å­¦ä¹ ä¸Šï¼Œç„¶è€Œéš¾ä»¥æ”¶é›†ä¸åŒåˆ†è¾¨ç‡çš„ç›¸åŒåœºæ™¯çš„å›¾åƒï¼Œå› æ­¤é€šå¸¸é€šè¿‡å¯¹HRå›¾åƒé¢„å®šä¹‰é€€åŒ–æ¥è·å¾—SRæ•°æ®é›†ä¸­çš„LRå›¾åƒã€‚ä¸ºäº†é˜²æ­¢é¢„å®šä¹‰é€€åŒ–å¸¦æ¥çš„ä¸åˆ©å½±å“ï¼Œæ— ç›‘ç£çš„è¶…åˆ†è¾¨ç‡æˆä¸ºé€‰æ‹©ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæä¾›éé…å¯¹å›¾åƒï¼ˆHRæˆ–LRï¼‰ç”¨äºè®­ç»ƒï¼Œå®é™…ä¸Šå¾—åˆ°çš„æ¨¡å‹æ›´å¯èƒ½åº”å¯¹å®é™…åœºæ™¯ä¸­çš„SRé—®é¢˜ã€‚ â€¢ é›¶å‡»ï¼ˆzero shotï¼‰è¶…åˆ†è¾¨ç‡ å•ä¸ªå›¾åƒå†…éƒ¨çš„ç»Ÿè®¡æ•°æ®è¶³ä»¥æä¾›è¶…åˆ†è¾¨ç‡æ‰€éœ€çš„ä¿¡æ¯ï¼Œæ‰€ä»¥é›¶å‡»è¶…åˆ†è¾¨ç‡ï¼ˆZSSRï¼‰åœ¨æµ‹è¯•æ—¶è®­ç»ƒå°å›¾åƒç‰¹å®šçš„SRç½‘ç»œè¿›è¡Œæ— ç›‘ç£SRï¼Œè€Œä¸æ˜¯åœ¨å¤§æ•°æ®é›†ä¸Šè®­ç»ƒé€šç”¨æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ ¸ä¼°è®¡æ–¹æ³•ç›´æ¥ä»å•ä¸ªæµ‹è¯•å›¾åƒä¼°è®¡é€€åŒ–å†…æ ¸ï¼Œå¹¶åœ¨æµ‹è¯•å›¾åƒä¸Šæ‰§è¡Œä¸åŒå°ºåº¦å› å­çš„é€€åŒ–æ¥æ„å»ºå°æ•°æ®é›†ã€‚ç„¶ååœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒè¶…åˆ†è¾¨ç‡çš„å°CNNæ¨¡å‹ç”¨äºæœ€ç»ˆé¢„æµ‹ã€‚ ZSSRåˆ©ç”¨å›¾åƒå†…éƒ¨ç‰¹å®šä¿¡æ¯çš„è·¨å°ºåº¦å¤ç°è¿™ä¸€ç‰¹ç‚¹ï¼Œå¯¹éç†æƒ³æ¡ä»¶ä¸‹ï¼ˆébi-cubicé€€åŒ–æ ¸è·å¾—çš„å›¾åƒï¼Œå—æ¨¡ç³Šã€å™ªå£°å’Œå‹ç¼©ç•¸å˜ç­‰å½±å“ï¼‰æ›´æ¥è¿‘ç°å®ä¸–ç•Œåœºæ™¯çš„å›¾åƒï¼Œæ¯”ä»¥å‰çš„æ–¹æ³•æ€§èƒ½æé«˜ä¸€å¤§æˆªï¼ŒåŒæ—¶åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼ˆbi-cubicæ’å€¼æ„å»ºçš„å›¾åƒï¼‰ï¼Œå’Œä»¥å‰æ–¹æ³•ç»“æœå·®ä¸å¤šã€‚å°½ç®¡è¿™æ ·ï¼Œç”±äºéœ€è¦åœ¨æµ‹è¯•æœŸé—´ä¸ºæ¯ä¸ªå›¾åƒè®­ç»ƒå•ä¸ªç½‘ç»œï¼Œä½¿å¾—å…¶æµ‹è¯•æ—¶é—´è¿œæ¯”å…¶ä»–SRæ¨¡å‹é•¿ã€‚ â€¢ å¼±ç›‘ç£SR ä¸ºäº†åœ¨è¶…åˆ†è¾¨ç‡ä¸­ä¸å¼•å…¥é¢„é€€åŒ–ï¼Œå¼±ç›‘ç£å­¦ä¹ çš„SRæ¨¡å‹ï¼Œå³ä½¿ç”¨ä¸æˆå¯¹çš„LR-HRå›¾åƒï¼Œæ˜¯ä¸€ç§æ–¹æ¡ˆã€‚ä¸€äº›æ–¹æ³•å­¦ä¹ HR-LRé€€åŒ–æ¨¡å‹å¹¶ç”¨äºæ„å»ºè®­ç»ƒSRæ¨¡å‹çš„æ•°æ®é›†ï¼Œè€Œå¦å¤–ä¸€äº›æ–¹æ³•è®¾è®¡å‘¨æœŸå¾ªç¯ï¼ˆcycle-in-cycleï¼‰ç½‘ç»œåŒæ—¶å­¦ä¹ LR-HRå’ŒHR-LRæ˜ å°„ã€‚ ç”±äºé¢„é€€åŒ–æ˜¯æ¬¡ä¼˜çš„ï¼Œä»æœªé…å¯¹çš„LR-HRæ•°æ®é›†ä¸­å­¦ä¹ é€€åŒ–æ˜¯å¯è¡Œçš„ã€‚ä¸€ç§æ–¹æ³•ç§°ä¸ºâ€œä¸¤æ­¥æ³•â€ï¼š 1ï¼‰è®­ç»ƒHR-LR çš„GANæ¨¡å‹ï¼Œç”¨ä¸æˆå¯¹çš„LR-HRå›¾åƒå­¦ä¹ é€€åŒ–ï¼› 2ï¼‰åŸºäºç¬¬ä¸€ä¸ªGANæ¨¡å‹ï¼Œä½¿ç”¨æˆå¯¹çš„LR-HRå›¾åƒè®­ç»ƒLR- HR çš„GANæ¨¡å‹æ‰§è¡ŒSRã€‚ å¯¹äºHRåˆ°LR çš„GANæ¨¡å‹ï¼ŒHRå›¾åƒè¢«é¦ˆé€åˆ°ç”Ÿæˆå™¨äº§ç”ŸLRè¾“å‡ºï¼Œä¸ä»…éœ€è¦åŒ¹é…HRå›¾åƒç¼©å°ï¼ˆå¹³å‡æ± åŒ–ï¼‰è·å¾—çš„LRå›¾åƒï¼Œè€Œä¸”è¿˜è¦åŒ¹é…çœŸå®LRå›¾åƒçš„åˆ†å¸ƒã€‚è®­ç»ƒä¹‹åï¼Œç”Ÿæˆå™¨ä½œä¸ºé€€åŒ–æ¨¡å‹ç”ŸæˆLR-HRå›¾åƒå¯¹ã€‚ å¯¹äºLRåˆ°HR çš„GANæ¨¡å‹ï¼Œç”Ÿæˆå™¨ï¼ˆå³SRæ¨¡å‹ï¼‰å°†ç”Ÿæˆçš„LRå›¾åƒä½œä¸ºè¾“å…¥å¹¶é¢„æµ‹HRè¾“å‡ºï¼Œä¸ä»…éœ€è¦åŒ¹é…ç›¸åº”çš„HRå›¾åƒè€Œä¸”è¿˜åŒ¹é…HRå›¾åƒçš„åˆ†å¸ƒ ã€‚ åœ¨â€œä¸¤æ­¥æ³•â€ä¸­ï¼Œæ— ç›‘ç£æ¨¡å‹æœ‰æ•ˆåœ°æé«˜äº†è¶…åˆ†è¾¨ç‡çœŸå®ä¸–ç•ŒLRå›¾åƒçš„è´¨é‡ï¼Œæ¯”ä»¥å‰æ–¹æ³•æ€§èƒ½è·å¾—äº†å¾ˆå¤§æ”¹è¿›ã€‚ æ— ç›‘ç£SRçš„å¦ä¸€ç§æ–¹æ³•æ˜¯å°†LRç©ºé—´å’ŒHRç©ºé—´è§†ä¸ºä¸¤ä¸ªåŸŸï¼Œå¹¶ä½¿ç”¨å‘¨æœŸå¾ªç¯ç»“æ„å­¦ä¹ å½¼æ­¤ä¹‹é—´çš„æ˜ å°„ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒç›®çš„åŒ…æ‹¬æ¨é€æ˜ å°„ç»“æœå»åŒ¹é…ç›®æ ‡çš„åŸŸåˆ†å¸ƒï¼Œå¹¶é€šè¿‡æ¥å›ï¼ˆround tripï¼‰æ˜ å°„ä½¿å›¾åƒæ¢å¤ã€‚ â€¢ æ·±åº¦å›¾åƒå…ˆéªŒçŸ¥è¯† CNNç»“æ„åœ¨é€†é—®é¢˜ä¹‹å‰æ•è·å¤§é‡çš„ä½çº§å›¾åƒç»Ÿè®¡é‡ï¼Œæ‰€ä»¥åœ¨æ‰§è¡ŒSRä¹‹å‰å¯ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„CNNä½œä¸ºæ‰‹å·¥å…ˆéªŒçŸ¥è¯†ã€‚å…·ä½“åœ°è®²ï¼Œå®šä¹‰ç”Ÿæˆå™¨ç½‘ç»œï¼Œå°†éšæœºå‘é‡zä½œä¸ºè¾“å…¥å¹¶å°è¯•ç”Ÿæˆç›®æ ‡HRå›¾åƒIã€‚è®­ç»ƒç›®æ ‡æ˜¯ç½‘ç»œæ‰¾åˆ°ä¸€ä¸ªIË† yï¼Œå…¶ä¸‹é‡‡æ ·IË†yä¸LRå›¾åƒIxç›¸åŒã€‚ å› ä¸ºç½‘ç»œéšæœºåˆå§‹åŒ–ï¼Œä»æœªåœ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¿‡è®­ç»ƒï¼Œæ‰€ä»¥å”¯ä¸€çš„å…ˆéªŒçŸ¥è¯†æ˜¯CNNç»“æ„æœ¬èº«ã€‚è™½ç„¶è¿™ç§æ–¹æ³•çš„æ€§èƒ½ä»ç„¶æ¯”ç›‘ç£æ–¹æ³•å·®å¾ˆå¤šï¼Œä½†è¿œè¿œè¶…è¿‡ä¼ ç»Ÿçš„bicubicä¸Šé‡‡æ ·ã€‚æ­¤å¤–ï¼Œè¡¨ç°å‡ºçš„CNNæ¶æ„æœ¬èº«åˆç†æ€§ï¼Œä¿ƒä½¿å°†æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸CNNç»“æ„æˆ–è‡ªç›¸ä¼¼æ€§ç­‰å…ˆéªŒçŸ¥è¯†ç›¸ç»“åˆæ¥æé«˜è¶…åˆ†è¾¨ç‡ã€‚ ç‰¹å®šSR ç‰¹å®šSRé¢†åŸŸä¸»è¦åŒ…æ‹¬æ·±åº¦å›¾ã€äººè„¸å›¾åƒã€é«˜å…‰è°±å›¾åƒå’Œè§†é¢‘ç­‰å†…å®¹çš„SRåº”ç”¨ã€‚ é¢éƒ¨å›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå³é¢éƒ¨å¹»è§‰ï¼ˆFHï¼Œ face hallucinationï¼‰ï¼Œé€šå¸¸å¯ä»¥å¸®åŠ©å…¶ä»–ä¸é¢éƒ¨ç›¸å…³çš„ä»»åŠ¡ã€‚ä¸é€šç”¨å›¾åƒç›¸æ¯”ï¼Œé¢éƒ¨å›¾åƒå…·æœ‰æ›´å¤šä¸é¢éƒ¨ç›¸å…³çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå› æ­¤å°†é¢éƒ¨å…ˆéªŒçŸ¥è¯†ï¼ˆä¾‹å¦‚ï¼Œå…³é”®ç‚¹ï¼Œç»“æ„è§£æå›¾å’Œèº«ä»½ï¼‰ç»“åˆåˆ°FHä¸­æ˜¯éå¸¸æµè¡Œä¸”æœ‰å¸Œæœ›çš„æ–¹æ³•ã€‚åˆ©ç”¨é¢éƒ¨å…ˆéªŒçŸ¥è¯†çš„æœ€ç›´æ¥çš„æ–¹å¼æ˜¯çº¦æŸæ‰€ç”Ÿæˆçš„HRå›¾åƒå…·æœ‰ä¸åŸºç¡€äº‹å®ï¼ˆGTï¼‰çš„HRå›¾åƒç›¸åŒçš„é¢éƒ¨ç›¸å…³ä¿¡æ¯ã€‚ ä¸å…¨è‰²å›¾åƒï¼ˆPANï¼Œpanchromatic imagesï¼‰ï¼Œå³å…·æœ‰3ä¸ªæ³¢æ®µçš„RGBå›¾åƒç›¸æ¯”ï¼Œæœ‰æ•°ç™¾ä¸ªæ³¢æ®µçš„é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼Œhyperspectral imagesï¼‰æä¾›äº†ä¸°å¯Œçš„å…‰è°±ç‰¹å¾å¹¶æœ‰åŠ©äºå„ç§è§†è§‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºç¡¬ä»¶é™åˆ¶ï¼Œæ”¶é›†é«˜è´¨é‡çš„HSIæ¯”æ”¶é›†PANæ›´å›°éš¾ï¼Œæ”¶é›†çš„HSIåˆ†è¾¨ç‡è¦ä½å¾—å¤šã€‚å› æ­¤ï¼Œè¶…åˆ†è¾¨ç‡è¢«å¼•å…¥è¯¥é¢†åŸŸï¼Œç ”ç©¶äººå‘˜å€¾å‘äºå°†HR PANå’ŒLR HSIç»“åˆèµ·æ¥é¢„æµ‹HR HSIã€‚ å°±è§†é¢‘è¶…åˆ†è¾¨ç‡è€Œè¨€ï¼Œå¤šä¸ªå¸§æä¾›æ›´å¤šçš„åœºæ™¯ä¿¡æ¯ï¼Œä¸ä»…æœ‰å¸§å†…ç©ºé—´ä¾èµ–æ€§è€Œä¸”æœ‰å¸§é—´æ—¶é—´ä¾èµ–æ€§ï¼ˆä¾‹å¦‚ï¼Œè¿åŠ¨ã€äº®åº¦å’Œé¢œè‰²å˜åŒ–ï¼‰ã€‚å¤§å¤šæ•°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ›´å¥½åœ°åˆ©ç”¨æ—¶ç©ºä¾èµ–æ€§ï¼ŒåŒ…æ‹¬æ˜¾å¼è¿åŠ¨è¡¥å¿ï¼ˆä¾‹å¦‚ï¼Œå…‰æµç®—æ³•ã€åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼‰å’Œé€’å½’æ–¹æ³•ç­‰ã€‚ å‚è€ƒæ–‡çŒ® Z Wang, J Chen, S Hoi,â€œDeep Learning for Image Super-resolution: A Surveyâ€, Arxiv 1902.06068,2019 W Yang et al.,â€œDeep Learning for Single Image Super-Resolution: A Brief Reviewâ€, Archiv 1808.03344, 2018 Z Li et al.,â€œFeedback Network for Image Super-Resolutionâ€, CVPR 2019 C Chen et al.,â€œCamera Lens Super-Resolutionâ€, CVPR 2019 K Zhang et al.,â€œDeep Plug-and-Play Super-Resolution for Arbitrary Blur Kernelsâ€, CVPR 2019 -å®Œ- "},"super_resolution/SR.html":{"url":"super_resolution/SR.html","title":"è¶…åˆ†è¾¨ç‡æŠ€æœ¯","keywords":"","body":"è¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼ˆSuper-Resolution, SRï¼‰æ˜¯æŒ‡ä»è§‚æµ‹åˆ°çš„ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºå‡ºç›¸åº”çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œåœ¨ç›‘æ§è®¾å¤‡ã€å«æ˜Ÿå›¾åƒå’ŒåŒ»å­¦å½±åƒç­‰é¢†åŸŸéƒ½æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚ æœ¬æ–‡é’ˆå¯¹ç«¯åˆ°ç«¯çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å•å¼ å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•(Single Image Super-Resolution, SISR)ï¼Œæ€»ç»“ä¸€ä¸‹ä»SRCNNåˆ°EDSRçš„å‘å±•å†ç¨‹ã€‚(æ’åˆ—é¡ºåºå¤§è‡´æŒ‰è®ºæ–‡ä¸­ç»™å‡ºçš„4å€ä¸Šé‡‡æ ·ç»“æœçš„å³°å€¼ä¿¡å™ªæ¯”(Peak Signal to Noise Ratio, PSNR)ä»ä½åˆ°é«˜æ’åˆ—) 1. SRCNN (Learning a Deep Convolutional Network for Image Super-Resolution, ECCV2014) SRCNNæ˜¯æ·±åº¦å­¦ä¹ ç”¨åœ¨è¶…åˆ†è¾¨ç‡é‡å»ºä¸Šçš„å¼€å±±ä¹‹ä½œã€‚SRCNNçš„ç½‘ç»œç»“æ„éå¸¸ç®€å•ï¼Œä»…ä»…ç”¨äº†ä¸‰ä¸ªå·ç§¯å±‚ï¼Œç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ SRCNN SRCNNé¦–å…ˆä½¿ç”¨åŒä¸‰æ¬¡(bicubic)æ’å€¼å°†ä½åˆ†è¾¨ç‡å›¾åƒæ”¾å¤§æˆç›®æ ‡å°ºå¯¸ï¼Œæ¥ç€é€šè¿‡ä¸‰å±‚å·ç§¯ç½‘ç»œæ‹Ÿåˆéçº¿æ€§æ˜ å°„ï¼Œæœ€åè¾“å‡ºé«˜åˆ†è¾¨ç‡å›¾åƒç»“æœã€‚æœ¬æ–‡ä¸­ï¼Œä½œè€…å°†ä¸‰å±‚å·ç§¯çš„ç»“æ„è§£é‡Šæˆä¸‰ä¸ªæ­¥éª¤ï¼šå›¾åƒå—çš„æå–å’Œç‰¹å¾è¡¨ç¤ºï¼Œç‰¹å¾éçº¿æ€§æ˜ å°„å’Œæœ€ç»ˆçš„é‡å»ºã€‚ ä¸‰ä¸ªå·ç§¯å±‚ä½¿ç”¨çš„å·ç§¯æ ¸çš„å¤§å°åˆ†ä¸ºä¸º9x9,ï¼Œ1x1å’Œ5x5ï¼Œå‰ä¸¤ä¸ªçš„è¾“å‡ºç‰¹å¾ä¸ªæ•°åˆ†åˆ«ä¸º64å’Œ32ã€‚ç”¨Timofteæ•°æ®é›†ï¼ˆåŒ…å«91å¹…å›¾åƒï¼‰å’ŒImageNetå¤§æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨å‡æ–¹è¯¯å·®(Mean Squared Error, MSE)ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæœ‰åˆ©äºè·å¾—è¾ƒé«˜çš„PSNRã€‚ code: http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html 2. FSRCNN (Accelerating the Super-Resolution Convolutional Neural Network, ECCV2016) FSRCNNä¸SRCNNéƒ½æ˜¯é¦™æ¸¯ä¸­æ–‡å¤§å­¦Dong Chaoï¼Œ Xiaoou Tangç­‰äººçš„å·¥ä½œã€‚FSRCNNæ˜¯å¯¹ä¹‹å‰SRCNNçš„æ”¹è¿›ï¼Œä¸»è¦åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šä¸€æ˜¯åœ¨æœ€åä½¿ç”¨äº†ä¸€ä¸ªåå·ç§¯å±‚æ”¾å¤§å°ºå¯¸ï¼Œå› æ­¤å¯ä»¥ç›´æ¥å°†åŸå§‹çš„ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥åˆ°ç½‘ç»œä¸­ï¼Œè€Œä¸æ˜¯åƒä¹‹å‰SRCNNé‚£æ ·éœ€è¦å…ˆé€šè¿‡bicubicæ–¹æ³•æ”¾å¤§å°ºå¯¸ã€‚äºŒæ˜¯æ”¹å˜ç‰¹å¾ç»´æ•°ï¼Œä½¿ç”¨æ›´å°çš„å·ç§¯æ ¸å’Œä½¿ç”¨æ›´å¤šçš„æ˜ å°„å±‚ã€‚ä¸‰æ˜¯å¯ä»¥å…±äº«å…¶ä¸­çš„æ˜ å°„å±‚ï¼Œå¦‚æœéœ€è¦è®­ç»ƒä¸åŒä¸Šé‡‡æ ·å€ç‡çš„æ¨¡å‹ï¼Œåªéœ€è¦fine-tuningæœ€åçš„åå·ç§¯å±‚ã€‚ ç”±äºFSRCNNä¸éœ€è¦åœ¨ç½‘ç»œå¤–éƒ¨è¿›è¡Œæ”¾å¤§å›¾ç‰‡å°ºå¯¸çš„æ“ä½œï¼ŒåŒæ—¶é€šè¿‡æ·»åŠ æ”¶ç¼©å±‚å’Œæ‰©å¼ å±‚ï¼Œå°†ä¸€ä¸ªå¤§å±‚ç”¨ä¸€äº›å°å±‚æ¥ä»£æ›¿ï¼Œå› æ­¤FSRCNNä¸SRCNNç›¸æ¯”æœ‰è¾ƒå¤§çš„é€Ÿåº¦æå‡ã€‚FSRCNNåœ¨è®­ç»ƒæ—¶ä¹Ÿå¯ä»¥åªfine-tuningæœ€åçš„åå·ç§¯å±‚ï¼Œå› æ­¤è®­ç»ƒé€Ÿåº¦ä¹Ÿæ›´å¿«ã€‚FSRCNNä¸SCRNNçš„ç»“æ„å¯¹æ¯”å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ FSRCNN FSRCNNå¯ä»¥åˆ†ä¸ºäº”ä¸ªéƒ¨åˆ†ã€‚ç‰¹å¾æå–ï¼šSRCNNä¸­é’ˆå¯¹çš„æ˜¯æ’å€¼åçš„ä½åˆ†è¾¨ç‡å›¾åƒï¼Œé€‰å–çš„æ ¸å¤§å°ä¸º9Ã—9ï¼Œè¿™é‡Œç›´æ¥æ˜¯å¯¹åŸå§‹çš„ä½åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œæ“ä½œï¼Œå› æ­¤å¯ä»¥é€‰å°ä¸€ç‚¹ï¼Œè®¾ç½®ä¸º5Ã—5ã€‚æ”¶ç¼©ï¼šé€šè¿‡åº”ç”¨1Ã—1çš„å·ç§¯æ ¸è¿›è¡Œé™ç»´ï¼Œå‡å°‘ç½‘ç»œçš„å‚æ•°ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚éçº¿æ€§æ˜ å°„ï¼šæ„Ÿå—é‡å¤§ï¼Œèƒ½å¤Ÿè¡¨ç°çš„æ›´å¥½ã€‚SRCNNä¸­ï¼Œé‡‡ç”¨çš„æ˜¯5Ã—5çš„å·ç§¯æ ¸ï¼Œä½†æ˜¯5Ã—5çš„å·ç§¯æ ¸è®¡ç®—é‡ä¼šæ¯”è¾ƒå¤§ã€‚ç”¨ä¸¤ä¸ªä¸²è”çš„3Ã—3çš„å·ç§¯æ ¸å¯ä»¥æ›¿ä»£ä¸€ä¸ª5Ã—5çš„å·ç§¯æ ¸ï¼ŒåŒæ—¶ä¸¤ä¸ªä¸²è”çš„å°å·ç§¯æ ¸éœ€è¦çš„å‚æ•°3Ã—3Ã—2=18æ¯”ä¸€ä¸ªå¤§å·ç§¯æ ¸5Ã—5=25çš„å‚æ•°è¦å°ã€‚FSRCNNç½‘ç»œä¸­é€šè¿‡mä¸ªæ ¸å¤§å°ä¸º3Ã—3çš„å·ç§¯å±‚è¿›è¡Œä¸²è”ã€‚æ‰©å¼ ï¼šä½œè€…å‘ç°ä½ç»´åº¦çš„ç‰¹å¾å¸¦æ¥çš„é‡å»ºæ•ˆæœä¸æ˜¯å¤ªå¥½ï¼Œå› æ­¤åº”ç”¨1Ã—1çš„å·ç§¯æ ¸è¿›è¡Œæ‰©ç»´ï¼Œç›¸å½“äºæ”¶ç¼©çš„é€†è¿‡ç¨‹ã€‚åå·ç§¯å±‚ï¼šå¯ä»¥å ªç§°æ˜¯å·ç§¯å±‚çš„é€†æ“ä½œï¼Œå¦‚æœæ­¥é•¿ä¸ºnï¼Œé‚£ä¹ˆå°ºå¯¸æ”¾å¤§nå€ï¼Œå®ç°äº†ä¸Šé‡‡æ ·çš„æ“ä½œã€‚ FSRCNNä¸­æ¿€æ´»å‡½æ•°é‡‡ç”¨PReLUï¼ŒæŸå¤±å‡½æ•°ä»ç„¶æ˜¯å‡æ–¹è¯¯å·®ã€‚å¯¹CNNæ¥è¯´ï¼ŒSet91å¹¶ä¸è¶³å¤Ÿå»è®­ç»ƒå¤§çš„ç½‘ç»œç»“æ„ï¼ŒFSRCNNæå‡ºgeneral-100 + Set91è¿›è¡Œå……å½“è®­ç»ƒé›†ã€‚å¹¶ä¸”è¿›è¡Œæ•°æ®å¢å¼ºï¼Œ1ï¼‰ç¼©å°å°ºå¯¸ä¸ºåŸæ¥çš„0.9, 0.8, 0.7å’Œ0.6ã€‚2ï¼‰æ—‹è½¬ 90Â°ï¼Œ180Â°å’Œ270Â°ï¼Œå› æ­¤è·å¾—äº†æ•°æ®é‡çš„æå‡ã€‚ code: http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html 3. ESPCN (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network, CVPR2016) ä½œè€…åœ¨æœ¬æ–‡ä¸­ä»‹ç»åˆ°ï¼ŒåƒSRCNNé‚£æ ·çš„æ–¹æ³•ï¼Œç”±äºéœ€è¦å°†ä½åˆ†è¾¨ç‡å›¾åƒé€šè¿‡ä¸Šé‡‡æ ·æ’å€¼å¾—åˆ°ä¸é«˜åˆ†è¾¨ç‡å›¾åƒç›¸åŒå¤§å°çš„å°ºå¯¸ï¼Œå†è¾“å…¥åˆ°ç½‘ç»œä¸­ï¼Œè¿™æ„å‘³ç€è¦åœ¨è¾ƒé«˜çš„åˆ†è¾¨ç‡ä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼Œä»è€Œå¢åŠ äº†è®¡ç®—å¤æ‚åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›´æ¥åœ¨ä½åˆ†è¾¨ç‡å›¾åƒå°ºå¯¸ä¸Šæå–ç‰¹å¾ï¼Œè®¡ç®—å¾—åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒçš„é«˜æ•ˆæ–¹æ³•ã€‚ESPCNç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ ESPCN ESPCNçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯äºšåƒç´ å·ç§¯å±‚(sub-pixel convolutional layer)ã€‚ç½‘ç»œçš„è¾“å…¥æ˜¯åŸå§‹ä½åˆ†è¾¨ç‡å›¾åƒï¼Œé€šè¿‡ä¸‰ä¸ªå·ç§¯å±‚ä»¥åï¼Œå¾—åˆ°é€šé“æ•°ä¸º çš„ä¸è¾“å…¥å›¾åƒå¤§å°ä¸€æ ·çš„ç‰¹å¾å›¾åƒã€‚å†å°†ç‰¹å¾å›¾åƒæ¯ä¸ªåƒç´ çš„ ä¸ªé€šé“é‡æ–°æ’åˆ—æˆä¸€ä¸ª çš„åŒºåŸŸï¼Œå¯¹åº”é«˜åˆ†è¾¨ç‡å›¾åƒä¸­ä¸€ä¸ª å¤§å°çš„å­å—ï¼Œä»è€Œå¤§å°ä¸º çš„ç‰¹å¾å›¾åƒè¢«é‡æ–°æ’åˆ—æˆ çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚æˆ‘ç†è§£çš„äºšåƒç´ å·ç§¯å±‚åŒ…å«ä¸¤ä¸ªè¿‡ç¨‹ï¼Œä¸€ä¸ªæ™®é€šçš„å·ç§¯å±‚å’Œåé¢çš„æ’åˆ—åƒç´ çš„æ­¥éª¤ã€‚å°±æ˜¯è¯´ï¼Œæœ€åä¸€å±‚å·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾ä¸ªæ•°éœ€è¦è®¾ç½®æˆå›ºå®šå€¼ï¼Œå³æ”¾å¤§å€æ•°rçš„å¹³æ–¹ï¼Œè¿™æ ·æ€»çš„åƒç´ ä¸ªæ•°å°±ä¸è¦å¾—åˆ°çš„é«˜åˆ†è¾¨ç‡å›¾åƒä¸€è‡´ï¼Œå°†åƒç´ è¿›è¡Œé‡æ–°æ’åˆ—å°±èƒ½å¾—åˆ°é«˜åˆ†è¾¨ç‡å›¾ã€‚ åœ¨ESPCNç½‘ç»œä¸­ï¼Œå›¾åƒå°ºå¯¸æ”¾å¤§è¿‡ç¨‹çš„æ’å€¼å‡½æ•°è¢«éšå«åœ°åŒ…å«åœ¨å‰é¢çš„å·ç§¯å±‚ä¸­ï¼Œå¯ä»¥è‡ªåŠ¨å­¦ä¹ åˆ°ã€‚ç”±äºå·ç§¯è¿ç®—éƒ½æ˜¯åœ¨ä½åˆ†è¾¨ç‡å›¾åƒå°ºå¯¸å¤§å°ä¸Šè¿›è¡Œï¼Œå› æ­¤æ•ˆç‡ä¼šè¾ƒé«˜ã€‚ è®­ç»ƒæ—¶ï¼Œå¯ä»¥å°†è¾“å…¥çš„è®­ç»ƒæ•°æ®æ ‡ç­¾ï¼Œé¢„å¤„ç†æˆé‡æ–°æ’åˆ—æ“ä½œå‰çš„æ ¼å¼ï¼Œæ¯”å¦‚å°†21Ã—21çš„å•é€šé“å›¾ï¼Œé¢„å¤„ç†æˆ9ä¸ªé€šé“ï¼Œ7Ã—7çš„å›¾ï¼Œè¿™æ ·åœ¨è®­ç»ƒæ—¶ï¼Œå°±ä¸éœ€è¦åšé‡æ–°æ’åˆ—çš„æ“ä½œã€‚å¦å¤–ï¼ŒESPCNæ¿€æ´»å‡½æ•°é‡‡ç”¨tanhæ›¿ä»£äº†ReLUã€‚æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®ã€‚ github(tensorflow): https://github.com/drakelevy/ESPCN-TensorFlow github(pytorch): https://github.com/leftthomas/ESPCN github(caffe): https://github.com/wangxuewen99/Super-Resolution/tree/master/ESPCN 4. VDSR (Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR2016) åœ¨ä»‹ç»VDSRä¹‹å‰ï¼Œé¦–å…ˆæƒ³å…ˆæä¸€ä¸‹ä½•æºæ˜åœ¨2015å¹´çš„æ—¶å€™æå‡ºçš„æ®‹å·®ç½‘ç»œResNetã€‚ResNetçš„æå‡ºï¼Œè§£å†³äº†ä¹‹å‰ç½‘ç»œç»“æ„æ¯”è¾ƒæ·±æ—¶æ— æ³•è®­ç»ƒçš„é—®é¢˜ï¼Œæ€§èƒ½ä¹Ÿå¾—åˆ°äº†æå‡ï¼ŒResNetä¹Ÿè·å¾—äº†CVPR2016çš„best paperã€‚æ®‹å·®ç½‘ç»œç»“æ„(residual network)è¢«åº”ç”¨åœ¨äº†å¤§é‡çš„å·¥ä½œä¸­ã€‚ æ­£å¦‚åœ¨VDSRè®ºæ–‡ä¸­ä½œè€…æåˆ°ï¼Œè¾“å…¥çš„ä½åˆ†è¾¨ç‡å›¾åƒå’Œè¾“å‡ºçš„é«˜åˆ†è¾¨ç‡å›¾åƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç›¸ä¼¼çš„ï¼Œä¹Ÿå°±æ˜¯æŒ‡ä½åˆ†è¾¨ç‡å›¾åƒæºå¸¦çš„ä½é¢‘ä¿¡æ¯ä¸é«˜åˆ†è¾¨ç‡å›¾åƒçš„ä½é¢‘ä¿¡æ¯ç›¸è¿‘ï¼Œè®­ç»ƒæ—¶å¸¦ä¸Šè¿™éƒ¨åˆ†ä¼šå¤šèŠ±è´¹å¤§é‡çš„æ—¶é—´ï¼Œå®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦å­¦ä¹ é«˜åˆ†è¾¨ç‡å›¾åƒå’Œä½åˆ†è¾¨ç‡å›¾åƒä¹‹é—´çš„é«˜é¢‘éƒ¨åˆ†æ®‹å·®å³å¯ã€‚æ®‹å·®ç½‘ç»œç»“æ„çš„æ€æƒ³ç‰¹åˆ«é€‚åˆç”¨æ¥è§£å†³è¶…åˆ†è¾¨ç‡é—®é¢˜ï¼Œå¯ä»¥è¯´å½±å“äº†ä¹‹åçš„æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚VDSRæ˜¯æœ€ç›´æ¥æ˜æ˜¾çš„å­¦ä¹ æ®‹å·®çš„ç»“æ„ï¼Œå…¶ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ VDSR VDSRå°†æ’å€¼åå¾—åˆ°çš„å˜æˆç›®æ ‡å°ºå¯¸çš„ä½åˆ†è¾¨ç‡å›¾åƒä½œä¸ºç½‘ç»œçš„è¾“å…¥ï¼Œå†å°†è¿™ä¸ªå›¾åƒä¸ç½‘ç»œå­¦åˆ°çš„æ®‹å·®ç›¸åŠ å¾—åˆ°æœ€ç»ˆçš„ç½‘ç»œçš„è¾“å‡ºã€‚VDSRä¸»è¦æœ‰4ç‚¹è´¡çŒ®ã€‚1.åŠ æ·±äº†ç½‘ç»œç»“æ„(20å±‚)ï¼Œä½¿å¾—è¶Šæ·±çš„ç½‘ç»œå±‚æ‹¥æœ‰æ›´å¤§çš„æ„Ÿå—é‡ã€‚æ–‡ç« é€‰å–3Ã—3çš„å·ç§¯æ ¸ï¼Œæ·±åº¦ä¸ºDçš„ç½‘ç»œæ‹¥æœ‰(2D+1)Ã—(2D+1)çš„æ„Ÿå—é‡ã€‚2.é‡‡ç”¨æ®‹å·®å­¦ä¹ ï¼Œæ®‹å·®å›¾åƒæ¯”è¾ƒç¨€ç–ï¼Œå¤§éƒ¨åˆ†å€¼éƒ½ä¸º0æˆ–è€…æ¯”è¾ƒå°ï¼Œå› æ­¤æ”¶æ•›é€Ÿåº¦å¿«ã€‚VDSRè¿˜åº”ç”¨äº†è‡ªé€‚åº”æ¢¯åº¦è£å‰ª(Adjustable Gradient Clipping)ï¼Œå°†æ¢¯åº¦é™åˆ¶åœ¨æŸä¸€èŒƒå›´ï¼Œä¹Ÿèƒ½å¤ŸåŠ å¿«æ”¶æ•›è¿‡ç¨‹ã€‚3.VDSRåœ¨æ¯æ¬¡å·ç§¯å‰éƒ½å¯¹å›¾åƒè¿›è¡Œè¡¥0æ“ä½œï¼Œè¿™æ ·ä¿è¯äº†æ‰€æœ‰çš„ç‰¹å¾å›¾å’Œæœ€ç»ˆçš„è¾“å‡ºå›¾åƒåœ¨å°ºå¯¸ä¸Šéƒ½ä¿æŒä¸€è‡´ï¼Œè§£å†³äº†å›¾åƒé€šè¿‡é€æ­¥å·ç§¯ä¼šè¶Šæ¥è¶Šå°çš„é—®é¢˜ã€‚æ–‡ä¸­è¯´å®éªŒè¯æ˜è¡¥0æ“ä½œå¯¹è¾¹ç•Œåƒç´ çš„é¢„æµ‹ç»“æœä¹Ÿèƒ½å¤Ÿå¾—åˆ°æå‡ã€‚4.VDSRå°†ä¸åŒå€æ•°çš„å›¾åƒæ··åˆåœ¨ä¸€èµ·è®­ç»ƒï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„ä¸€ä¸ªæ¨¡å‹å°±å¯ä»¥è§£å†³ä¸åŒå€æ•°çš„è¶…åˆ†è¾¨ç‡é—®é¢˜ã€‚ code: https://cv.snu.ac.kr/research/VDSR/ github(caffe): https://github.com/huangzehao/caffe-vdsr github(tensorflow): https://github.com/Jongchan/tensorflow-vdsr github(pytorch): https://github.com/twtygqyy/pytorch-vdsr 5. DRCN (Deeply-Recursive Convolutional Network for Image Super-Resolution, CVPR2016) DRCNä¸ä¸Šé¢çš„VDSRéƒ½æ˜¯æ¥è‡ªé¦–å°”å›½ç«‹å¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤çš„å·¥ä½œï¼Œä¸¤ç¯‡è®ºæ–‡éƒ½å‘è¡¨åœ¨CVPR2016ä¸Šï¼Œä¸¤ç§æ–¹æ³•çš„ç»“æœéå¸¸æ¥è¿‘ã€‚DRCNç¬¬ä¸€æ¬¡å°†ä¹‹å‰å·²æœ‰çš„é€’å½’ç¥ç»ç½‘ç»œ(Recursive Neural Network)ç»“æ„åº”ç”¨åœ¨è¶…åˆ†è¾¨ç‡å¤„ç†ä¸­ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æ®‹å·®å­¦ä¹ çš„æ€æƒ³(æ–‡ä¸­çš„è·³è·ƒè¿æ¥ï¼ˆSkip-Connectionï¼‰)ï¼ŒåŠ æ·±äº†ç½‘ç»œç»“æ„(16ä¸ªé€’å½’)ï¼Œå¢åŠ äº†ç½‘ç»œæ„Ÿå—é‡ï¼Œæå‡äº†æ€§èƒ½ã€‚DRCNç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ DRCN DRCNè¾“å…¥çš„æ˜¯æ’å€¼åçš„å›¾åƒï¼Œåˆ†ä¸ºä¸‰ä¸ªæ¨¡å—ï¼Œç¬¬ä¸€ä¸ªæ˜¯Embedding networkï¼Œç›¸å½“äºç‰¹å¾æå–ï¼Œç¬¬äºŒä¸ªæ˜¯Inference network, ç›¸å½“äºç‰¹å¾çš„éçº¿æ€§æ˜ å°„ï¼Œç¬¬ä¸‰ä¸ªæ˜¯Reconstruction network,å³ä»ç‰¹å¾å›¾åƒæ¢å¤æœ€åçš„é‡å»ºç»“æœã€‚å…¶ä¸­çš„Inference networkæ˜¯ä¸€ä¸ªé€’å½’ç½‘ç»œï¼Œå³æ•°æ®å¾ªç¯åœ°é€šè¿‡è¯¥å±‚å¤šæ¬¡ã€‚å°†è¿™ä¸ªå¾ªç¯è¿›è¡Œå±•å¼€ï¼Œç­‰æ•ˆäºä½¿ç”¨åŒä¸€ç»„å‚æ•°çš„å¤šä¸ªä¸²è”çš„å·ç§¯å±‚ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ DRCN å…¶ä¸­çš„ åˆ° æ˜¯Dä¸ªå…±äº«å‚æ•°çš„å·ç§¯å±‚ã€‚å°†è¿™Dä¸ªå·ç§¯å±‚çš„æ¯ä¸€å±‚çš„ç»“æœéƒ½é€šè¿‡ç›¸åŒçš„Reconstruction Netï¼Œåœ¨Reconstruction Netä¸­ä¸è¾“å…¥çš„å›¾åƒç›¸åŠ ï¼Œå¾—åˆ°Dä¸ªè¾“å‡ºé‡å»ºç»“æœã€‚è¿™äº›æ‰€æœ‰çš„ç»“æœåœ¨è®­ç»ƒæ—¶éƒ½åŒæ—¶è¢«ç›‘ç£ï¼Œå³æ‰€æœ‰çš„é€’å½’éƒ½è¢«ç›‘ç£ï¼Œä½œè€…ç§°ä¹‹ä¸ºé€’å½’ç›‘ç£(Recursive-Supervision)ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ã€‚å°†Dä¸ªé€’å½’å¾—åˆ°çš„ç»“æœå†åŠ æƒå¹³å‡ï¼š å¾—åˆ°ä¸€ä¸ªæ€»è¾“å‡ºã€‚æ¯ä¸ªåŠ æƒ åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¹Ÿä¸æ–­åœ°æ›´æ–°ã€‚æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°å°±éœ€è¦ä¼˜åŒ–æ¯ä¸€ä¸ªé€’å½’å±‚è¾“å‡ºçš„è¯¯å·®å’Œæ€»è¾“å‡ºçš„è¯¯å·®ï¼š è¡¨ç¤ºçš„æ˜¯æƒå€¼è¡°å‡(weight decay)ã€‚ çš„åˆå§‹å€¼è®¾ç½®å¾—æ¯”è¾ƒé«˜ä»¥ä½¿å¾—è®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼Œå› ä¸ºè®­ç»ƒå¼€å§‹çš„é˜¶æ®µé€’å½’æ›´å®¹æ˜“æ”¶æ•›ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œ é€æ¸è¡°å‡æ¥æå‡æœ€ç»ˆè¾“å‡ºçš„æ€§èƒ½ã€‚ code: https://cv.snu.ac.kr/research/DRCN/ github(tensorflow): https://github.com/jiny2001/deeply-recursive-cnn-tf 6. RED (Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections, NIPS2016) è¿™ç¯‡æ–‡ç« æå‡ºäº†ç”±å¯¹ç§°çš„å·ç§¯å±‚-åå·ç§¯å±‚æ„æˆçš„ç½‘ç»œç»“æ„ï¼Œä½œä¸ºä¸€ä¸ªç¼–ç -è§£ç æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ç”±ä½è´¨å›¾åƒåˆ°åŸå§‹å›¾åƒç«¯åˆ°ç«¯çš„æ˜ å°„ã€‚ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ RED REDç½‘ç»œçš„ç»“æ„æ˜¯å¯¹ç§°çš„ï¼Œæ¯ä¸ªå·ç§¯å±‚éƒ½æœ‰å¯¹åº”çš„åå·ç§¯å±‚ã€‚å·ç§¯å±‚ç”¨æ¥è·å–å›¾åƒçš„æŠ½è±¡å†…å®¹ï¼Œåå·ç§¯å±‚ç”¨æ¥æ”¾å¤§ç‰¹å¾å°ºå¯¸å¹¶ä¸”æ¢å¤å›¾åƒç»†èŠ‚ã€‚å·ç§¯å±‚å°†è¾“å…¥å›¾åƒå°ºå¯¸å‡å°åï¼Œå†é€šè¿‡åå·ç§¯å±‚ä¸Šé‡‡æ ·å˜å¤§ï¼Œä½¿å¾—è¾“å…¥è¾“å‡ºçš„å°ºå¯¸ä¸€æ ·ã€‚æ¯ä¸€ç»„é•œåƒå¯¹åº”çš„å·ç§¯å±‚å’Œåå·ç§¯å±‚æœ‰ç€è·³çº¿è¿æ¥ç»“æ„ï¼Œå°†ä¸¤éƒ¨åˆ†å…·æœ‰åŒæ ·å°ºå¯¸çš„ç‰¹å¾(è¦è¾“å…¥å·ç§¯å±‚çš„ç‰¹å¾å’Œå¯¹åº”çš„åå·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾)åšç›¸åŠ æ“ä½œ(ResNeté‚£æ ·çš„æ“ä½œ)åå†è¾“å…¥åˆ°ä¸‹ä¸€ä¸ªåå·ç§¯å±‚ï¼Œæ“ä½œè¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ è¿™æ ·çš„ç»“æ„èƒ½å¤Ÿè®©åå‘ä¼ æ’­ä¿¡å·èƒ½å¤Ÿç›´æ¥ä¼ é€’åˆ°åº•å±‚ï¼Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŒæ—¶èƒ½å°†å·ç§¯å±‚çš„ç»†èŠ‚ä¼ é€’ç»™åå·ç§¯å±‚ï¼Œèƒ½å¤Ÿæ¢å¤å‡ºæ›´å¹²å‡€çš„å›¾ç‰‡ã€‚å¯ä»¥çœ‹åˆ°ï¼Œç½‘ç»œä¸­æœ‰ä¸€æ¡çº¿æ˜¯å°†è¾“å…¥çš„å›¾åƒè¿æ¥åˆ°åé¢ä¸æœ€åçš„ä¸€å±‚åå·ç§¯å±‚çš„è¾“å‡ºç›¸åŠ ï¼Œä¹Ÿå°±æ˜¯VDSRä¸­ç”¨åˆ°çš„æ–¹å¼ï¼Œå› æ­¤REDä¸­é—´çš„å·ç§¯å±‚å’Œåå·ç§¯å±‚å­¦ä¹ çš„ç‰¹å¾æ˜¯ç›®æ ‡å›¾åƒå’Œä½è´¨å›¾åƒä¹‹é—´çš„æ®‹å·®ã€‚REDçš„ç½‘ç»œæ·±åº¦ä¸º30å±‚ï¼ŒæŸå¤±å‡½æ•°ç”¨çš„å‡æ–¹è¯¯å·®ã€‚ 7. DRRN (Image Super-Resolution via Deep Recursive Residual Network, CVPR2017) DRRNçš„ä½œè€…åº”è¯¥æ˜¯å—åˆ°äº†ResNetã€VDSRå’ŒDRCNçš„å¯å‘ï¼Œé‡‡ç”¨äº†æ›´æ·±çš„ç½‘ç»œç»“æ„æ¥è·å–æ€§èƒ½çš„æå‡ã€‚ä½œè€…ä¹Ÿåœ¨æ–‡ä¸­ç”¨å›¾ç‰‡ç¤ºä¾‹æ¯”è¾ƒäº†DRRNä¸ä¸Šè¿°ä¸‰ä¸ªç½‘ç»œçš„åŒºåˆ«ï¼Œæ¯”è¾ƒç¤ºä¾‹å›¾å¦‚ä¸‹æ‰€ç¤ºã€‚ DRRN DRRNä¸­çš„æ¯ä¸ªæ®‹å·®å•å…ƒéƒ½å…±åŒæ‹¥æœ‰ä¸€ä¸ªç›¸åŒçš„è¾“å…¥ï¼Œå³é€’å½’å—ä¸­çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å‡ºã€‚æ¯ä¸ªæ®‹å·®å•å…ƒéƒ½åŒ…å«2ä¸ªå·ç§¯å±‚ã€‚åœ¨ä¸€ä¸ªé€’å½’å—å†…ï¼Œæ¯ä¸ªæ®‹å·®å•å…ƒå†…å¯¹åº”ä½ç½®ç›¸åŒçš„å·ç§¯å±‚å‚æ•°éƒ½å…±äº«(å›¾ä¸­DRRNçš„æµ…ç»¿è‰²å—æˆ–æµ…çº¢è‰²å—)ã€‚ä½œè€…åˆ—å‡ºäº†ResNetã€VDSRã€DRCNå’ŒDRRNå››è€…çš„ä¸»è¦ç­–ç•¥ã€‚ResNetæ˜¯é“¾æ¨¡å¼çš„å±€éƒ¨æ®‹å·®å­¦ä¹ ã€‚VDSRæ˜¯å…¨å±€æ®‹å·®å­¦ä¹ ã€‚DRCNæ˜¯å…¨å±€æ®‹å·®å­¦ä¹ +å•æƒé‡çš„é€’å½’å­¦ä¹ +å¤šç›®æ ‡ä¼˜åŒ–ã€‚DRRNæ˜¯å¤šè·¯å¾„æ¨¡å¼çš„å±€éƒ¨æ®‹å·®å­¦ä¹ +å…¨å±€æ®‹å·®å­¦ä¹ +å¤šæƒé‡çš„é€’å½’å­¦ä¹ ã€‚ æ–‡ç« ä¸­æ¯”è¾ƒäº†ä¸åŒçš„é€’å½’å—å’Œæ®‹å·®å•å…ƒæ•°é‡çš„å®éªŒç»“æœï¼Œæœ€ç»ˆé€‰ç”¨çš„æ˜¯1ä¸ªé€’å½’å—å’Œ25ä¸ªæ®‹å·®å•å…ƒï¼Œæ·±åº¦ä¸º52å±‚çš„ç½‘ç»œç»“æ„ã€‚æ€»ä¹‹ï¼ŒDRRNå°±æ˜¯é€šè¿‡å¯¹ä¹‹å‰å·²æœ‰çš„ResNetç­‰ç»“æ„è¿›è¡Œè°ƒæ•´ï¼Œé‡‡å–æ›´æ·±çš„ç½‘ç»œç»“æ„å¾—åˆ°ç»“æœçš„æå‡ã€‚ github(caffe): tyshiwo/DRRN_CVPR17 github(pytorch): https://github.com/jt827859032/DRRN-pytorch 8. LapSRN (Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution, CVPR2017) è®ºæ–‡ä¸­ä½œè€…å…ˆæ€»ç»“äº†ä¹‹å‰çš„æ–¹æ³•å­˜åœ¨æœ‰ä¸‰ç‚¹é—®é¢˜ã€‚ä¸€æ˜¯æœ‰çš„æ–¹æ³•åœ¨è¾“å…¥å›¾åƒè¿›ç½‘ç»œå‰ï¼Œéœ€è¦ä½¿ç”¨é¢„å…ˆå®šä¹‰å¥½çš„ä¸Šé‡‡æ ·æ“ä½œ(ä¾‹å¦‚bicubic)æ¥è·å¾—ç›®æ ‡çš„ç©ºé—´å°ºå¯¸ï¼Œè¿™æ ·çš„æ“ä½œå¢åŠ äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¼è‡´å¯è§çš„é‡å»ºä¼ªå½±ã€‚è€Œæœ‰çš„æ–¹æ³•ä½¿ç”¨äº†äºšåƒç´ å·ç§¯å±‚æˆ–è€…åå·ç§¯å±‚è¿™æ ·çš„æ“ä½œæ¥æ›¿æ¢é¢„å…ˆå®šä¹‰å¥½çš„ä¸Šé‡‡æ ·æ“ä½œï¼Œè¿™äº›æ–¹æ³•çš„ç½‘ç»œç»“æ„åˆç›¸å¯¹æ¯”è¾ƒç®€å•ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¹¶ä¸èƒ½å­¦å¥½ä½åˆ†è¾¨ç‡å›¾åƒåˆ°é«˜åˆ†è¾¨ç‡å›¾åƒå¤æ‚çš„æ˜ å°„ã€‚äºŒæ˜¯åœ¨è®­ç»ƒç½‘ç»œæ—¶ä½¿ç”¨ å‹æŸå¤±å‡½æ•°æ—¶ï¼Œä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿæ¨¡ç³Šçš„é¢„æµ‹ï¼Œæ¢å¤å‡ºçš„é«˜åˆ†è¾¨ç‡å›¾ç‰‡å¾€å¾€ä¼šå¤ªè¿‡äºå¹³æ»‘ã€‚ä¸‰æ˜¯åœ¨é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œå¦‚æœåªç”¨ä¸€æ¬¡ä¸Šé‡‡æ ·çš„æ“ä½œï¼Œåœ¨è·å¾—å¤§å€æ•°(8å€ä»¥ä¸Š)çš„ä¸Šé‡‡æ ·å› å­æ—¶å°±ä¼šæ¯”è¾ƒå›°éš¾ã€‚è€Œä¸”åœ¨ä¸åŒçš„åº”ç”¨æ—¶ï¼Œéœ€è¦è®­ç»ƒä¸åŒä¸Šé‡‡æ ·å€æ•°çš„æ¨¡å‹ã€‚é’ˆå¯¹è¿™ä¸‰ç‚¹é—®é¢˜ï¼Œä½œè€…æå‡ºäº†LapSRNï¼Œç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ LapSRN LapSRNçš„ç»“æ„å¯ä»¥çœ‹æˆæœ‰å¤šçº§ï¼Œæ¯ä¸€çº§å®Œæˆä¸€æ¬¡2å€çš„ä¸Šé‡‡æ ·æ“ä½œï¼Œè¦å®ç°8å€çš„ä¸Šé‡‡æ ·å°±éœ€è¦æœ‰ä¸‰çº§ã€‚åœ¨æ¯ä¸€çº§ä¸­ï¼Œå…ˆé€šè¿‡ä¸€äº›çº§è”çš„å·ç§¯å±‚æå–ç‰¹å¾ï¼Œæ¥ç€é€šè¿‡ä¸€ä¸ªåå·ç§¯å±‚å°†æå–å‡ºçš„ç‰¹å¾çš„å°ºå¯¸ä¸Šé‡‡æ ·2å€ã€‚åå·ç§¯å±‚åè¿æœ‰ä¸¤ä¸ªå·ç§¯å±‚ï¼Œä¸€ä¸ªå·ç§¯å±‚çš„ä½œç”¨æ˜¯ç»§ç»­æå–ç‰¹å¾ï¼Œå¦å¤–ä¸€ä¸ªå·ç§¯å±‚çš„ä½œç”¨æ˜¯é¢„æµ‹å‡ºè¿™ä¸€çº§çš„æ®‹å·®ã€‚è¾“å…¥å›¾åƒåœ¨æ¯ä¸€çº§ä¹Ÿç»è¿‡ä¸€ä¸ªåå·ç§¯å±‚ä½¿å°ºå¯¸ä¸Šé‡‡æ ·2å€ï¼Œå†ä¸å¯¹åº”çº§çš„æ®‹å·®ç›¸åŠ ï¼Œå°±èƒ½é‡æ„å‡ºè¿™ä¸€çº§çš„ä¸Šé‡‡æ ·ç»“æœã€‚LapSRNè®¾è®¡æŸå¤±å‡½æ•°ä¸ºï¼š å…¶ä¸­ï¼Œ å«ä½œCharbonnieræƒ©ç½šå‡½æ•°( èŒƒæ•°çš„å˜å½¢)ï¼Œ å¤§å°è®¾ç½®ä¸º0.001ã€‚xè¡¨ç¤ºä½åˆ†è¾¨ç‡å›¾åƒï¼Œyè¡¨ç¤ºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œrè¡¨ç¤ºæ®‹å·®ï¼Œsè¡¨ç¤ºå¯¹åº”çš„çº§ã€‚Nè¡¨ç¤ºè®­ç»ƒæ—¶batch sizeçš„å¤§å°ï¼ŒLè¡¨ç¤ºç½‘ç»œä¸€å…±æœ‰å¤šå°‘çº§ã€‚é€šè¿‡å°†é«˜åˆ†è¾¨ç‡å›¾ä¸‹é‡‡æ ·ï¼Œåœ¨æ¯ä¸€çº§éƒ½å­˜åœ¨æœ‰å¯¹åº”çš„ground truthè¿›è¡Œç›‘ç£ï¼Œå› æ­¤æ¯ä¸€çº§éƒ½æœ‰ä¸€ä¸ªæŸå¤±ï¼Œè®­ç»ƒçš„æ—¶å€™å°±æ˜¯è¦æŠŠæ¯ä¸€çº§çš„æŸå¤±çš„å’Œé™ä½ã€‚ LapSRNé€šè¿‡é€æ­¥ä¸Šé‡‡æ ·ï¼Œä¸€çº§ä¸€çº§é¢„æµ‹æ®‹å·®çš„æ–¹å¼ï¼Œåœ¨åšé«˜å€ä¸Šé‡‡æ ·æ—¶ï¼Œä¹Ÿèƒ½å¾—åˆ°ä¸­é—´ä½å€ä¸Šé‡‡æ ·ç»“æœçš„è¾“å‡ºã€‚ç”±äºå°ºå¯¸æ˜¯é€æ­¥æ”¾å¤§ï¼Œä¸æ˜¯æ‰€æœ‰çš„æ“ä½œéƒ½åœ¨å¤§å°ºå¯¸ç‰¹å¾ä¸Šè¿›è¡Œï¼Œå› æ­¤é€Ÿåº¦æ¯”è¾ƒå¿«ã€‚LapSRNè®¾è®¡äº†æŸå¤±å‡½æ•°æ¥è®­ç»ƒç½‘ç»œï¼Œå¯¹æ¯ä¸€çº§çš„ç»“æœéƒ½è¿›è¡Œç›‘ç£ï¼Œå› æ­¤å–å¾—äº†ä¸é”™çš„ç»“æœã€‚ github(matconvnet): https://github.com/phoenix104104/LapSRN github(pytorch): https://github.com/twtygqyy/pytorch-LapSRN github(tensorflow): https://github.com/zjuela/LapSRN-tensorflow 9. SRDenseNet (Image Super-Resolution Using Dense Skip Connections, ICCV2017) DenseNetæ˜¯CVPR2017çš„best papaerè·å¥–è®ºæ–‡ã€‚DenseNetåœ¨ç¨ å¯†å—(dense block)ä¸­å°†æ¯ä¸€å±‚çš„ç‰¹å¾éƒ½è¾“å…¥ç»™ä¹‹åçš„æ‰€æœ‰å±‚ï¼Œä½¿æ‰€æœ‰å±‚çš„ç‰¹å¾éƒ½ä¸²è”(concatenate)èµ·æ¥ï¼Œè€Œä¸æ˜¯åƒResNeté‚£æ ·ç›´æ¥ç›¸åŠ ã€‚è¿™æ ·çš„ç»“æ„ç»™æ•´ä¸ªç½‘ç»œå¸¦æ¥äº†å‡è½»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€åŠ å¼ºç‰¹å¾ä¼ æ’­ã€æ”¯æŒç‰¹å¾å¤ç”¨ã€å‡å°‘å‚æ•°æ•°é‡çš„ä¼˜ç‚¹ã€‚ä¸€ä¸ªç¨ å¯†å—çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ dense block SRDenseNetå°†ç¨ å¯†å—ç»“æ„åº”ç”¨åˆ°äº†è¶…åˆ†è¾¨ç‡é—®é¢˜ä¸Šï¼Œå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ SRDenseNet SRDenseNetå¯ä»¥åˆ†æˆå››ä¸ªéƒ¨åˆ†ã€‚é¦–å…ˆæ˜¯ç”¨ä¸€ä¸ªå·ç§¯å±‚å­¦ä¹ ä½å±‚çš„ç‰¹å¾ï¼Œæ¥ç€ç”¨å¤šä¸ªç¨ å¯†å—å­¦ä¹ é«˜å±‚çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡å‡ ä¸ªåå·ç§¯å±‚å­¦åˆ°ä¸Šé‡‡æ ·æ»¤æ³¢å™¨å‚æ•°ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªå·ç§¯å±‚ç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚ æ–‡ç« ä¸­é’ˆå¯¹ç”¨äºæœ€åé‡å»ºçš„è¾“å…¥å†…å®¹ä¸åŒï¼Œè®¾è®¡äº†ä¸‰ç§ç»“æ„å¹¶åšäº†æ¯”è¾ƒã€‚ä¸€æ˜¯åå·ç§¯å±‚åªè¾“å…¥æœ€é¡¶å±‚ç¨ å¯†å—çš„è¾“å‡ºã€‚äºŒæ˜¯æ·»åŠ äº†ä¸€ä¸ªè·³è·ƒè¿æ¥ï¼Œå°†æœ€åº•å±‚å·ç§¯å±‚çš„è¾“å‡ºç‰¹å¾å’Œæœ€é¡¶å±‚ç¨ å¯†å—çš„è¾“å‡ºç‰¹å¾ä¸²è”èµ·æ¥ï¼Œå†è¾“å…¥åå·ç§¯å±‚ã€‚ä¸‰æ˜¯æ·»åŠ äº†ç¨ å¯†è·³è·ƒè¿æ¥ï¼Œå°±æ˜¯æŠŠç¨ å¯†å—çœ‹æˆä¸€ä¸ªæ•´ä½“ï¼Œç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å‡ºä»¥åŠæ¯ä¸ªç¨ å¯†å—çš„è¾“å‡ºï¼Œéƒ½è¾“å…¥ç»™åœ¨ä¹‹åçš„æ‰€æœ‰ç¨ å¯†å—ï¼Œåƒæ˜¯æŠŠåœ¨åå·ç§¯å±‚ä¹‹å‰çš„æ•´ä¸ªç½‘ç»œä¹Ÿè®¾è®¡æˆåƒç¨ å¯†å—é‚£æ ·çš„ç»“æ„ã€‚ç”±äºè¿™æ ·åšï¼Œæ‰€æœ‰çš„ç‰¹å¾éƒ½ä¸²è”èµ·æ¥ï¼Œè¿™æ ·ç›´æ¥è¾“å…¥åå·ç§¯å±‚ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå› æ­¤æ·»åŠ äº†ä¸€ä¸ªæ ¸å¤§å°ä¸º1Ã—1çš„å·ç§¯å±‚æ¥å‡å°ç‰¹å¾æ•°é‡ï¼Œè¿™ä¸ªå·ç§¯å±‚è¢«ç§°ä¸ºç“¶é¢ˆå±‚ã€‚æœ€åçš„ç»“æœæ˜¯è¶Šå¤æ‚çš„è¶Šå¥½ï¼Œ3>2>1ã€‚æ–‡ç« ä¸­åˆ†æçš„æ˜¯ï¼Œå—ç›Šäºä½å±‚ç‰¹å¾å’Œé«˜å±‚ç‰¹å¾çš„ç»“åˆï¼Œè¶…åˆ†è¾¨ç‡é‡å»ºçš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚åƒç¬¬ä¸‰ç§ç»“æ„æŠŠæ‰€æœ‰æ·±åº¦å±‚çš„ç‰¹å¾éƒ½ä¸²è”èµ·æ¥ï¼Œå¾—åˆ°äº†æœ€ä½³çš„ç»“æœï¼Œè¯´æ˜ä¸åŒæ·±åº¦å±‚çš„ç‰¹å¾ä¹‹é—´åŒ…å«çš„ä¿¡æ¯æ˜¯äº’è¡¥çš„ã€‚ 10. SRGAN(SRResNet) (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, CVPR2017) åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œå°†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(Generative Adversarial Network, GAN)ç”¨åœ¨äº†è§£å†³è¶…åˆ†è¾¨ç‡é—®é¢˜ä¸Šã€‚æ–‡ç« æåˆ°ï¼Œè®­ç»ƒç½‘ç»œæ—¶ç”¨å‡æ–¹å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè™½ç„¶èƒ½å¤Ÿè·å¾—å¾ˆé«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼Œä½†æ˜¯æ¢å¤å‡ºæ¥çš„å›¾åƒé€šå¸¸ä¼šä¸¢å¤±é«˜é¢‘ç»†èŠ‚ï¼Œä½¿äººä¸èƒ½æœ‰å¥½çš„è§†è§‰æ„Ÿå—ã€‚SRGANåˆ©ç”¨æ„ŸçŸ¥æŸå¤±(perceptual loss)å’Œå¯¹æŠ—æŸå¤±(adversarial loss)æ¥æå‡æ¢å¤å‡ºçš„å›¾ç‰‡çš„çœŸå®æ„Ÿã€‚æ„ŸçŸ¥æŸå¤±æ˜¯åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–å‡ºçš„ç‰¹å¾ï¼Œé€šè¿‡æ¯”è¾ƒç”Ÿæˆå›¾ç‰‡ç»è¿‡å·ç§¯ç¥ç»ç½‘ç»œåçš„ç‰¹å¾å’Œç›®æ ‡å›¾ç‰‡ç»è¿‡å·ç§¯ç¥ç»ç½‘ç»œåçš„ç‰¹å¾çš„å·®åˆ«ï¼Œä½¿ç”Ÿæˆå›¾ç‰‡å’Œç›®æ ‡å›¾ç‰‡åœ¨è¯­ä¹‰å’Œé£æ ¼ä¸Šæ›´ç›¸ä¼¼ã€‚ä¸€ä¸ªGANæ‰€è¦å®Œæˆçš„å·¥ä½œï¼ŒGANåŸæ–‡ä¸¾äº†ä¸ªä¾‹å­ï¼šç”Ÿæˆç½‘ç»œ(G)æ˜¯å°å‡é’çš„äººï¼Œåˆ¤åˆ«ç½‘ç»œ(D)æ˜¯æ£€æµ‹å‡é’çš„äººã€‚Gçš„å·¥ä½œæ˜¯è®©è‡ªå·±å°å‡ºæ¥çš„å‡é’å°½é‡èƒ½éª—è¿‡Dï¼ŒDåˆ™è¦å°½å¯èƒ½çš„åˆ†è¾¨è‡ªå·±æ‹¿åˆ°çš„é’ç¥¨æ˜¯é“¶è¡Œä¸­çš„çœŸç¥¨ç¥¨è¿˜æ˜¯Gå°å‡ºæ¥çš„å‡ç¥¨ç¥¨ã€‚å¼€å§‹çš„æ—¶å€™å‘¢ï¼ŒGæŠ€æœ¯ä¸è¿‡å…³ï¼ŒDèƒ½æŒ‡å‡ºè¿™ä¸ªå‡é’å“ªé‡Œå¾ˆå‡ã€‚Gæ¯æ¬¡å¤±è´¥ä¹‹åéƒ½è®¤çœŸæ€»ç»“ç»éªŒï¼ŒåŠªåŠ›æå‡è‡ªå·±ï¼Œæ¯æ¬¡éƒ½è¿›æ­¥ã€‚ç›´åˆ°æœ€åï¼ŒDæ— æ³•åˆ¤æ–­é’ç¥¨çš„çœŸå‡â€¦â€¦SRGANçš„å·¥ä½œå°±æ˜¯ï¼š Gç½‘é€šè¿‡ä½åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œç”±Dç½‘åˆ¤æ–­æ‹¿åˆ°çš„å›¾åƒæ˜¯ç”±Gç½‘ç”Ÿæˆçš„ï¼Œè¿˜æ˜¯æ•°æ®åº“ä¸­çš„åŸå›¾åƒã€‚å½“Gç½‘èƒ½æˆåŠŸéª—è¿‡Dç½‘çš„æ—¶å€™ï¼Œé‚£æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è¿™ä¸ªGANå®Œæˆè¶…åˆ†è¾¨ç‡äº†ã€‚ æ–‡ç« ä¸­ï¼Œç”¨å‡æ–¹è¯¯å·®ä¼˜åŒ–SRResNet(SRGANçš„ç”Ÿæˆç½‘ç»œéƒ¨åˆ†)ï¼Œèƒ½å¤Ÿå¾—åˆ°å…·æœ‰å¾ˆé«˜çš„å³°å€¼ä¿¡å™ªæ¯”çš„ç»“æœã€‚åœ¨è®­ç»ƒå¥½çš„VGGæ¨¡å‹çš„é«˜å±‚ç‰¹å¾ä¸Šè®¡ç®—æ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–SRGANï¼Œå¹¶ç»“åˆSRGANçš„åˆ¤åˆ«ç½‘ç»œï¼Œèƒ½å¤Ÿå¾—åˆ°å³°å€¼ä¿¡å™ªæ¯”è™½ç„¶ä¸æ˜¯æœ€é«˜ï¼Œä½†æ˜¯å…·æœ‰é€¼çœŸè§†è§‰æ•ˆæœçš„ç»“æœã€‚SRGANç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ åœ¨ç”Ÿæˆç½‘ç»œéƒ¨åˆ†(SRResNet)éƒ¨åˆ†åŒ…å«å¤šä¸ªæ®‹å·®å—ï¼Œæ¯ä¸ªæ®‹å·®å—ä¸­åŒ…å«ä¸¤ä¸ª3Ã—3çš„å·ç§¯å±‚ï¼Œå·ç§¯å±‚åæ¥æ‰¹è§„èŒƒåŒ–å±‚(batch normalization, BN)å’ŒPReLUä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œä¸¤ä¸ª2Ã—äºšåƒç´ å·ç§¯å±‚(sub-pixel convolution layers)è¢«ç”¨æ¥å¢å¤§ç‰¹å¾å°ºå¯¸ã€‚åœ¨åˆ¤åˆ«ç½‘ç»œéƒ¨åˆ†åŒ…å«8ä¸ªå·ç§¯å±‚ï¼Œéšç€ç½‘ç»œå±‚æ•°åŠ æ·±ï¼Œç‰¹å¾ä¸ªæ•°ä¸æ–­å¢åŠ ï¼Œç‰¹å¾å°ºå¯¸ä¸æ–­å‡å°ï¼Œé€‰å–æ¿€æ´»å‡½æ•°ä¸ºLeakyReLUï¼Œæœ€ç»ˆé€šè¿‡ä¸¤ä¸ªå…¨è¿æ¥å±‚å’Œæœ€ç»ˆçš„sigmoidæ¿€æ´»å‡½æ•°å¾—åˆ°é¢„æµ‹ä¸ºè‡ªç„¶å›¾åƒçš„æ¦‚ç‡ã€‚SRGANçš„æŸå¤±å‡½æ•°ä¸ºï¼š å…¶ä¸­å†…å®¹æŸå¤±å¯ä»¥æ˜¯åŸºäºå‡æ–¹è¯¯å·®çš„æŸå¤±çš„æŸå¤±å‡½æ•°ï¼š ä¹Ÿå¯ä»¥æ˜¯åŸºäºè®­ç»ƒå¥½çš„ä»¥ReLUä¸ºæ¿€æ´»å‡½æ•°çš„VGGæ¨¡å‹çš„æŸå¤±å‡½æ•°: iå’Œjè¡¨ç¤ºVGG19ç½‘ç»œä¸­ç¬¬iä¸ªæœ€å¤§æ± åŒ–å±‚(maxpooling)åçš„ç¬¬jä¸ªå·ç§¯å±‚å¾—åˆ°çš„ç‰¹å¾ã€‚å¯¹æŠ—æŸå¤±ä¸ºï¼š æ–‡ç« ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç”¨åŸºäºå‡æ–¹è¯¯å·®çš„æŸå¤±å‡½æ•°è®­ç»ƒçš„SRResNetï¼Œå¾—åˆ°äº†ç»“æœå…·æœ‰å¾ˆé«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼Œä½†æ˜¯ä¼šä¸¢å¤±ä¸€äº›é«˜é¢‘éƒ¨åˆ†ç»†èŠ‚ï¼Œå›¾åƒæ¯”è¾ƒå¹³æ»‘ã€‚è€ŒSRGANå¾—åˆ°çš„ç»“æœåˆ™æœ‰æ›´å¥½çš„è§†è§‰æ•ˆæœã€‚å…¶ä¸­ï¼Œåˆå¯¹å†…å®¹æŸå¤±åˆ†åˆ«è®¾ç½®æˆåŸºäºå‡æ–¹è¯¯å·®ã€åŸºäºVGGæ¨¡å‹ä½å±‚ç‰¹å¾å’ŒåŸºäºVGGæ¨¡å‹é«˜å±‚ç‰¹å¾ä¸‰ç§æƒ…å†µä½œäº†æ¯”è¾ƒï¼Œåœ¨åŸºäºå‡æ–¹è¯¯å·®çš„æ—¶å€™è¡¨ç°æœ€å·®ï¼ŒåŸºäºVGGæ¨¡å‹é«˜å±‚ç‰¹å¾æ¯”åŸºäºVGGæ¨¡å‹ä½å±‚ç‰¹å¾çš„å†…å®¹æŸå¤±èƒ½ç”Ÿæˆæ›´å¥½çš„çº¹ç†ç»†èŠ‚ã€‚ github(tensorflow): https://github.com/zsdonghao/SRGAN github(tensorflow): https://github.com/buriburisuri/SRGAN github(torch): https://github.com/junhocho/SRGAN github(caffe): https://github.com/ShenghaiRong/caffe_srgan github(tensorflow): https://github.com/brade31919/SRGAN-tensorflow github(keras): https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks github(pytorch): ai-tor/PyTorch-SRGAN 11. EDSR (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW2017) EDSRæ˜¯NTIRE2017è¶…åˆ†è¾¨ç‡æŒ‘æˆ˜èµ›ä¸Šè·å¾—å† å†›çš„æ–¹æ¡ˆã€‚å¦‚è®ºæ–‡ä¸­æ‰€è¯´ï¼ŒEDSRæœ€æœ‰æ„ä¹‰çš„æ¨¡å‹æ€§èƒ½æå‡æ˜¯å»é™¤æ‰äº†SRResNetå¤šä½™çš„æ¨¡å—ï¼Œä»è€Œå¯ä»¥æ‰©å¤§æ¨¡å‹çš„å°ºå¯¸æ¥æå‡ç»“æœè´¨é‡ã€‚EDSRçš„ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ EDSR å¯ä»¥çœ‹åˆ°ï¼ŒEDSRåœ¨ç»“æ„ä¸Šä¸SRResNetç›¸æ¯”ï¼Œå°±æ˜¯æŠŠæ‰¹è§„èŒƒåŒ–å¤„ç†(batch normalization, BN)æ“ä½œç»™å»æ‰äº†ã€‚æ–‡ç« ä¸­è¯´ï¼ŒåŸå§‹çš„ResNetæœ€ä¸€å¼€å§‹æ˜¯è¢«æå‡ºæ¥è§£å†³é«˜å±‚çš„è®¡ç®—æœºè§†è§‰é—®é¢˜ï¼Œæ¯”å¦‚åˆ†ç±»å’Œæ£€æµ‹ï¼Œç›´æ¥æŠŠResNetçš„ç»“æ„åº”ç”¨åˆ°åƒè¶…åˆ†è¾¨ç‡è¿™æ ·çš„ä½å±‚è®¡ç®—æœºè§†è§‰é—®é¢˜ï¼Œæ˜¾ç„¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚ç”±äºæ‰¹è§„èŒƒåŒ–å±‚æ¶ˆè€—äº†ä¸å®ƒå‰é¢çš„å·ç§¯å±‚ç›¸åŒå¤§å°çš„å†…å­˜ï¼Œåœ¨å»æ‰è¿™ä¸€æ­¥æ“ä½œåï¼Œç›¸åŒçš„è®¡ç®—èµ„æºä¸‹ï¼ŒEDSRå°±å¯ä»¥å †å æ›´å¤šçš„ç½‘ç»œå±‚æˆ–è€…ä½¿æ¯å±‚æå–æ›´å¤šçš„ç‰¹å¾ï¼Œä»è€Œå¾—åˆ°æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚EDSRç”¨L1èŒƒæ•°æ ·å¼çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç½‘ç»œæ¨¡å‹ã€‚åœ¨è®­ç»ƒæ—¶å…ˆè®­ç»ƒä½å€æ•°çš„ä¸Šé‡‡æ ·æ¨¡å‹ï¼Œæ¥ç€ç”¨è®­ç»ƒä½å€æ•°ä¸Šé‡‡æ ·æ¨¡å‹å¾—åˆ°çš„å‚æ•°æ¥åˆå§‹åŒ–é«˜å€æ•°çš„ä¸Šé‡‡æ ·æ¨¡å‹ï¼Œè¿™æ ·èƒ½å‡å°‘é«˜å€æ•°ä¸Šé‡‡æ ·æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶è®­ç»ƒç»“æœä¹Ÿæ›´å¥½ã€‚ è¿™ç¯‡æ–‡ç« è¿˜æå‡ºäº†ä¸€ä¸ªèƒ½åŒæ—¶ä¸åŒä¸Šé‡‡æ ·å€æ•°çš„ç½‘ç»œç»“æ„MDSRï¼Œå¦‚ä¸‹å›¾ã€‚ MDSR MDSRçš„ä¸­é—´éƒ¨åˆ†è¿˜æ˜¯å’ŒEDSRä¸€æ ·ï¼Œåªæ˜¯åœ¨ç½‘ç»œå‰é¢æ·»åŠ äº†ä¸åŒçš„é¢„è®­ç»ƒå¥½çš„æ¨¡å‹æ¥å‡å°‘ä¸åŒå€æ•°çš„è¾“å…¥å›¾ç‰‡çš„å·®å¼‚ã€‚åœ¨ç½‘ç»œæœ€åï¼Œä¸åŒå€æ•°ä¸Šé‡‡æ ·çš„ç»“æ„å¹³è¡Œæ’åˆ—æ¥è·å¾—ä¸åŒå€æ•°çš„è¾“å‡ºç»“æœã€‚ ä»æ–‡ç« ç»™å‡ºçš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼ŒEDSRèƒ½å¤Ÿå¾—åˆ°å¾ˆå¥½çš„ç»“æœã€‚å¢å¤§æ¨¡å‹å‚æ•°æ•°é‡ä»¥åï¼Œç»“æœåˆæœ‰äº†è¿›ä¸€æ­¥çš„æå‡ã€‚å› æ­¤å¦‚æœèƒ½å¤Ÿè§£å†³è®­ç»ƒå›°éš¾çš„é—®é¢˜ï¼Œç½‘ç»œè¶Šæ·±ï¼Œå‚æ•°è¶Šå¤šï¼Œå¯¹æå‡ç»“æœç¡®å®æ˜¯æœ‰å¸®åŠ©å§ã€‚ github(torch): https://github.com/LimBee/NTIRE2017 github(tensorflow): https://github.com/jmiller656/EDSR-Tensorflow github(pytorch): https://github.com/thstkdgus35/EDSR-PyTorch é€šè¿‡ä»¥ä¸Š11ç¯‡æœ‰å…³æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„è®ºæ–‡ï¼Œå¯ä»¥çœ‹åˆ°é€šè¿‡ç½‘ç»œç»“æ„ã€æŸå¤±å‡½æ•°ä»¥åŠè®­ç»ƒæ–¹å¼çš„æ¼”å˜ï¼Œæ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡æ–¹æ³•åœ¨ç»“æœã€é€Ÿåº¦ä»¥åŠåº”ç”¨æ€§ä¸Šéƒ½æœ‰äº†ä¸æ–­çš„æé«˜ã€‚è¿™é‡Œå†æ”¾ä¸Šä¸€ç¯‡æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡æ–¹æ³•ç»¼è¿°çš„é“¾æ¥(Super-Resolution via Deep Learning)ä»¥åŠgithubä¸Šä¸€ä¸ªè¶…åˆ†è¾¨ç‡æ–¹æ³•çš„æ€»ç»“(https://github.com/YapengTian/Single-Image-Super-Resolution)ã€‚ éå¸¸æ„Ÿè°¢è®¸å¤šçŸ¥ä¹å’Œåšå®¢ä¸Šçš„æ–‡ç« ï¼Œç”±äºæ¯”è¾ƒå¤šï¼Œè¿™é‡Œåˆ—å‡ºå‚è€ƒå¾—æ¯”è¾ƒå¤šçš„å‡ ä¸ªèµ„æºï¼š https://zhuanlan.zhihu.com/p/25532538?utm_source=tuicool&utm_medium=referral http://blog.csdn.net/u011692048/article/category/7121139 http://blog.csdn.net/wangkun1340378/article/category/7004439 "},"super_resolution/code_dataset.html":{"url":"super_resolution/code_dataset.html","title":"è¶…åˆ†è¾¨ç‡ä»£ç æ•°æ®é›†åˆé›†","keywords":"","body":"Awesome-Super-Resolutionï¼ˆin progressï¼‰ repositories repositories Awesome paper list: å›¾åƒè¶…åˆ†è¾¨ï¼š https://github.com/YapengTian/Single-Image-Super-Resolution è¶…åˆ†è¾¨Benckmarkï¼š https://github.com/huangzehao/Super-Resolution.Benckmark è§†é¢‘è¶…åˆ†è¾¨ï¼š https://github.com/flyywh/Video-Super-Resolution https://github.com/LoSealL/VideoSuperResolution Awesome paper list: Single-Image-Super-Resolution Super-Resolution.Benckmark Video-Super-Resolution VideoSuperResolution Awesome repos: repo Framework EDSR-PyTorch PyTorch Image-Super-Resolution Keras image-super-resolution Keras Super-Resolution-Zoo MxNet super-resolution Keras neural-enhance Theano srez Tensorflow waifu2x Torch BasicSR PyTorch super-resolution PyTorch VideoSuperResolution Tensorflow video-super-resolution Pytorch Datasets Note this table is referenced from here. Name Usage Link Comments Set5 Test download jbhuang0604 SET14 Test download jbhuang0604 BSD100 Test download jbhuang0604 Urban100 Test download jbhuang0604 Manga109 Test website SunHay80 Test download jbhuang0604 BSD300 Train/Val download BSD500 Train/Val download 91-Image Train download Yang DIV2K2017 Train/Val website NTIRE2017 Real SR Train/Val website NTIRE2019 Waterloo Train website VID4 Test download 4 videos MCL-V Train website 12 videos GOPRO Train/Val website 33 videos, deblur CelebA Train website Human faces Sintel Train/Val website Optical flow FlyingChairs Train website Optical flow Vimeo-90k Train/Test website 90k HQ videos Dataset collections Benckmark and DIV2K: Set5, Set14, B100, Urban100, Manga109, DIV2K2017 include bicubic downsamples with x2,3,4,8 SR_testing_datasets: Test: Set5, Set14, B100, Urban100, Manga109, Historical; Train: T91,General100, BSDS200 paper Non-DL based approach SCSR: TIP2010, Jianchao Yang et al.paper, code ANR: ICCV2013, Radu Timofte et al. paper, code A+: ACCV 2014, Radu Timofte et al. paper, code IA: CVPR2016, Radu Timofte et al. paper SelfExSR: CVPR2015, Jia-Bin Huang et al. paper, code NBSRF: ICCV2015, Jordi Salvador et al. paper RFL: ICCV2015, Samuel Schulter et al paper, code DL based approach Note this table is referenced from here Model Published Code Keywords SRCNN ECCV14 Keras Kaiming RAISR arXiv - Google, Pixel 3 ESPCN CVPR16 Keras Real time/SISR/VideoSR VDSR CVPR16 Matlab Deep, Residual DRCN CVPR16 Matlab Recurrent DRRN CVPR17 Caffe, PyTorch Recurrent LapSRN CVPR17 Matlab Huber loss IRCNN CVPR17 Matlab EDSR CVPR17 PyTorch NTIRE17 Champion BTSRN CVPR17 - NTIRE17 SelNet CVPR17 - NTIRE17 TLSR CVPR17 - NTIRE17 SRGAN CVPR17 Tensorflow 1st proposed GAN VESPCN CVPR17 - VideoSR MemNet ICCV17 Caffe SRDenseNet ICCV17 -, PyTorch Dense SPMC ICCV17 Tensorflow VideoSR EnhanceNet ICCV17 TensorFlow Perceptual Loss PRSR ICCV17 TensorFlow an extension of PixelCNN AffGAN ICLR17 - MS-LapSRN TPAMI18 Matlab Fast LapSRN DCSCN arXiv Tensorflow IDN CVPR18 Caffe Fast DSRN CVPR18 TensorFlow Dual stateï¼ŒRecurrent RDN CVPR18 Torch Deep, BI-BD-DN SRMD CVPR18 Matlab Denoise/Deblur/SR DBPN CVPR18 PyTorch NTIRE18 Champion WDSR CVPR18 PyTorchï¼ŒTensorFlow NTIRE18 Champion ProSRN CVPR18 PyTorch NTIRE18 ZSSR CVPR18 Tensorflow Zero-shot FRVSR CVPR18 PDF VideoSR DUF CVPR18 Tensorflow VideoSR TDAN arXiv - VideoSRï¼ŒDeformable Align SFTGAN CVPR18 PyTorch CARN ECCV18 PyTorch Lightweight RCAN ECCV18 PyTorch Deep, BI-BD-DN MSRN ECCV18 PyTorch SRFeat ECCV18 Tensorflow GAN ESRGAN ECCV18 PyTorch PRIM18 region 3 Champion FEQE ECCV18 Tensorflow Fast NLRN NIPS18 Tensorflow Non-local, Recurrent SRCliqueNet NIPS18 - Wavelet CBDNet arXiv Matlab Blind-denoise TecoGAN arXiv Tensorflow VideoSR GAN RBPN CVPR19 PyTorch VideoSR SRFBN CVPR19 PyTorch Feedback MoreMNAS arXiv - Lightweightï¼ŒNAS FALSR arXiv TensorFlow Lightweightï¼ŒNAS Meta-SR arXiv Arbitrary Magnification AWSRN arXiv PyTorch Lightweight OISR CVPR19 PyTorch ODE-inspired Network DPSR CVPR19 PyTorch DNI CVPR19 PyTorch MAANet arXiv Multi-view Aware Attention RNAN ICLR19 PyTorch Residual Non-local Attention FSTRN CVPR19 - VideoSR, fast spatio-temporal residual block MsDNN arXiv TensorFlow NTIRE19 real SR 21th place Super Resolution surveyï¼š [1] Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang, Jing-Hao Xue. Deep Learning for Single Image Super-Resolution: A Brief Review. arxiv, 2018. paper [2]Saeed Anwar, Salman Khan, Nick Barnes. A Deep Journey into Super-resolution: A survey. arxiv, 2019.paper Super-Resolution.Benckmark A curated list of super-resolution resources and a benchmark for single image super-resolution algorithms. See my implementated super-resolution algorithms: SRGAN VDSR CSCN TODO Build a benckmark like SelfExSR_Code State-of-the-art algorithms Classical Sparse Coding Method ScSR [Web] Image super-resolution as sparse representation of raw image patches (CVPR2008), Jianchao Yang et al. Image super-resolution via sparse representation (TIP2010), Jianchao Yang et al. Coupled dictionary training for image super-resolution (TIP2011), Jianchao Yang et al. Anchored Neighborhood Regression Method ANR [Web] Anchored Neighborhood Regression for Fast Example-Based Super-Resolution (ICCV2013), Radu Timofte et al. A+ [Web] A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution (ACCV2014), Radu Timofte et al. IA [Web] Seven ways to improve example-based single image super resolution (CVPR2016), Radu Timofte et al. Self-Exemplars SelfExSR [Web] Single Image Super-Resolution from Transformed Self-Exemplars (CVPR2015), Jia-Bin Huang et al. Bayes NBSRF [Web] Naive Bayes Super-Resolution Forest (ICCV2015), Jordi Salvador et al. Deep Learning Method SRCNN [Web] [waifu2x by nagadomi] Image Super-Resolution Using Deep Convolutional Networks (ECCV2014), Chao Dong et al. Image Super-Resolution Using Deep Convolutional Networks (TPAMI2015), Chao Dong et al. CSCN [Web] Deep Networks for Image Super-Resolution with Sparse Prior (ICCV2015), Zhaowen Wang et al. Robust Single Image Super-Resolution via Deep Networks with Sparse Prior (TIP2016), Ding Liu et al. VDSR [Web] [Unofficial Implementation in Caffe] Accurate Image Super-Resolution Using Very Deep Convolutional Networks (CVPR2016), Jiwon Kim et al. DRCN [Web] Deeply-Recursive Convolutional Network for Image Super-Resolution (CVPR2016), Jiwon Kim et al. ESPCN [PDF] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (CVPR2016), Wenzhe Shi et al. Is the deconvolution layer the same as a convolutional layer? [PDF] Checkerboard artifact free sub-pixel convolution [PDF] FSRCNN [Web] Acclerating the Super-Resolution Convolutional Neural Network (ECCV2016), Dong Chao et al. LapSRN [Web] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017), Wei-Sheng Lai et al. EDSR [PDF] Enhanced Deep Residual Networks for Single Image Super-Resolution (Winner of NTIRE2017 Super-Resolution Challenge), Bee Lim et al. Perceptual Loss and GAN Perceptual Loss [PDF] Perceptual Losses for Real-Time Style Transfer and Super-Resolution (ECCV2016), Justin Johnson et al. SRGAN [PDF] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (CVPR2017), Christian Ledig et al. AffGAN [PDF] AMORTISED MAP INFERENCE FOR IMAGE SUPER-RESOLUTION (ICLR2017), Casper Kaae SÃ¸nderby et al. EnhanceNet [PDF] EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis, Mehdi S. M. Sajjadi et al. neural-enchance [Github] Video SR VESPCN [PDF] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation (CVPR2017), Jose Caballero et al. Dicussion Deconvolution and Sub-Pixel Convolution Deconvolution and Checkerboard Artifacts SubPixel Datasets Test Dataset Image source Set 5 Bevilacqua et al. BMVC 2012 Set 14 Zeyde et al. LNCS 2010 BSD 100 Martin et al. ICCV 2001 Urban 100 Huang et al. CVPR 2015 Train Dataset Image source Yang 91 Yang et al. CVPR 2008 BSD 200 Martin et al. ICCV 2001 General 100 Dong et al. ECCV 2016 ImageNet Olga Russakovsky et al. IJCV 2015 COCO Tsung-Yi Lin et al. ECCV 2014 Quantitative comparisons Results from papers of VDSR, DRCN, CSCN and IA. Note: IA use enchanced prediction trick to improve result. Results on Set 5 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 33.66/0.9929 36.54/0.9544 36.66/0.9542 36.49/0.9537 36.93/0.9552 37.53/0.9587 37.63/0.9588 37.39/ 3x - PSNR/SSIM 30.39/0.8682 32.59/0.9088 32.75/0.9090 32.58/0.9093 33.10/0.9144 33.66/0.9213 33.82/0.9226 33.46/ 4x - PSNR/SSIM 28.42/0.8104 30.28/0.8603 30.48/0.8628 30.31/0.8619 30.86/0.8732 31.35/0.8838 31.53/0.8854 31.10/ Results on Set 14 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 30.24/0.8688 32.28/0.9056 32.42/0.9063 32.22/0.9034 32.56/0.9074 33.03/0.9124 33.04/0.9118 32.87/ 3x - PSNR/SSIM 27.55/0.7742 29.13/0.8188 29.28/0.8209 29.16/0.8196 29.41/0.8238 29.77/0.8314 29.76/0.8311 29.69/ 4x - PSNR/SSIM 26.00/0.7027 27.32/0.7491 27.49/0.7503 27.40/0.7518 27.64/0.7587 28.01/0.7674 28.02/0.7670 27.88/ Results on BSD 100 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 29.56/0.8431 31.21/0.8863 31.36/0.8879 31.18/0.8855 31.40/0.8884 31.90/0.8960 31.85/0.8942 31.79/ 3x - PSNR/SSIM 27.21/0.7385 28.29/0.7835 28.41/0.7863 28.29/0.7840 28.50/0.7885 28.82/0.7976 28.80/0.7963 28.76/ 4x - PSNR/SSIM 25.96/0.6675 26.82/0.7087 "},"super_resolution/baseline.html":{"url":"super_resolution/baseline.html","title":"è¶…åˆ†è¾¨ç‡baseline","keywords":"","body":"Super-Resolution.Benckmark A curated list of super-resolution resources and a benchmark for single image super-resolution algorithms. See my implementated super-resolution algorithms: SRGAN VDSR CSCN TODO Build a benckmark like SelfExSR_Code State-of-the-art algorithms Classical Sparse Coding Method ScSR [Web] Image super-resolution as sparse representation of raw image patches (CVPR2008), Jianchao Yang et al. Image super-resolution via sparse representation (TIP2010), Jianchao Yang et al. Coupled dictionary training for image super-resolution (TIP2011), Jianchao Yang et al. Anchored Neighborhood Regression Method ANR [Web] Anchored Neighborhood Regression for Fast Example-Based Super-Resolution (ICCV2013), Radu Timofte et al. A+ [Web] A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution (ACCV2014), Radu Timofte et al. IA [Web] Seven ways to improve example-based single image super resolution (CVPR2016), Radu Timofte et al. Self-Exemplars SelfExSR [Web] Single Image Super-Resolution from Transformed Self-Exemplars (CVPR2015), Jia-Bin Huang et al. Bayes NBSRF [Web] Naive Bayes Super-Resolution Forest (ICCV2015), Jordi Salvador et al. Deep Learning Method SRCNN [Web] [waifu2x by nagadomi] Image Super-Resolution Using Deep Convolutional Networks (ECCV2014), Chao Dong et al. Image Super-Resolution Using Deep Convolutional Networks (TPAMI2015), Chao Dong et al. CSCN [Web] Deep Networks for Image Super-Resolution with Sparse Prior (ICCV2015), Zhaowen Wang et al. Robust Single Image Super-Resolution via Deep Networks with Sparse Prior (TIP2016), Ding Liu et al. VDSR [Web] [Unofficial Implementation in Caffe] Accurate Image Super-Resolution Using Very Deep Convolutional Networks (CVPR2016), Jiwon Kim et al. DRCN [Web] Deeply-Recursive Convolutional Network for Image Super-Resolution (CVPR2016), Jiwon Kim et al. ESPCN [PDF] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (CVPR2016), Wenzhe Shi et al. Is the deconvolution layer the same as a convolutional layer? [PDF] Checkerboard artifact free sub-pixel convolution [PDF] FSRCNN [Web] Acclerating the Super-Resolution Convolutional Neural Network (ECCV2016), Dong Chao et al. LapSRN [Web] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017), Wei-Sheng Lai et al. EDSR [PDF] Enhanced Deep Residual Networks for Single Image Super-Resolution (Winner of NTIRE2017 Super-Resolution Challenge), Bee Lim et al. Perceptual Loss and GAN Perceptual Loss [PDF] Perceptual Losses for Real-Time Style Transfer and Super-Resolution (ECCV2016), Justin Johnson et al. SRGAN [PDF] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (CVPR2017), Christian Ledig et al. AffGAN [PDF] AMORTISED MAP INFERENCE FOR IMAGE SUPER-RESOLUTION (ICLR2017), Casper Kaae SÃ¸nderby et al. EnhanceNet [PDF] EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis, Mehdi S. M. Sajjadi et al. neural-enchance [Github] Video SR VESPCN [PDF] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation (CVPR2017), Jose Caballero et al. Dicussion Deconvolution and Sub-Pixel Convolution Deconvolution and Checkerboard Artifacts SubPixel Datasets Test Dataset Image source Set 5 Bevilacqua et al. BMVC 2012 Set 14 Zeyde et al. LNCS 2010 BSD 100 Martin et al. ICCV 2001 Urban 100 Huang et al. CVPR 2015 Train Dataset Image source Yang 91 Yang et al. CVPR 2008 BSD 200 Martin et al. ICCV 2001 General 100 Dong et al. ECCV 2016 ImageNet Olga Russakovsky et al. IJCV 2015 COCO Tsung-Yi Lin et al. ECCV 2014 Quantitative comparisons Results from papers of VDSR, DRCN, CSCN and IA. Note: IA use enchanced prediction trick to improve result. Results on Set 5 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 33.66/0.9929 36.54/0.9544 36.66/0.9542 36.49/0.9537 36.93/0.9552 37.53/0.9587 37.63/0.9588 37.39/ 3x - PSNR/SSIM 30.39/0.8682 32.59/0.9088 32.75/0.9090 32.58/0.9093 33.10/0.9144 33.66/0.9213 33.82/0.9226 33.46/ 4x - PSNR/SSIM 28.42/0.8104 30.28/0.8603 30.48/0.8628 30.31/0.8619 30.86/0.8732 31.35/0.8838 31.53/0.8854 31.10/ Results on Set 14 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 30.24/0.8688 32.28/0.9056 32.42/0.9063 32.22/0.9034 32.56/0.9074 33.03/0.9124 33.04/0.9118 32.87/ 3x - PSNR/SSIM 27.55/0.7742 29.13/0.8188 29.28/0.8209 29.16/0.8196 29.41/0.8238 29.77/0.8314 29.76/0.8311 29.69/ 4x - PSNR/SSIM 26.00/0.7027 27.32/0.7491 27.49/0.7503 27.40/0.7518 27.64/0.7587 28.01/0.7674 28.02/0.7670 27.88/ Results on BSD 100 Scale Bicubic A+ SRCNN SelfExSR CSCN VDSR DRCN IA 2x - PSNR/SSIM 29.56/0.8431 31.21/0.8863 31.36/0.8879 31.18/0.8855 31.40/0.8884 31.90/0.8960 31.85/0.8942 31.79/ 3x - PSNR/SSIM 27.21/0.7385 28.29/0.7835 28.41/0.7863 28.29/0.7840 28.50/0.7885 28.82/0.7976 28.80/0.7963 28.76/ 4x - PSNR/SSIM 25.96/0.6675 26.82/0.7087 26.90/0.7101 26.84/0.7106 27.03/0.7161 27.29/0.7251 27.23/0.7233 27.25/ "},"super_resolution/loss.html":{"url":"super_resolution/loss.html","title":"è¶…åˆ†è¾¨ç‡çš„æŸå¤±å‡½æ•°æ€»ç»“","keywords":"","body":"è¶…åˆ†è¾¨ç‡çš„æŸå¤±å‡½æ•°æ€»ç»“ MSEï¼Œæ˜¯å›¾åƒç©ºé—´çš„å†…å®¹â€œç›¸ä¼¼â€ï¼Œè€Œåœ¨å›¾åƒä¸Šæ™®éå­˜åœ¨åŒºåŸŸï¼Œå…¶å±äºæŸä¸ªç±»åˆ«ï¼ˆè€è™çš®ï¼Œè‰ï¼Œæ¸”ç½‘ç­‰ï¼‰ï¼Œå¦‚æœå‡ºç°çº¹ç†æˆ–è€…ç½‘æ ¼ï¼Œé‚£ä¹ˆä¼˜åŒ–MSEå¾ˆå®¹æ˜“å°†è¿™ä¸ªåŒºåŸŸç£¨å¹³ï¼Œå³å¹³æ»‘ã€‚ã€ç›´æ¥å¯¹åº”è¯„ä»·æŒ‡æ ‡MSEå’ŒMAEï¼Œä»¥åŠPSNRï¼Œä½†æ˜¯PSNRé«˜ä¸è§å¾—å¥½ï¼Œå›¾åƒå¯èƒ½ä¸è‡ªç„¶ã€‚è§æ–‡çŒ®2çš„å›¾ï¼šPSNR/SSIMä¸æ„ŸçŸ¥è´¨é‡(è§†è§‰æ•ˆæœ)çš„ä¸€è‡´æ€§ã€‘ PSNR/SSIMä¸æ„ŸçŸ¥è´¨é‡(è§†è§‰æ•ˆæœ)çš„ä¸€è‡´æ€§ L1ï¼Œå¯å¿å—å¼‚å¸¸å€¼ï¼Œç›¸è¾ƒäºMSEå’ŒL2æ˜¯æ²¡æœ‰é‚£ä¹ˆå¹³æ»‘ä¸€äº›çš„ã€‚ Perceptual lossï¼Œæ˜¯ç‰¹å¾ç©ºé—´çš„ç±»åˆ«/çº¹ç†â€œç›¸ä¼¼â€ã€‚æ¯•ç«Ÿæœ‰å­¦è€…è®¤ä¸ºæ·±åº¦å·ç§¯ç½‘ç»œç”¨äºå›¾åƒåˆ†ç±»ï¼Œåˆ©ç”¨çš„æ˜¯ç‰©ä½“çš„çº¹ç†å·®å¼‚ã€‚ å¤šå°ºåº¦(MS)-SSIMï¼Œé‚£å°±æ˜¯å›¾åƒç©ºé—´çš„ç»“æ„â€œç›¸ä¼¼â€ã€‚åœ¨æ–‡çŒ®[1]ä¸­ï¼Œå°±æ‰¾åˆ°äº†MS-SSIM+L1çš„æ··åˆæŸå¤±é€‚åˆäºå›¾åƒå¤åŸã€‚ å›¾åƒSRé—®é¢˜æœ‰ä¸¤ä¸ªä¸»è¦æ–¹å‘ï¼Œè§æ–‡çŒ®[2]çš„å›¾ã€‚å…¶ä¸€ä¸ºäº†å®ç°æ›´å¥½çš„å›¾åƒé‡å»ºæ•ˆæœï¼Œå³ç°åº¦/RGBå¤åŸï¼›å…¶äºŒä¸ºäº†å®ç°æ›´å¥½çš„è§†è§‰è´¨é‡ï¼Œå³çœ‹èµ·æ¥â€œè‡ªç„¶â€ã€‚æœ‰æ²¡æœ‰æ–¹æ³•æ˜¯å…¼é¡¾ä¸¤è€…çš„å‘¢ï¼Ÿä¼¼ä¹ã€‚ã€‚ã€‚ å›¾åƒSRçš„ä¸¤ä¸ªæ–¹å‘ [1] Loss Functions for Image Restoration with Neural Networks IEEE TCI 2017 [[paper]] [[code]] [2] 2018 PIRM Challenge on Perceptual Image Super-resolution [[paper]] "},"data/":{"url":"data/","title":"å›¾ç‰‡å’Œæ•°æ®å¤„ç†","keywords":"","body":" å›¾ç‰‡å¤„ç† å›¾åƒæ•°æ®å¢å¼º æ•°æ®å¢å¼º imaaug æ•°æ®å¢å¼ºå¤§æ€å™¨ "},"data/picture.html":{"url":"data/picture.html","title":"å›¾ç‰‡å¤„ç†","keywords":"","body":"å‰è¨€ï¼šç”¨CNNè¿›è¡Œè®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œé€šå¸¸éœ€è¦å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œæœ‰æ—¶å€™ä¹Ÿå«åšæ•°æ®å¢å¼ºï¼Œå¸¸è§çš„å›¾åƒå¤„ç†çš„Pythonåº“ï¼šOpenCVã€PILã€matplotlibã€tensorflowç­‰ï¼Œè¿™é‡Œç”¨TensorFlowä»‹ç»å›¾åƒå¤„ç†çš„è¿‡ç¨‹ å›¾ç‰‡å¤„ç† å±•ç¤ºä¸€å¼ å›¾ç‰‡ æ³¨æ„éœ€è¦å¯¹å›¾åƒè¿›è¡Œè§£ç ï¼Œç„¶åè¿›è¡Œå±•ç¤ºï¼Œç”¨tf.image.decode_png å…ˆå®šä¹‰ä¸€ä¸ªå›¾ç‰‡å±•ç¤ºçš„å‡½æ•°ä»£ç å¦‚ä¸‹ï¼š import numpy as np import tensorflow as tf import matplotlib.pyplot as plt def show_image_tensor(image_tensor): #ä½¿ç”¨äº¤äº’å¼å›è¯ image = image_tensor.eval() print(\"å›¾ç‰‡çš„å¤§å°ä¸ºï¼š{}\".format(image.shape)) if len(image.shape)==3 and image.shape[2]==1: plt.imshow(image[:,:,0],cmap=\"Greys_r\") plt.show() elif len(image.shape)==3: plt.imshow(image) plt.show() è¿›è¡Œå›¾åƒçš„è¯»å–å’Œè§£ç ï¼Œç„¶åè°ƒç”¨å‡½æ•°è¿›è¡Œå±•ç¤º #1è¯»å–ã€ç¼–ç ã€å±•ç¤º file_content=tf.read_file(image_path) image_tensor = tf.image.decode_png(file_content,channels=3) show_image_tensor(image_tensor) ç»“æœå¦‚ä¸‹ï¼š å›¾ç‰‡çš„å¤§å°ä¸ºï¼š(512, 512, 3) ä¿®æ”¹å¤§å°ï¼Œå‹ç¼©æˆ–è€…æ”¾å¤§ ç”¨tf.image.resize_images \"\"\" BILINEAR = 0 çº¿æ€§æ’å€¼ï¼Œé»˜è®¤ NEAREST_NEIGHBOR = 1 æœ€è¿‘é‚»æ’å€¼ï¼Œå¤±çœŸæœ€å° BICUBIC = 2 ä¸‰æ¬¡æ’å€¼ AREA = 3 é¢ç§¯æ’å€¼ # images: ç»™å®šéœ€è¦è¿›è¡Œå¤§å°è½¬æ¢çš„å›¾åƒå¯¹åº”çš„tensorå¯¹è±¡ï¼Œæ ¼å¼ä¸ºï¼š[height, width, num_channels]æˆ–è€…[batch, height, width, num_channels] # APIè¿”å›å€¼å’Œimagesæ ¼å¼ä¸€æ ·ï¼Œå”¯ä¸€åŒºåˆ«æ˜¯heightå’Œwidthå˜åŒ–ä¸ºç»™å®šçš„å€¼ \"\"\" resize_image_tensor = tf.image.resize_images(images=image_tensor,size=(20,20), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) show_image_tensor(resize_image_tensor)#æ³¨æ„å‰é¢è¿›è¡Œè§£ç çš„æ—¶å€™ä¸€å®šè¦ç”¨tf.image.decode_png ç»“æœï¼š å›¾ç‰‡çš„å¤§å°ä¸ºï¼š(20, 20, 3) æ³¨æ„ï¼šå½“æ”¾å¤§æ—¶å€™ï¼Œå‡ ä¹å›¾åƒä¸å¤±çœŸ å‰ªåˆ‡ æˆ–è€…æ˜¯å¡«å……ç”¨tf.image.resize_image_with_crop_or_pad # å›¾ç‰‡é‡ç½®å¤§å°ï¼Œé€šè¿‡å›¾ç‰‡çš„å‰ªåˆ‡æˆ–è€…å¡«å……ï¼ˆä»ä¸­é—´å¼€å§‹è®¡ç®—æ–°å›¾ç‰‡çš„å¤§å°ï¼‰ corp_pad_image_tensor = tf.image.resize_image_with_crop_or_pad(image_tensor,300,300) show_image_tensor(corp_pad_image_tensor) ä¸Šè¿°ä¸ºä¸­é—´ä½ç½®å‰ªåˆ‡æˆ–è€…å¡«å……ï¼Œä¸‹é¢ä»‹ç»ä»»æ„ä½ç½®å‰ªåˆ‡æˆ–è€…å¡«å…… # å¡«å……æ•°æ®ï¼ˆç»™å®šä½ç½®å¼€å§‹å¡«å……ï¼‰ pad_image_tensor = tf.image.pad_to_bounding_box(image=image_tensor, offset_height=200, offset_width=50, target_height=1000,target_width=1000) # show_image_tensor(pad_image_tensor) corp_to_bounding_box_image_tensor=tf.image.crop_to_bounding_box(image=image_tensor, offset_height=20, offset_width=50, target_height=300,target_width=400) show_image_tensor(corp_to_bounding_box_image_tensor) æ•°æ®å¢å¼º å½“è®­ç»ƒæ•°æ®æœ‰é™çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡ä¸€äº›å˜æ¢æ¥ä»å·²æœ‰çš„è®­ ç»ƒæ•°æ®é›†ä¸­ç”Ÿæˆä¸€äº›æ–°çš„æ•°æ®ï¼Œæ¥æ‰©å¤§è®­ç»ƒæ•°æ®ã€‚æ•°æ®å¢å¼ºçš„æ–¹æ³•æœ‰ï¼š é•œåƒï¼Œç¿»è½¬ ä¾‹å¦‚ï¼šä»¥å‚ç›´å¹³é¢ä¸ºå¯¹ç§°è½´å¦‚ä¸‹ï¼š ä»£ç å¦‚ä¸‹ï¼š # ä¸Šä¸‹äº¤æ¢ filp_up_down_image_tensor = tf.image.flip_up_down(image_tensor) # show_image_tensor(filp_up_down_image_tensor) filp_left_right_image_tensor = tf.image.flip_left_right(image_tensor) show_image_tensor(filp_left_right_image_tensor) ä»¥æ°´å¹³é¢ä¸ºå¯¹ç§°è½´å¦‚ä¸‹ï¼š è½¬ç½®ï¼Œç›¸å½“äºçŸ©é˜µçš„è½¬ç½®,90åº¦è½¬æ¢ # è½¬ç½® transpose_image_tensor = tf.image.transpose_image(image_tensor) # show_image_tensor(transpose_image_tensor) # æ—‹è½¬ï¼ˆ90åº¦ã€180åº¦ã€270åº¦....ï¼‰ # k*90åº¦æ—‹è½¬ï¼Œé€†æ—¶é’ˆæ—‹è½¬ k_rot90_image_tensor = tf.image.rot90(image_tensor, k=4) # show_image_tensor(k_rot90_image_tensor) é¢œè‰²ç©ºé—´è½¬æ¢ æ³¨æ„ï¼šé¢œè‰²ç©ºé—´çš„è½¬æ¢å¿…é¡»è®²imageçš„å€¼è½¬æ¢ä¸ºfloat32ç±»å‹ï¼Œä¸èƒ½ä½¿ç”¨unit8ç±»å‹ å›¾åƒåŸºæœ¬æ ¼å¼ï¼š rgbï¼ˆé¢œè‰²ï¼‰0-255,ä¸‰ä¸ª255ä¸ºç™½è‰²ï¼Œè½¬åŒ–ä¸ºfloat32å°±æ˜¯æŠŠåŒºé—´å˜ä¸º0-1 hsvï¼ˆh: å›¾åƒçš„è‰²å½©/è‰²åº¦ï¼Œs:å›¾åƒçš„é¥±å’Œåº¦ï¼Œvï¼šå›¾åƒçš„äº®åº¦ï¼‰ grabï¼ˆç°åº¦ï¼‰ # é¢œè‰²ç©ºé—´çš„è½¬æ¢å¿…é¡»è®²imageçš„å€¼è½¬æ¢ä¸ºfloat32ç±»å‹ï¼Œä¸èƒ½ä½¿ç”¨unit8ç±»å‹ float32_image_tensor = tf.image.convert_image_dtype(image_tensor, dtype=tf.float32) # show_image_tensor(float32_image_tensor) # rgb -> hsvï¼ˆh: å›¾åƒçš„è‰²å½©/è‰²åº¦ï¼Œs:å›¾åƒçš„é¥±å’Œåº¦ï¼Œvï¼šå›¾åƒçš„äº®åº¦ï¼‰ hsv_image_tensor= tf.image.rgb_to_hsv(float32_image_tensor) show_image_tensor(hsv_image_tensor) # hsv -> rgb rgb_image_tensor = tf.image.hsv_to_rgb(float32_image_tensor) # show_image_tensor(rgb_image_tensor) # rgb -> gray gray_image_tensor = tf.image.rgb_to_grayscale(rgb_image_tensor) show_image_tensor(gray_image_tensor) å¯ä»¥ä»é¢œè‰²ç©ºé—´ä¸­æå–å›¾åƒçš„è½®å»“ä¿¡æ¯(å›¾åƒçš„äºŒå€¼åŒ–) a = gray_image_tensor b = tf.less_equal(a,0.4) # 0æ˜¯é»‘ï¼Œ1æ˜¯ç™½ # condition?true:false # conditionã€xã€yæ ¼å¼å¿…é¡»ä¸€æ¨¡ä¸€æ ·ï¼Œå½“conditionä¸­çš„å€¼ä¸ºtrueçš„ä¹‹åï¼Œè¿”å›xå¯¹åº”ä½ç½®çš„å€¼ï¼Œå¦åˆ™è¿”å›yå¯¹åº”ä½ç½®çš„å€¼ # å¯¹äºaä¸­æ‰€æœ‰å¤§äº0.4çš„åƒç´ å€¼ï¼Œè®¾ç½®ä¸º0 c = tf.where(condition=b,x=a,y=a-a) # å¯¹äºaä¸­æ‰€æœ‰å°äºç­‰äº0.4çš„åƒç´ å€¼ï¼Œè®¾ç½®ä¸º1 d= tf.where(condition=b,x=c-c+1,y=c) show_image_tensor(d) è¿™æ ·çš„æ–¹æ³•ï¼Œå¯ä»¥è¿ç”¨åˆ°è½¦ç‰Œè®¾åˆ«çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹è½¦ç‰Œè‡ªåŠ¨è¿›è¡Œæˆªå–ã€‚ å›¾åƒè°ƒæ•´ï¼ˆäº®åº¦è°ƒæ•´ï¼Œå¯¹æ¯”åº¦è°ƒæ•´ï¼Œgammerè°ƒæ•´ï¼Œå½’ä¸€åŒ–æ“ä½œï¼‰ äº®åº¦è°ƒæ•´ image: RGBå›¾åƒä¿¡æ¯ï¼Œè®¾ç½®ä¸ºfloatç±»å‹å’Œunit8ç±»å‹çš„æ•ˆæœä¸ä¸€æ ·ï¼Œä¸€èˆ¬å»ºè®®è®¾ç½®ä¸ºfloatç±»å‹ delta: å–å€¼èŒƒå›´(-1,1ï¼‰ä¹‹é—´çš„floatç±»å‹çš„å€¼ï¼Œè¡¨ç¤ºå¯¹äºäº®åº¦çš„å‡å¼±æˆ–è€…å¢å¼ºçš„ç³»æ•°å€¼ åº•å±‚æ‰§è¡Œï¼šrgb -> hsv -> h,s,v*delta -> rgb åŒç†è¿˜æœ‰è‰²è°ƒå’Œé¥±å’Œåº¦ adiust_brightness_image_tensor = tf.image.adjust_brightness(image=image_tensor, delta=-0.8) # show_image_tensor(adiust_brightness_image_tensor) # è‰²è°ƒè°ƒæ•´ # image: RGBå›¾åƒä¿¡æ¯ï¼Œè®¾ç½®ä¸ºfloatç±»å‹å’Œunit8ç±»å‹çš„æ•ˆæœä¸ä¸€æ ·ï¼Œä¸€èˆ¬å»ºè®®è®¾ç½®ä¸ºfloatç±»å‹ # delta: å–å€¼èŒƒå›´(-1,1ï¼‰ä¹‹é—´çš„floatç±»å‹çš„å€¼ï¼Œè¡¨ç¤ºå¯¹äºè‰²è°ƒçš„å‡å¼±æˆ–è€…å¢å¼ºçš„ç³»æ•°å€¼ # åº•å±‚æ‰§è¡Œï¼šrgb -> hsv -> h*delta,s,v -> rgb adjust_hue_image_tensor = tf.image.adjust_hue(image_tensor, delta=-0.8) # show_image_tensor(adjust_hue_image_tensor) # é¥±å’Œåº¦è°ƒæ•´ # image: RGBå›¾åƒä¿¡æ¯ï¼Œè®¾ç½®ä¸ºfloatç±»å‹å’Œunit8ç±»å‹çš„æ•ˆæœä¸ä¸€æ ·ï¼Œä¸€èˆ¬å»ºè®®è®¾ç½®ä¸ºfloatç±»å‹ # saturation_factor: ä¸€ä¸ªfloatç±»å‹çš„å€¼ï¼Œè¡¨ç¤ºå¯¹äºé¥±å’Œåº¦çš„å‡å¼±æˆ–è€…å¢å¼ºçš„ç³»æ•°å€¼ï¼Œé¥±å’Œå› å­ # åº•å±‚æ‰§è¡Œï¼šrgb -> hsv -> h,s*saturation_factor,v -> rgb adjust_saturation_image_tensor = tf.image.adjust_saturation(image_tensor, saturation_factor=20) show_image_tensor(adjust_saturation_image_tensor) # å¯¹æ¯”åº¦è°ƒæ•´ï¼Œå…¬å¼ï¼š(x-mean) * contrast_factor + mean(å°çš„æ›´å°ï¼Œå¤§çš„æ›´å¤§ï¼‰ adiust_contrast_image_tensor=tf.image.adjust_contrast(images=image_tensor, contrast_factor=1000) show_image_tensor(adiust_contrast_image_tensor) # å›¾åƒçš„gammaæ ¡æ­£ # images: è¦æ±‚å¿…é¡»æ˜¯floatç±»å‹çš„æ•°æ® # gammaï¼šä»»æ„å€¼ï¼ŒOup = In * Gammaåªè¦ä¸æ˜¯ç™½è‰²ï¼Œéƒ½åŠ æ·± adjust_gamma_image_tensor = tf.image.adjust_gamma(float32_image_tensor, gamma=10) # show_image_tensor(adjust_gamma_image_tensor) # å›¾åƒçš„å½’ä¸€åŒ–(x-mean)/adjusted_sttdev, adjusted_sttdev=max(stddev, 1.0/sqrt(image.NumElements())) # per_image_standardization_image_tensor = tf.image.per_image_standardization(image_tensor) # show_image_tensor(per_image_standardization_image_tensor) å™ªéŸ³æ•°æ®çš„åŠ å…¥ é«˜æ–¯å™ªå£°ã€æ¨¡ç³Šå¤„ç† # noisy_image_tensor = image_tensor + tf.cast(50 * tf.random_normal(shape=[512, 512, 3], mean=0, stddev=0.1), tf.uint8) noisy_image_tensor = image_tensor + tf.cast( tf.random_uniform(shape=[512, 512, 3], minval=60, maxval=70), tf.uint8) show_image_tensor(noisy_image_tensor) "},"data/picture_enhance.html":{"url":"data/picture_enhance.html","title":"å›¾åƒæ•°æ®å¢å¼º","keywords":"","body":"å¯¼è¯» å›¾åƒå¢å¼ºæ˜¯CVé¢†åŸŸéå¸¸å¸¸ç”¨çš„æŠ€æœ¯ï¼Œè¿™é‡Œæ‰¾åˆ°ä¸€ä¸ªéå¸¸å¥½ç”¨çš„å›¾åƒå¢å¼ºçš„å·¥å…·ï¼Œå¯ä»¥ç”¨äºPytorchå’ŒKerasï¼Œè€Œä¸”åŠŸèƒ½å¼ºå¤§ï¼Œä½¿ç”¨ç®€å•ï¼Œæ›´é‡è¦çš„æ˜¯å¯ä»¥æˆå¯¹çš„è¿›è¡Œå›¾åƒå¢å¼ºï¼Œç®€ç›´æ˜¯å®æˆ˜åˆ©å™¨ï¼Œæœ‰äº†è¿™ä¸ªï¼Œå¦ˆå¦ˆå†ä¹Ÿä¸ç”¨æ‹…å¿ƒæˆ‘çš„æ•°æ®ä¸å¤Ÿäº†ã€‚ Augmentoræ˜¯ä¸€ä¸ªPythonçš„å›¾åƒå¢å¼ºåº“ã€‚è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åº“ï¼Œä¸ä¾èµ–ä¸æŸä¸ªå¹³å°æˆ–æŸä¸ªæ¡†æ¶ï¼Œéå¸¸çš„æ–¹ä¾¿ï¼Œå¯ä»¥è¿›è¡Œç»†ç²’åº¦çš„å¢å¼ºæ§åˆ¶ï¼Œè€Œä¸”å®ç°äº†å¤§éƒ¨åˆ†çš„å¢å¼ºæŠ€æœ¯ã€‚ä½¿ç”¨äº†éšæœºçš„æ–¹æ³•æ¥æ„å»ºåŸºç¡€çš„æ¨¡å—ï¼Œç”¨æˆ·å¯ä»¥æŠŠè¿™äº›æ¨¡å—ç»„æˆpiplineä½¿ç”¨ã€‚ å®‰è£… Augmentoræ˜¯Pythonå†™çš„ã€‚è¿˜æœ‰ä¸€ä¸ªJuliaçš„ç‰ˆæœ¬ï¼Œé“¾æ¥ï¼šhttps://github.com/Evizero/Augmentor.jl ä½¿ç”¨pipå®‰è£…ï¼š pip install Augmentor ä»æºç å®‰è£…çš„è¯ï¼Œè¯·çœ‹ç¼–è¯‘æ–‡æ¡£ã€‚å‡çº§ç‰ˆæœ¬çš„è¯ï¼š pip install Augmentor --upgrade æ–‡æ¡£ å®Œæ•´çš„æ–‡æ¡£é“¾æ¥: http://augmentor.readthedocs.io å¿«é€ŸæŒ‡å—å’Œä½¿ç”¨ Augmentorçš„ç›®çš„æ˜¯è¿›è¡Œè‡ªåŠ¨çš„å›¾åƒå¢å¼ºï¼ˆç”Ÿæˆäººé€ æ•°æ®ï¼‰ä¸ºäº†æ‰©å±•æ•°æ®é›†ä½œä¸ºæœºå™¨å­¦ä¹ ç®—æ³•çš„è¾“å…¥ï¼Œç‰¹åˆ«æ˜¯ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ ã€‚ è¿™ä¸ªåŒ…é€šè¿‡åˆ›å»ºä¸€ä¸ªå¢å¼ºçš„pipelineï¼Œå³å®šä¹‰ä¸€ç³»åˆ—çš„æ“ä½œã€‚è¿™äº›æ“ä½œæœ‰æ¯”å¦‚æ—‹è½¬å’Œå˜æ¢ï¼Œä¸€ä¸ªåŠ ä¸€ä¸ªæˆä¸ºä¸€ä¸ªå¢å¼ºçš„pipelineï¼Œå½“å®Œæˆçš„æ—¶å€™ï¼Œpipelineå¯ä»¥æ‰§è¡Œï¼Œå¢å¼ºä¹‹åçš„æ•°æ®ä¹Ÿåˆ›å»ºæˆåŠŸã€‚ å¼€å§‹æ—¶ï¼Œéœ€è¦åˆå§‹åŒ–pipelineå¯¹è±¡ï¼ŒæŒ‡å‘ä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚ import Augmentor p = Augmentor.Pipeline(\"/path/to/images\") ç„¶åå¯ä»¥åœ¨pipelineå¯¹è±¡ä¸­æ·»åŠ æ“ä½œï¼š p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10) p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5) æ¯ä¸ªå‡½æ•°éœ€è¦åˆ¶å®šä¸€ä¸ªæ¦‚ç‡ï¼Œç”¨æ¥å†³å®šæ˜¯å¦éœ€è¦å¯¹è¿™ä¸ªå›¾åƒè¿›è¡Œè¿™ä¸ªæ“ä½œã€‚ ä¸€æ—¦ä½ åˆ›å»ºäº†pipelineï¼Œå¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·ï¼Œå°±åƒè¿™æ ·ï¼š p.sample(10000) è¿™æ ·ä¼šäº§ç”Ÿ10000ä¸ªå¢å¼ºä¹‹åçš„å›¾åƒã€‚é»˜è®¤ä¼šå†™åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„åä¸ºoutputçš„ç›®å½•ä¸­ï¼Œè¿™ä¸ªæŒ‡å®šæ–‡ä»¶å¤¹å°±æ˜¯åˆå§‹åŒ–æ—¶æŒ‡å®šçš„é‚£ä¸ªã€‚ å¦‚æœä½ æƒ³è¿›è¡Œä¸€æ¬¡å›¾åƒçš„å¢å¼ºæ“ä½œï¼Œå¯ä»¥ä½¿ç”¨process(): p.process() è¿™ä¸ªå‡½æ•°åœ¨è¿›è¡Œæ•°æ®é›†ç¼©æ”¾çš„æ—¶å€™ä¼šæœ‰ç”¨ã€‚å¯ä»¥åˆ›å»ºä¸€ä¸ªpipelineï¼Œå…¶ä¸­æ‰€æœ‰çš„æ“ä½œçš„æ¦‚ç‡éƒ½è®¾ç½®ä¸º1ï¼Œç„¶åä½¿ç”¨process()æ–¹æ³•ã€‚ å¤šçº¿ç¨‹ Augmentor (version >=0.2.1) ç°åœ¨ä½¿ç”¨å¤šçº¿ç¨‹æŠ€æœ¯æ¥æé«˜é€Ÿåº¦ã€‚ å¯¹äºåŸå§‹å›¾åƒéå¸¸å°çš„å›¾åƒæ¥è¯´ï¼ŒæŸäº›pipelineå¯èƒ½ä¼šå˜æ…¢ã€‚å¦‚æœå‘ç°è¿™ç§æƒ…å†µï¼Œå¯ä»¥è®¾ç½®multi_threadedä¸ºFalseã€‚ p.sample(100, multi_threaded=False) é»˜è®¤çš„æƒ…å†µä¸‹ï¼Œsample()å‡½æ•°æ˜¯ä½¿ç”¨å¤šçº¿ç¨‹çš„ã€‚è¿™ä¸ªåªåœ¨ä¿å­˜åˆ°ç£ç›˜çš„æ—¶å€™å®ç°ã€‚ç”Ÿæˆå™¨ä¹Ÿä¼šåœ¨ä¸‹ä¸ªç‰ˆæœ¬ä½¿ç”¨å¤šçº¿ç¨‹ã€‚ Ground Truthæ•°æ® å›¾åƒå¯ä»¥ä¸¤ä¸ªä¸€ç»„çš„é€šè¿‡pipelineï¼Œæ‰€ä»¥ground truthçš„å›¾åƒå¯ä»¥åŒç­‰çš„è¿›è¡Œå¢å¼ºã€‚ ä¸ºäº†å¹¶è¡Œçš„å¯¹åŸå§‹æ•°æ®è¿›è¡Œground truthçš„å¢å¼ºï¼Œå¯ä»¥ä½¿ç”¨ground_truth()æ–¹æ³•å¢åŠ ä¸€ä¸ªground truthçš„æ–‡ä»¶å¤¹åˆ°pipelineä¸­ï¼š p = Augmentor.Pipeline(\"/path/to/images\") # Point to a directory containing ground truth data. # Images with the same file names will be added as ground truth data # and augmented in parallel to the original data. p.ground_truth(\"/path/to/ground_truth_images\") # Add operations to the pipeline as normal: p.rotate(probability=1, max_left_rotation=5, max_right_rotation=5) p.flip_left_right(probability=0.5) p.zoom_random(probability=0.5, percentage_area=0.8) p.flip_top_bottom(probability=0.5) p.sample(50) å¤šæ©æ¨¡/å›¾åƒå¢å¼º ä½¿ç”¨DataPipelineç±» (Augmentor version >= 0.2.3)ï¼Œå¯ä»¥å¯¹æœ‰å¤šä¸ªç›¸å…³çš„æ©æ¨¡çš„å›¾åƒè¿›è¡Œå¢å¼ºï¼š ä»»æ„é•¿åº¦çš„å›¾åƒåˆ—è¡¨éƒ½å¯ä»¥æˆç»„çš„é€šè¿‡pipelineï¼Œå¹¶ä¸”ä½¿ç”¨DataPipelineç±»åŒæ ·çš„è¿›è¡Œå¢å¼ºã€‚è¿™ä¸ªå¯¹äºground truthå›¾åƒæœ‰å¥½å‡ ä¸ªæ©æ¨¡çš„æ—¶å€™éå¸¸æœ‰ç”¨ã€‚ä¸¾ä¸ªä¾‹å­ã€‚ ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå›¾åƒå’Œæ©æ¨¡åŒ…å«åœ¨ä¸€ä¸ªimagesçš„æ•°æ®ç»“æ„ä¸­ï¼Œå¯¹åº”çš„æ ‡ç­¾åœ¨yä¸­ï¼š p = Augmentor.DataPipeline(images, y) p.rotate(1, max_left_rotation=5, max_right_rotation=5) p.flip_top_bottom(0.5) p.zoom_random(1, percentage_area=0.5) augmented_images, labels = p.sample(100) DataPipelineç›´æ¥è¿”å›å›¾åƒï¼Œå¹¶ä¸å­˜å‚¨åœ¨ç£ç›˜ä¸­ï¼Œä¹Ÿä¸ä»ç£ç›˜ä¸­è¯»å–æ•°æ®ã€‚å›¾åƒé€šè¿‡åˆå§‹åŒ–ç›´æ¥ä¼ åˆ°DataPipelineä¸­ã€‚imagesçš„æ•°æ®ç»“æ„çš„åˆ›å»ºç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒhttps://github.com/mdbloice/Augmentor/blob/master/notebooks/Multiple-Mask-Augmentation.ipynbã€‚ Keraså’ŒPytorchçš„ç”Ÿæˆå™¨ å¦‚æœä½ ä¸æƒ³å°†å›¾åƒå­˜å‚¨åˆ°ç¡¬ç›˜ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ç”Ÿæˆå™¨ï¼Œgeneratorï¼Œä½¿ç”¨Kerasçš„æƒ…å†µï¼š g = p.keras_generator(batch_size=128) images, labels = next(g) è¿”å›çš„å›¾åƒçš„batchsizeæ˜¯128ï¼Œè¿˜æœ‰å¯¹åº”çš„labelsã€‚Generatorè¿”å›çš„æ•°æ®æ˜¯ä¸ç¡®å®šçš„ï¼Œå¯ä»¥ç”¨æ¥åœ¨çº¿ç”Ÿæˆå¢å¼ºçš„æ•°æ®ï¼Œç”¨åœ¨è®­ç»ƒç¥ç»ç½‘ç»œä¸­ã€‚ åŒæ ·çš„ï¼Œä½ å¯ä»¥ä½¿ç”¨Pytorchï¼š import torchvision transforms = torchvision.transforms.Compose([ p.torch_transform(), torchvision.transforms.ToTensor(), ]) ä¸»è¦åŠŸèƒ½ å¼¹æ€§ç•¸å˜ ä½¿ç”¨å¼¹æ€§ç•¸å˜ï¼Œä¸€å¼ å›¾åƒå¯ä»¥ç”Ÿæˆè®¸å¤šå›¾åƒã€‚ è¿™ä¸ªè¾“å…¥å›¾åƒæœ‰ä¸€ä¸ªåƒç´ å®½çš„é»‘è¾¹ï¼Œè¡¨æ˜äº†åœ¨è¿›è¡Œç•¸å˜çš„æ—¶å€™ï¼Œæ²¡æœ‰æ”¹å˜å°ºå¯¸ï¼Œä¹Ÿæ²¡æœ‰åœ¨æ–°çš„å›¾åƒä¸Šè¿›è¡Œä»»ä½•çš„paddingã€‚ å…·ä½“çš„åŠŸèƒ½å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°: é€è§†å˜æ¢ æ€»å…±æœ‰12ä¸ªä¸åŒç±»å‹çš„é€è§†å˜æ¢ã€‚4ä¸­æœ€å¸¸ç”¨çš„å¦‚ä¸‹ï¼š å‰©ä¸‹çš„8ç§é€è§†å˜æ¢: ä¿æŒå¤§å°çš„æ—‹è½¬ é»˜è®¤ä¿æŒåŸå§‹æ–‡ä»¶å¤§å°çš„æ—‹è½¬ï¼š å¯¹æ¯”å…¶ä»–è½¯ä»¶çš„æ—‹è½¬: ä¿æŒå¤§å°çš„å‰ªåˆ‡ å‰ªåˆ‡çš„åŒæ—¶ä¹Ÿä¼šè‡ªåŠ¨ä»å‰ªåˆ‡å›¾åƒä¸­è£å‰ªæ­£ç¡®çš„åŒºåŸŸï¼Œæ‰€ä»¥å›¾åƒä¸­æ²¡æœ‰é»‘çš„åŒºåŸŸæˆ–è€…paddingã€‚ å¯¹æ¯”æ™®é€šçš„å‰ªåˆ‡æ“ä½œï¼š è£å‰ª è£å‰ªåŒæ ·ä¹Ÿä½¿ç”¨äº†ä¸€ç§æ›´åŠ é€‚åˆæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼š éšæœºæ“¦é™¤ éšæœºæ“¦é™¤æ˜¯ä¸€ç§ä½¿æ¨¡å‹å¯¹é®æŒ¡æ›´åŠ é²æ£’çš„æŠ€æœ¯ã€‚è¿™ä¸ªå¯¹ä½¿ç”¨ç¥ç»ç½‘ç»œè®­ç»ƒç‰©ä½“æ£€æµ‹çš„æ—¶å€™éå¸¸æœ‰ç”¨ï¼š çœ‹ Pipeline.random_erasing() æ–‡æ¡£äº†è§£æ›´å¤šçš„ç”¨æ³•ã€‚ æŠŠæ“ä½œä¸²æˆPipeline ä½¿ç”¨å‡ ä¸ªæ“ä½œï¼Œå•ä¸ªå›¾åƒå¯ä»¥å¢å¼ºæˆè®¸å¤šçš„æ–°å›¾åƒï¼Œå¯¹åº”åŒæ ·çš„labelï¼š åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†3ä¸ªæ“ä½œï¼šé¦–å…ˆåšäº†ç•¸å˜æ“ä½œï¼Œç„¶åè¿›è¡Œäº†å·¦å³çš„é•œåƒï¼Œæ¦‚ç‡ä¸º0.5ï¼Œæœ€åä»¥0.5çš„æ¦‚ç‡åšäº†ä¸Šä¸‹çš„ç¿»è½¬ã€‚ç„¶åä»è¿™ä¸ªpipelineä¸­é‡‡æ ·äº†100æ¬¡ï¼Œå¾—åˆ°äº†100ä¸ªæ•°æ®ã€‚ p.random_distortion(probability=1, grid_width=4, grid_height=4, magnitude=8) p.flip_left_right(probability=0.5) p.flip_top_bottom(probability=0.5) p.sample(100) æŒ‡å— ä½¿ç”¨ç”Ÿæˆå™¨å’ŒKerasé›†æˆ Augmentor å¯ä»¥ç”¨æ¥æ›¿æ¢Kerasä¸­çš„augmentationåŠŸèƒ½ã€‚Augmentor å¯ä»¥åˆ›å»ºä¸€ä¸ªç”Ÿäº§å™¨æ¥äº§ç”Ÿå¢å¼ºåçš„å›¾åƒï¼Œç»†èŠ‚å¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„notebookï¼š ä»æœ¬åœ°æ–‡ä»¶å¤¹ä¸­è¯»å–å›¾åƒè¿›è¡Œå¢å¼ºï¼Œç„¶åä½¿ç”¨ç”Ÿæˆå™¨å°†å¢å¼ºçš„å›¾åƒæµé€åˆ°å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œå‚è§ https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras.ipynb å¢å¼ºå†…å­˜ä¸­çš„å›¾åƒï¼Œä½¿ç”¨ç”Ÿæˆå™¨å°†æ–°çš„å›¾åƒé€åˆ°Kerasçš„ç½‘ç»œä¸­ï¼Œå‚è§ https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras_Array_Data.ipynb Augmentor å…è®¸æ¯ä¸ªç±»å®šä¹‰ä¸åŒçš„pipelinesï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨åˆ†ç±»é—®é¢˜ä¸­ä¸ºä¸åŒçš„ç±»åˆ«å®šä¹‰ä¸åŒçš„å¢å¼ºç­–ç•¥ã€‚ ä¾‹å­åœ¨è¿™é‡Œï¼šhttps://github.com/mdbloice/Augmentor/blob/master/notebooks/Per_Class_Augmentation_Strategy.ipynb å®Œæ•´çš„ä¾‹å­ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€å¼ å›¾åƒæ¥å®Œæˆä¸€ä¸ªå¢å¼ºçš„ä»»åŠ¡ï¼Œæ¼”ç¤ºä¸€ä¸‹Augmentorçš„pipelineå’Œä¸€äº›åŠŸèƒ½ã€‚ é¦–å…ˆï¼Œå¯¼å…¥åŒ…ï¼Œåˆå§‹åŒ–Pipelineå¯¹è±¡ï¼ŒæŒ‡å®šä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹é‡Œæ”¾ç€ä½ çš„å›¾åƒã€‚ import Augmentor p = Augmentor.Pipeline(\"/home/user/augmentor_data_tests\") ç„¶åä½ å¯ä»¥åœ¨pipelineä¸­æ·»åŠ å„ç§æ“ä½œï¼š p.rotate90(probability=0.5) p.rotate270(probability=0.5) p.flip_left_right(probability=0.8) p.flip_top_bottom(probability=0.3) p.crop_random(probability=1, percentage_area=0.5) p.resize(probability=1.0, width=120, height=120) æ“ä½œæ·»åŠ å®Œäº†ä¹‹åï¼Œå¯ä»¥è¿›è¡Œé‡‡æ ·ï¼š p.sample(100) å…¶ä¸­çš„å‡ ä¸ª å¢å¼ºçš„å›¾åƒå¯¹è¾¹ç¼˜æ£€æµ‹ä»»åŠ¡ä¹Ÿè®¸å¾ˆæœ‰ç”¨ "},"data/data.html":{"url":"data/data.html","title":"æ•°æ®å¢å¼º","keywords":"","body":"å¾ˆå¤šå®é™…çš„é¡¹ç›®ï¼Œæˆ‘ä»¬éƒ½éš¾ä»¥æœ‰å……è¶³çš„æ•°æ®æ¥å®Œæˆä»»åŠ¡ï¼Œè¦ä¿è¯å®Œç¾çš„å®Œæˆä»»åŠ¡ï¼Œæœ‰ä¸¤ä»¶äº‹æƒ…éœ€è¦åšå¥½ï¼š(1)å¯»æ‰¾æ›´å¤šçš„æ•°æ®ã€‚(2)å……åˆ†åˆ©ç”¨å·²æœ‰çš„æ•°æ®è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»Šå¤©å°±æ¥è¯´è¯´æ•°æ®å¢å¼ºã€‚ ä½œè€… | è¨€æœ‰ä¸‰ ç¼–è¾‘ | è¨€æœ‰ä¸‰ 1 ä»€ä¹ˆæ˜¯æ•°æ®å¢å¼ºï¼Ÿ æ•°æ®å¢å¼ºä¹Ÿå«æ•°æ®æ‰©å¢ï¼Œæ„æ€æ˜¯åœ¨ä¸å®è´¨æ€§çš„å¢åŠ æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè®©æœ‰é™çš„æ•°æ®äº§ç”Ÿç­‰ä»·äºæ›´å¤šæ•°æ®çš„ä»·å€¼ã€‚ æ¯”å¦‚ä¸Šå›¾ï¼Œç¬¬1åˆ—æ˜¯åŸå›¾ï¼Œåé¢3åˆ—æ˜¯å¯¹ç¬¬1åˆ—ä½œä¸€äº›éšæœºçš„è£å‰ªã€æ—‹è½¬æ“ä½œå¾—æ¥ã€‚ æ¯å¼ å›¾å¯¹äºç½‘ç»œæ¥è¯´éƒ½æ˜¯ä¸åŒçš„è¾“å…¥ï¼ŒåŠ ä¸ŠåŸå›¾å°±å°†æ•°æ®æ‰©å……åˆ°åŸæ¥çš„10å€ã€‚å‡å¦‚æˆ‘ä»¬è¾“å…¥ç½‘ç»œçš„å›¾ç‰‡çš„åˆ†è¾¨ç‡å¤§å°æ˜¯256Ã—256ï¼Œè‹¥é‡‡ç”¨éšæœºè£å‰ªæˆ224Ã—224çš„æ–¹å¼ï¼Œé‚£ä¹ˆä¸€å¼ å›¾æœ€å¤šå¯ä»¥äº§ç”Ÿ32Ã—32å¼ ä¸åŒçš„å›¾ï¼Œæ•°æ®é‡æ‰©å……å°†è¿‘1000å€ã€‚è™½ç„¶è®¸å¤šçš„å›¾ç›¸ä¼¼åº¦å¤ªé«˜ï¼Œå®é™…çš„æ•ˆæœå¹¶ä¸ç­‰ä»·ï¼Œä½†ä»…ä»…æ˜¯è¿™æ ·ç®€å•çš„ä¸€ä¸ªæ“ä½œï¼Œæ•ˆæœå·²ç»éå‡¡äº†ã€‚ å¦‚æœå†è¾…åŠ©å…¶ä»–çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå°†è·å¾—æ›´å¥½çš„å¤šæ ·æ€§ï¼Œè¿™å°±æ˜¯æ•°æ®å¢å¼ºçš„æœ¬è´¨ã€‚ æ•°æ®å¢å¼ºå¯ä»¥åˆ†ä¸ºï¼Œæœ‰ç›‘ç£çš„æ•°æ®å¢å¼ºå’Œæ— ç›‘ç£çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚å…¶ä¸­æœ‰ç›‘ç£çš„æ•°æ®å¢å¼ºåˆå¯ä»¥åˆ†ä¸ºå•æ ·æœ¬æ•°æ®å¢å¼ºå’Œå¤šæ ·æœ¬æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ— ç›‘ç£çš„æ•°æ®å¢å¼ºåˆ†ä¸ºç”Ÿæˆæ–°çš„æ•°æ®å’Œå­¦ä¹ å¢å¼ºç­–ç•¥ä¸¤ä¸ªæ–¹å‘ã€‚ 2 æœ‰ç›‘ç£çš„æ•°æ®å¢å¼º æœ‰ç›‘ç£æ•°æ®å¢å¼ºï¼Œå³é‡‡ç”¨é¢„è®¾çš„æ•°æ®å˜æ¢è§„åˆ™ï¼Œåœ¨å·²æœ‰æ•°æ®çš„åŸºç¡€ä¸Šè¿›è¡Œæ•°æ®çš„æ‰©å¢ï¼ŒåŒ…å«å•æ ·æœ¬æ•°æ®å¢å¼ºå’Œå¤šæ ·æœ¬æ•°æ®å¢å¼ºï¼Œå…¶ä¸­å•æ ·æœ¬åˆåŒ…æ‹¬å‡ ä½•æ“ä½œç±»ï¼Œé¢œè‰²å˜æ¢ç±»ã€‚ 2.1. å•æ ·æœ¬æ•°æ®å¢å¼º æ‰€è°“å•æ ·æœ¬æ•°æ®å¢å¼ºï¼Œå³å¢å¼ºä¸€ä¸ªæ ·æœ¬çš„æ—¶å€™ï¼Œå…¨éƒ¨å›´ç»•ç€è¯¥æ ·æœ¬æœ¬èº«è¿›è¡Œæ“ä½œï¼ŒåŒ…æ‹¬å‡ ä½•å˜æ¢ç±»ï¼Œé¢œè‰²å˜æ¢ç±»ç­‰ã€‚ (1) å‡ ä½•å˜æ¢ç±» å‡ ä½•å˜æ¢ç±»å³å¯¹å›¾åƒè¿›è¡Œå‡ ä½•å˜æ¢ï¼ŒåŒ…æ‹¬ç¿»è½¬ï¼Œæ—‹è½¬ï¼Œè£å‰ªï¼Œå˜å½¢ï¼Œç¼©æ”¾ç­‰å„ç±»æ“ä½œï¼Œä¸‹é¢å±•ç¤ºå…¶ä¸­çš„è‹¥å¹²ä¸ªæ“ä½œã€‚ æ°´å¹³ç¿»è½¬å’Œå‚ç›´ç¿»è½¬ éšæœºæ—‹è½¬ éšæœºè£å‰ª å˜å½¢ç¼©æ”¾ ç¿»è½¬æ“ä½œå’Œæ—‹è½¬æ“ä½œï¼Œå¯¹äºé‚£äº›å¯¹æ–¹å‘ä¸æ•æ„Ÿçš„ä»»åŠ¡ï¼Œæ¯”å¦‚å›¾åƒåˆ†ç±»ï¼Œéƒ½æ˜¯å¾ˆå¸¸è§çš„æ“ä½œï¼Œåœ¨caffeç­‰æ¡†æ¶ä¸­ç¿»è½¬å¯¹åº”çš„å°±æ˜¯mirroræ“ä½œã€‚ ç¿»è½¬å’Œæ—‹è½¬ä¸æ”¹å˜å›¾åƒçš„å¤§å°ï¼Œè€Œè£å‰ªä¼šæ”¹å˜å›¾åƒçš„å¤§å°ã€‚é€šå¸¸åœ¨è®­ç»ƒçš„æ—¶å€™ä¼šé‡‡ç”¨éšæœºè£å‰ªçš„æ–¹æ³•ï¼Œåœ¨æµ‹è¯•çš„æ—¶å€™é€‰æ‹©è£å‰ªä¸­é—´éƒ¨åˆ†æˆ–è€…ä¸è£å‰ªã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸€äº›ç«èµ›ä¸­è¿›è¡Œæ¨¡å‹æµ‹è¯•æ—¶ï¼Œä¸€èˆ¬éƒ½æ˜¯è£å‰ªè¾“å…¥çš„å¤šä¸ªç‰ˆæœ¬ç„¶åå°†ç»“æœè¿›è¡Œèåˆï¼Œå¯¹é¢„æµ‹çš„æ”¹è¿›æ•ˆæœéå¸¸æ˜æ˜¾ã€‚ ä»¥ä¸Šæ“ä½œéƒ½ä¸ä¼šäº§ç”Ÿå¤±çœŸï¼Œè€Œç¼©æ”¾å˜å½¢åˆ™æ˜¯å¤±çœŸçš„ã€‚ å¾ˆå¤šçš„æ—¶å€™ï¼Œç½‘ç»œçš„è®­ç»ƒè¾“å…¥å¤§å°æ˜¯å›ºå®šçš„ï¼Œä½†æ˜¯æ•°æ®é›†ä¸­çš„å›¾åƒå´å¤§å°ä¸ä¸€ï¼Œæ­¤æ—¶å°±å¯ä»¥é€‰æ‹©ä¸Šé¢çš„è£å‰ªæˆå›ºå®šå¤§å°è¾“å…¥æˆ–è€…ç¼©æ”¾åˆ°ç½‘ç»œçš„è¾“å…¥å¤§å°çš„æ–¹æ¡ˆï¼Œåè€…å°±ä¼šäº§ç”Ÿå¤±çœŸï¼Œé€šå¸¸æ•ˆæœæ¯”å‰è€…å·®ã€‚ (2) é¢œè‰²å˜æ¢ç±» ä¸Šé¢çš„å‡ ä½•å˜æ¢ç±»æ“ä½œï¼Œæ²¡æœ‰æ”¹å˜å›¾åƒæœ¬èº«çš„å†…å®¹ï¼Œå®ƒå¯èƒ½æ˜¯é€‰æ‹©äº†å›¾åƒçš„ä¸€éƒ¨åˆ†æˆ–è€…å¯¹åƒç´ è¿›è¡Œäº†é‡åˆ†å¸ƒã€‚å¦‚æœè¦æ”¹å˜å›¾åƒæœ¬èº«çš„å†…å®¹ï¼Œå°±å±äºé¢œè‰²å˜æ¢ç±»çš„æ•°æ®å¢å¼ºäº†ï¼Œå¸¸è§çš„åŒ…æ‹¬å™ªå£°ã€æ¨¡ç³Šã€é¢œè‰²å˜æ¢ã€æ“¦é™¤ã€å¡«å……ç­‰ç­‰ã€‚ åŸºäºå™ªå£°çš„æ•°æ®å¢å¼ºå°±æ˜¯åœ¨åŸæ¥çš„å›¾ç‰‡çš„åŸºç¡€ä¸Šï¼Œéšæœºå åŠ ä¸€äº›å™ªå£°ï¼Œæœ€å¸¸è§çš„åšæ³•å°±æ˜¯é«˜æ–¯å™ªå£°ã€‚æ›´å¤æ‚ä¸€ç‚¹çš„å°±æ˜¯åœ¨é¢ç§¯å¤§å°å¯é€‰å®šã€ä½ç½®éšæœºçš„çŸ©å½¢åŒºåŸŸä¸Šä¸¢å¼ƒåƒç´ äº§ç”Ÿé»‘è‰²çŸ©å½¢å—ï¼Œä»è€Œäº§ç”Ÿä¸€äº›å½©è‰²å™ªå£°ï¼Œä»¥Coarse Dropoutæ–¹æ³•ä¸ºä»£è¡¨ï¼Œç”šè‡³è¿˜å¯ä»¥å¯¹å›¾ç‰‡ä¸Šéšæœºé€‰å–ä¸€å—åŒºåŸŸå¹¶æ“¦é™¤å›¾åƒä¿¡æ¯ã€‚ æ·»åŠ Coarse Dropoutå™ªå£° é¢œè‰²å˜æ¢çš„å¦ä¸€ä¸ªé‡è¦å˜æ¢æ˜¯é¢œè‰²æ‰°åŠ¨ï¼Œå°±æ˜¯åœ¨æŸä¸€ä¸ªé¢œè‰²ç©ºé—´é€šè¿‡å¢åŠ æˆ–å‡å°‘æŸäº›é¢œè‰²åˆ†é‡ï¼Œæˆ–è€…æ›´æ”¹é¢œè‰²é€šé“çš„é¡ºåºã€‚ é¢œè‰²æ‰°åŠ¨ è¿˜æœ‰ä¸€äº›é¢œè‰²å˜æ¢ï¼Œæœ¬æ–‡å°±ä¸å†è¯¦è¿°ã€‚ å‡ ä½•å˜æ¢ç±»ï¼Œé¢œè‰²å˜æ¢ç±»çš„æ•°æ®å¢å¼ºæ–¹æ³•ç»†è‡´æ•°æ¥è¿˜æœ‰éå¸¸å¤šï¼Œæ¨èç»™å¤§å®¶ä¸€ä¸ªgité¡¹ç›®ï¼š https://github.com/aleju/imgaug é¢„è§ˆä¸€ä¸‹å®ƒèƒ½å®Œæˆçš„æ•°æ®å¢å¼ºæ“ä½œå§ã€‚ 2.2. å¤šæ ·æœ¬æ•°æ®å¢å¼º ä¸åŒäºå•æ ·æœ¬æ•°æ®å¢å¼ºï¼Œå¤šæ ·æœ¬æ•°æ®å¢å¼ºæ–¹æ³•åˆ©ç”¨å¤šä¸ªæ ·æœ¬æ¥äº§ç”Ÿæ–°çš„æ ·æœ¬ï¼Œä¸‹é¢ä»‹ç»å‡ ç§æ–¹æ³•ã€‚ (1) SMOTE[1] SMOTEå³Synthetic Minority Over-sampling Techniqueæ–¹æ³•ï¼Œå®ƒæ˜¯é€šè¿‡äººå·¥åˆæˆæ–°æ ·æœ¬æ¥å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ï¼Œä»è€Œæå‡åˆ†ç±»å™¨æ€§èƒ½ã€‚ ç±»ä¸å¹³è¡¡ç°è±¡æ˜¯å¾ˆå¸¸è§çš„ï¼Œå®ƒæŒ‡çš„æ˜¯æ•°æ®é›†ä¸­å„ç±»åˆ«æ•°é‡ä¸è¿‘ä¼¼ç›¸ç­‰ã€‚å¦‚æœæ ·æœ¬ç±»åˆ«ä¹‹é—´ç›¸å·®å¾ˆå¤§ï¼Œä¼šå½±å“åˆ†ç±»å™¨çš„åˆ†ç±»æ•ˆæœã€‚å‡è®¾å°æ ·æœ¬æ•°æ®æ•°é‡æå°‘ï¼Œå¦‚ä»…å æ€»ä½“çš„1%ï¼Œåˆ™å³ä½¿å°æ ·æœ¬è¢«é”™è¯¯åœ°å…¨éƒ¨è¯†åˆ«ä¸ºå¤§æ ·æœ¬ï¼Œåœ¨ç»éªŒé£é™©æœ€å°åŒ–ç­–ç•¥ä¸‹çš„åˆ†ç±»å™¨è¯†åˆ«å‡†ç¡®ç‡ä»èƒ½è¾¾åˆ°99%ï¼Œä½†ç”±äºæ²¡æœ‰å­¦ä¹ åˆ°å°æ ·æœ¬çš„ç‰¹å¾ï¼Œå®é™…åˆ†ç±»æ•ˆæœå°±ä¼šå¾ˆå·®ã€‚ SMOTEæ–¹æ³•æ˜¯åŸºäºæ’å€¼çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä¸ºå°æ ·æœ¬ç±»åˆæˆæ–°çš„æ ·æœ¬ï¼Œä¸»è¦æµç¨‹ä¸ºï¼š ç¬¬ä¸€æ­¥ï¼Œå®šä¹‰å¥½ç‰¹å¾ç©ºé—´ï¼Œå°†æ¯ä¸ªæ ·æœ¬å¯¹åº”åˆ°ç‰¹å¾ç©ºé—´ä¸­çš„æŸä¸€ç‚¹ï¼Œæ ¹æ®æ ·æœ¬ä¸å¹³è¡¡æ¯”ä¾‹ç¡®å®šå¥½ä¸€ä¸ªé‡‡æ ·å€ç‡Nï¼› ç¬¬äºŒæ­¥ï¼Œå¯¹æ¯ä¸€ä¸ªå°æ ·æœ¬ç±»æ ·æœ¬(x,y)ï¼ŒæŒ‰æ¬§æ°è·ç¦»æ‰¾å‡ºKä¸ªæœ€è¿‘é‚»æ ·æœ¬ï¼Œä»ä¸­éšæœºé€‰å–ä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå‡è®¾é€‰æ‹©çš„è¿‘é‚»ç‚¹ä¸º(xn,yn)ã€‚åœ¨ç‰¹å¾ç©ºé—´ä¸­æ ·æœ¬ç‚¹ä¸æœ€è¿‘é‚»æ ·æœ¬ç‚¹çš„è¿çº¿æ®µä¸Šéšæœºé€‰å–ä¸€ç‚¹ä½œä¸ºæ–°æ ·æœ¬ç‚¹ï¼Œæ»¡è¶³ä»¥ä¸‹å…¬å¼ï¼š ç¬¬ä¸‰æ­¥ï¼Œé‡å¤ä»¥ä¸Šçš„æ­¥éª¤ï¼Œç›´åˆ°å¤§ã€å°æ ·æœ¬æ•°é‡å¹³è¡¡ã€‚ è¯¥æ–¹æ³•çš„ç¤ºæ„å›¾å¦‚ä¸‹ã€‚ åœ¨pythonä¸­ï¼ŒSMOTEç®—æ³•å·²ç»å°è£…åˆ°äº†imbalanced-learnåº“ä¸­ï¼Œå¦‚ä¸‹å›¾ä¸ºç®—æ³•å®ç°çš„æ•°æ®å¢å¼ºçš„å®ä¾‹ï¼Œå·¦å›¾ä¸ºåŸå§‹æ•°æ®ç‰¹å¾ç©ºé—´å›¾ï¼Œå³å›¾ä¸ºSMOTEç®—æ³•å¤„ç†åçš„ç‰¹å¾ç©ºé—´å›¾ã€‚ (2) SamplePairing[2] SamplePairingæ–¹æ³•çš„åŸç†éå¸¸ç®€å•ï¼Œä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«ç»è¿‡åŸºç¡€æ•°æ®å¢å¼ºæ“ä½œ(å¦‚éšæœºç¿»è½¬ç­‰)å¤„ç†åç»åƒç´ ä»¥å–å¹³å‡å€¼çš„å½¢å¼å åŠ åˆæˆä¸€ä¸ªæ–°çš„æ ·æœ¬ï¼Œæ ‡ç­¾ä¸ºåŸæ ·æœ¬æ ‡ç­¾ä¸­çš„ä¸€ç§ã€‚è¿™ä¸¤å¼ å›¾ç‰‡ç”šè‡³ä¸é™åˆ¶ä¸ºåŒä¸€ç±»åˆ«ï¼Œè¿™ç§æ–¹æ³•å¯¹äºåŒ»å­¦å›¾åƒæ¯”è¾ƒæœ‰æ•ˆã€‚ ç»SamplePairingå¤„ç†åå¯ä½¿è®­ç»ƒé›†çš„è§„æ¨¡ä»Næ‰©å¢åˆ°NÃ—Nã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå› SamplePairingæ•°æ®å¢å¼ºæ“ä½œå¯èƒ½å¼•å…¥ä¸åŒæ ‡ç­¾çš„è®­ç»ƒæ ·æœ¬ï¼Œå¯¼è‡´åœ¨å„æ•°æ®é›†ä¸Šä½¿ç”¨SamplePairingè®­ç»ƒçš„è¯¯å·®æ˜æ˜¾å¢åŠ ï¼Œè€Œåœ¨éªŒè¯é›†ä¸Šè¯¯å·®åˆ™æœ‰è¾ƒå¤§å¹…åº¦é™ä½ã€‚ å°½ç®¡SamplePairingæ€è·¯ç®€å•ï¼Œæ€§èƒ½ä¸Šæå‡æ•ˆæœå¯è§‚ï¼Œç¬¦åˆå¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼Œä½†é—æ†¾çš„æ˜¯å¯è§£é‡Šæ€§ä¸å¼ºã€‚ (3) mixup[3] mixupæ˜¯Facebookäººå·¥æ™ºèƒ½ç ”ç©¶é™¢å’ŒMITåœ¨â€œBeyond Empirical Risk Minimizationâ€ä¸­æå‡ºçš„åŸºäºé‚»åŸŸé£é™©æœ€å°åŒ–åŸåˆ™çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå®ƒä½¿ç”¨çº¿æ€§æ’å€¼å¾—åˆ°æ–°æ ·æœ¬æ•°æ®ã€‚ ä»¤(xn,yn)æ˜¯æ’å€¼ç”Ÿæˆçš„æ–°æ•°æ®ï¼Œ(xi,yi)å’Œ(xj,yj)æ˜¯è®­ç»ƒé›†éšæœºé€‰å–çš„ä¸¤ä¸ªæ•°æ®ï¼Œåˆ™æ•°æ®ç”Ÿæˆæ–¹å¼å¦‚ä¸‹ Î»çš„å–æŒ‡èŒƒå›´ä»‹äº0åˆ°1ã€‚æå‡ºmixupæ–¹æ³•çš„ä½œè€…ä»¬åšäº†ä¸°å¯Œçš„å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜å¯ä»¥æ”¹è¿›æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ImageNetæ•°æ®é›†ã€CIFARæ•°æ®é›†ã€è¯­éŸ³æ•°æ®é›†å’Œè¡¨æ ¼æ•°æ®é›†ä¸­çš„æ³›åŒ–è¯¯å·®ï¼Œé™ä½æ¨¡å‹å¯¹å·²æŸåæ ‡ç­¾çš„è®°å¿†ï¼Œå¢å¼ºæ¨¡å‹å¯¹å¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§å’Œè®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ç¨³å®šæ€§ã€‚ SMOTEï¼ŒSamplePairingï¼Œmixupä¸‰è€…æ€è·¯ä¸Šæœ‰ç›¸åŒä¹‹å¤„ï¼Œéƒ½æ˜¯è¯•å›¾å°†ç¦»æ•£æ ·æœ¬ç‚¹è¿ç»­åŒ–æ¥æ‹ŸåˆçœŸå®æ ·æœ¬åˆ†å¸ƒï¼Œä¸è¿‡æ‰€å¢åŠ çš„æ ·æœ¬ç‚¹åœ¨ç‰¹å¾ç©ºé—´ä¸­ä»ä½äºå·²çŸ¥å°æ ·æœ¬ç‚¹æ‰€å›´æˆçš„åŒºåŸŸå†…ã€‚å¦‚æœèƒ½å¤Ÿåœ¨ç»™å®šèŒƒå›´ä¹‹å¤–é€‚å½“æ’å€¼ï¼Œä¹Ÿè®¸èƒ½å®ç°æ›´å¥½çš„æ•°æ®å¢å¼ºæ•ˆæœã€‚ 3 æ— ç›‘ç£çš„æ•°æ®å¢å¼º æ— ç›‘ç£çš„æ•°æ®å¢å¼ºæ–¹æ³•åŒ…æ‹¬ä¸¤ç±»ï¼š (1) é€šè¿‡æ¨¡å‹å­¦ä¹ æ•°æ®çš„åˆ†å¸ƒï¼Œéšæœºç”Ÿæˆä¸è®­ç»ƒæ•°æ®é›†åˆ†å¸ƒä¸€è‡´çš„å›¾ç‰‡ï¼Œä»£è¡¨æ–¹æ³•GAN[4]ã€‚ (2) é€šè¿‡æ¨¡å‹ï¼Œå­¦ä¹ å‡ºé€‚åˆå½“å‰ä»»åŠ¡çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»£è¡¨æ–¹æ³•AutoAugment[5]ã€‚ 3.1 GAN å…³äºGAN(generative adversarial networks)ï¼Œæˆ‘ä»¬å·²ç»è¯´çš„å¤ªå¤šäº†ã€‚å®ƒåŒ…å«ä¸¤ä¸ªç½‘ç»œï¼Œä¸€ä¸ªæ˜¯ç”Ÿæˆç½‘ç»œï¼Œä¸€ä¸ªæ˜¯å¯¹æŠ—ç½‘ç»œï¼ŒåŸºæœ¬åŸç†å¦‚ä¸‹ï¼š (1) Gæ˜¯ä¸€ä¸ªç”Ÿæˆå›¾ç‰‡çš„ç½‘ç»œï¼Œå®ƒæ¥æ”¶éšæœºçš„å™ªå£°zï¼Œé€šè¿‡å™ªå£°ç”Ÿæˆå›¾ç‰‡ï¼Œè®°åšG(z) ã€‚ (2) Dæ˜¯ä¸€ä¸ªåˆ¤åˆ«ç½‘ç»œï¼Œåˆ¤åˆ«ä¸€å¼ å›¾ç‰‡æ˜¯ä¸æ˜¯â€œçœŸå®çš„â€ï¼Œå³æ˜¯çœŸå®çš„å›¾ç‰‡ï¼Œè¿˜æ˜¯ç”±Gç”Ÿæˆçš„å›¾ç‰‡ã€‚ GANçš„ä»¥å‡ä¹±çœŸèƒ½åŠ›å°±ä¸å¤šè¯´äº†ã€‚ 3.2 Autoaugmentation[5] AutoAugmentæ˜¯Googleæå‡ºçš„è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®å¢å¼ºæ–¹æ¡ˆçš„ç ”ç©¶ï¼Œè¿™æ˜¯æ— ç›‘ç£æ•°æ®å¢å¼ºçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚å®ƒçš„åŸºæœ¬æ€è·¯æ˜¯ä½¿ç”¨å¢å¼ºå­¦ä¹ ä»æ•°æ®æœ¬èº«å¯»æ‰¾æœ€ä½³å›¾åƒå˜æ¢ç­–ç•¥ï¼Œå¯¹äºä¸åŒçš„ä»»åŠ¡å­¦ä¹ ä¸åŒçš„å¢å¼ºæ–¹æ³•ï¼Œæµç¨‹å¦‚ä¸‹ï¼š (1) å‡†å¤‡16ä¸ªå¸¸ç”¨çš„æ•°æ®å¢å¼ºæ“ä½œã€‚ (2) ä»16ä¸ªä¸­é€‰æ‹©5ä¸ªæ“ä½œï¼Œéšæœºäº§ç”Ÿä½¿ç”¨è¯¥æ“ä½œçš„æ¦‚ç‡å’Œç›¸åº”çš„å¹…åº¦ï¼Œå°†å…¶ç§°ä¸ºä¸€ä¸ªsub-policyï¼Œä¸€å…±äº§ç”Ÿ5ä¸ªsub-policesã€‚ (3) å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸€ä¸ªbatchçš„å›¾ç‰‡ï¼Œéšæœºé‡‡ç”¨5ä¸ªsub-policesæ“ä½œä¸­çš„ä¸€ç§ã€‚ (4) é€šè¿‡æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ¥åé¦ˆï¼Œä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ³•æ˜¯å¢å¼ºå­¦ä¹ æ–¹æ³•ã€‚ (5) ç»è¿‡80~100ä¸ªepochåç½‘ç»œå¼€å§‹å­¦ä¹ åˆ°æœ‰æ•ˆçš„sub-policiesã€‚ (6) ä¹‹åä¸²æ¥è¿™5ä¸ªsub-policiesï¼Œç„¶åå†è¿›è¡Œæœ€åçš„è®­ç»ƒã€‚ æ€»çš„æ¥è¯´ï¼Œå°±æ˜¯å­¦ä¹ å·²æœ‰æ•°æ®å¢å¼ºçš„ç»„åˆç­–ç•¥ï¼Œå¯¹äºé—¨ç‰Œæ•°å­—è¯†åˆ«ç­‰ä»»åŠ¡ï¼Œç ”ç©¶è¡¨æ˜å‰ªåˆ‡å’Œå¹³ç§»ç­‰å‡ ä½•å˜æ¢èƒ½å¤Ÿè·å¾—æœ€ä½³æ•ˆæœã€‚ è€Œå¯¹äºImageNetä¸­çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ŒAutoAugmentå­¦ä¹ åˆ°äº†ä¸ä½¿ç”¨å‰ªåˆ‡ï¼Œä¹Ÿä¸å®Œå…¨åè½¬é¢œè‰²ï¼Œå› ä¸ºè¿™äº›å˜æ¢ä¼šå¯¼è‡´å›¾åƒå¤±çœŸã€‚AutoAugmentå­¦ä¹ åˆ°çš„æ˜¯ä¾§é‡äºå¾®è°ƒé¢œè‰²å’Œè‰²ç›¸åˆ†å¸ƒã€‚ é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸€äº›æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç¯‡å¹…æœ‰é™ä¸åšè¿‡å¤šè§£è¯»ï¼Œè¯·æŒç»­å…³æ³¨ã€‚ 4 æ€è€ƒ æ•°æ®å¢å¼ºçš„æœ¬è´¨æ˜¯ä¸ºäº†å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé‚£å®ƒä¸å…¶ä»–çš„ä¸€äº›æ–¹æ³•æ¯”å¦‚dropoutï¼Œæƒé‡è¡°å‡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ (1) æƒé‡è¡°å‡ï¼Œdropoutï¼Œstochastic depthç­‰æ–¹æ³•ï¼Œæ˜¯ä¸“é—¨è®¾è®¡æ¥é™åˆ¶æ¨¡å‹çš„æœ‰æ•ˆå®¹é‡çš„ï¼Œç”¨äºå‡å°‘è¿‡æ‹Ÿåˆï¼Œè¿™ä¸€ç±»æ˜¯æ˜¾å¼çš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜è¿™ä¸€ç±»æ–¹æ³•å¯ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¹¶éå¿…è¦ï¼Œä¸”èƒ½åŠ›æœ‰é™ï¼Œè€Œä¸”å‚æ•°é«˜åº¦ä¾èµ–äºç½‘ç»œç»“æ„ç­‰å› ç´ ã€‚ (2) æ•°æ®å¢å¼ºåˆ™æ²¡æœ‰é™ä½ç½‘ç»œçš„å®¹é‡ï¼Œä¹Ÿä¸å¢åŠ è®¡ç®—å¤æ‚åº¦å’Œè°ƒå‚å·¥ç¨‹é‡ï¼Œæ˜¯éšå¼çš„è§„æ•´åŒ–æ–¹æ³•ã€‚å®é™…åº”ç”¨ä¸­æ›´æœ‰æ„ä¹‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸¸è¯´ï¼Œæ•°æ®è‡³ä¸Šã€‚ æˆ‘ä»¬æ€»æ˜¯åœ¨ä½¿ç”¨æœ‰é™çš„æ•°æ®æ¥è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒï¼Œå› æ­¤æ•°æ®å¢å¼ºæ“ä½œæ˜¯ä¸å¯ç¼ºå°‘çš„ä¸€ç¯ã€‚ä»ç ”ç©¶äººå‘˜æ‰‹å·¥å®šä¹‰æ•°æ®å¢å¼ºæ“ä½œï¼Œåˆ°åŸºäºæ— ç›‘ç£çš„æ–¹æ³•ç”Ÿæˆæ•°æ®å’Œå­¦ä¹ å¢å¼ºæ“ä½œçš„ç»„åˆï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é¢†åŸŸï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è‡ªè¡Œäº†è§£æ›´å¤šã€‚ æ›´å¤šå®æˆ˜å’Œç»†èŠ‚ï¼Œå¯ä»¥å»æˆ‘çš„liveä¸­é˜…è¯»ã€‚ è®¡ç®—æœºè§†è§‰ä¸­æ•°æ®å¢å¼ºåŸç†å’Œå®è·µwww.zhihu.com å‚è€ƒæ–‡çŒ® [1] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357. [2] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018. [3] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017. [4] Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative Adversarial Networks[J]. Advances in Neural Information Processing Systems, 2014, 3:2672-2680. [5] Cubuk E D, Zoph B, Mane D, et al. AutoAugment: Learning Augmentation Policies from Data.[J]. arXiv: Computer Vision and Pattern Recognition, 2018. åˆ‡ç‰‡ï¼ˆcropï¼‰ï¼š def crop(image, random_crop, image_size): if image.shape[1]>image_size: sz1 = int(image.shape[1]//2) sz2 = int(image_size//2) if random_crop: diff = sz1-sz2 (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1)) else: (h, v) = (0,0) image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:] return image ############################################################################ # å‡½æ•°ï¼šcrop # æè¿°ï¼šéšæœºè£å‰ªå›¾åƒ # # è¾“å…¥ï¼šå›¾åƒimage, crop_size # è¿”å›ï¼šå›¾åƒimage ############################################################################ def crop(image, crop_size, random_crop=True): if random_crop: # è‹¥éšæœºè£å‰ª if image.shape[1] > crop_size: sz1 = image.shape[1] // 2 sz2 = crop_size // 2 diff = sz1 - sz2 (h, v) = (np.random.randint(0, diff + 1), np.random.randint(0, diff + 1)) image = image[v:(v + crop_size), h:(h + crop_size), :] return image # å·¦å³ä¸Šä¸‹ç¿»è½¬ def flip(image, random_flip=True): if random_flip and np.random.choice([True, False]): image = np.fliplr(image) if random_flip and np.random.choice([True, False]): image = np.flipud(image) return image #å›¾åƒæ—‹è½¬ def random_rotate_image(image): angle = np.random.uniform(low=-10.0, high=10.0) return misc.imrotate(image, angle, 'bicubic') ############################################################################ # å‡½æ•°ï¼šrotation # æè¿°ï¼šéšæœºæ—‹è½¬å›¾ç‰‡ï¼Œå¢å¼ºæ•°æ®ï¼Œç”¨å›¾åƒè¾¹ç¼˜è¿›è¡Œå¡«å……ã€‚ # # è¾“å…¥ï¼šå›¾åƒimage # è¿”å›ï¼šå›¾åƒimage ############################################################################ def rotation(image, random_flip=True): if random_flip and np.random.choice([True, False]): w,h = image.shape[1], image.shape[0] # 0-180éšæœºäº§ç”Ÿæ—‹è½¬è§’åº¦ã€‚ angle = np.random.randint(0,180) RotateMatrix = cv2.getRotationMatrix2D(center=(image.shape[1]/2, image.shape[0]/2), angle=angle, scale=0.7) # image = cv2.warpAffine(image, RotateMatrix, (w,h), borderValue=(129,137,130)) #image = cv2.warpAffine(image, RotateMatrix, (w,h),borderValue=(129,137,130)) image = cv2.warpAffine(image, RotateMatrix, (w,h),borderMode=cv2.BORDER_REPLICATE) return image å›¾åƒå½’ä¸€åŒ–å¤„ç†ï¼š def prewhiten(x): mean = np.mean(x) std = np.std(x) std_adj = np.maximum(std, 1.0/np.sqrt(x.size)) y = np.multiply(np.subtract(x, mean), 1/std_adj) return y å›¾åƒå¹³ç§»ï¼š ############################################################################ # å‡½æ•°ï¼štranslation # æè¿°ï¼šéšæœºå¹³ç§»å›¾ç‰‡ï¼Œå¢å¼ºæ•°æ®ï¼Œç”¨å›¾åƒè¾¹ç¼˜è¿›è¡Œå¡«å……ã€‚ # # è¾“å…¥ï¼šå›¾åƒimage # è¿”å›ï¼šå›¾åƒimage ############################################################################ def translation(image, random_flip=True): if random_flip and np.random.choice([True, False]): w,h = 1920, 1080 H1 = np.float32([[1,0],[0,1]]) H2 = np.random.uniform(50,500, [2,1]) H = np.hstack([H1, H2]) # H = np.float32([[1,0,408],[0,1,431]]) print (H) image = cv2.warpAffine(image, H, (w,h), borderMode=cv2.BORDER_REPLICATE) return image è°ƒæ•´å…‰ç…§ from skimage import exposure import numpy as np def gen_exposure(image, random_xp=True): if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 1.2) # è°ƒæš— if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 1.5) # è°ƒæš— if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.9) # è°ƒäº® if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.8) # è°ƒäº® if random_xp and np.random.choice([True, False]): image = exposure.adjust_gamma(image, 0.7) # è°ƒæš— return image "},"data/imaaug.html":{"url":"data/imaaug.html","title":"imaaug æ•°æ®å¢å¼ºå¤§æ€å™¨","keywords":"","body":"imgaug This python library helps you with augmenting images for your machine learning projects. It converts a set of input images into a new, much larger set of slightly altered images. Â  Image Heatmaps Seg. Maps Keypoints Bounding Boxes,Polygons Original Input Gauss. Noise+Â Contrast+Â Sharpen Affine Crop+Â Pad Fliplr+Â Perspective More (strong) example augmentations of one input image: Features of the library Supports both common and exotic augmentation techniques. E.g. affine transformations, perspective transformations, contrast changes, gaussian noise, dropout of regions, hue/saturation changes, cropping/padding, blurring, ... Supports augmentation of: Images (full support for uint8, for other dtypes see documentation) Heatmaps (float32) Segmentation maps (integer-based, bool, float-based) Keypoints/Landmarks (int or float coordinates) Bounding Boxes (int or float coordinates) Polygons (int or float coordinates) (Beta) LineStrings (int or float coordinates) (Beta) Can augment all of the above automatically with the same sampled values. E.g. rotate both images and the segmentation maps on them by the same random value sampled from uniform(0Â°, 30Â°). Native support for heatmaps and segmentation maps that are smaller than their corresponding images. Define flexible stochastic ranges for each augmentation parameter. E.g. \"rotate each image by a value between -45 and 45 degrees\". E.g. \"rotate each image by ABS(N(0, 20.0))*(1+B(1.0, 1.0))\", where ABS(.) is the absolute function, N(.) the gaussian distribution and B(.) the beta distribution. Offers many helper functions. E.g. for drawing heatmaps, segmentation maps, keypoints, bounding boxes and polygons. E.g. for scaling segmentation maps, average/max pooling of images/maps or for padding images to desired aspect ratios (e.g. to square them). Define your augmentation sequence once at the start of the experiment, then apply it many times. Supports augmentation on multiple CPU cores. Documentation http://imgaug.readthedocs.io/en/latest/source/examples_basics.html - Quick example code on how to use the library. http://imgaug.readthedocs.io/en/latest/source/augmenters.html - Example code for some augmentation techniques. (See also the API, which usually contains examples for each augmenter.) http://imgaug.readthedocs.io/en/latest/source/api.html - API. For tutorial jupyter notebooks, see imgaug-doc/notebooks E.g. Load and Augment an Image, Multicore Support, or working with Keypoints/Landmarks, Bounding Boxes, Polygons, Line Strings, Heatmaps, Segmentation Maps Installation The library supports python 2.7 and 3.4+. To install the library, first install all requirements: pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio Shapely Then install imgaug either via pypi (can lag behind the github version): pip install imgaug or install the latest version directly from github: pip install git+https://github.com/aleju/imgaug Alternatively, you can download the repository via git clone https://github.com/aleju/imgaug and install manually via cd imgaug && python setup.py install. To deinstall the library, just execute pip uninstall imgaug. Recent Changes 0.2.8: Improved performance, dtype support and multicore augmentation. See changelog for more details. Overview of most augmenters The images below show examples for most augmentation techniques (values written in the form (a, b) mean that a value was randomly picked from the range a ): meta Noop ChannelShuffle Â  Â  Â  Â  Â  Â  arithmetic Add Add(per_channel=True) AdditiveGaussianNoise AdditiveGaussianNoise(per_channel=True) AdditiveLaplaceNoise AdditiveLaplaceNoise(per_channel=True) AdditivePoissonNoise AdditivePoissonNoise(per_channel=True) Multiply Multiply(per_channel=True) Dropout Dropout(per_channel=True) CoarseDropout(p=0.2) CoarseDropout(p=0.2, per_channel=True) ImpulseNoise SaltAndPepper Salt Pepper CoarseSaltAndPepper(p=0.2) CoarseSalt(p=0.2) CoarsePepper(p=0.2) Invert Invert(per_channel=True) JpegCompression Â  Â  blend Alphawith EdgeDetect(1.0) Alphawith EdgeDetect(1.0)(per_channel=True) SimplexNoiseAlphawith EdgeDetect(1.0) FrequencyNoiseAlphawith EdgeDetect(1.0) Â  Â  blur GaussianBlur AverageBlur MedianBlur BilateralBlur(sigma_color=250,sigma_space=250) MotionBlur(angle=0) MotionBlur(k=5) Â  Â  Â  Â  Â  Â  Â  Â  color AddToHueAndSaturation Grayscale Â  Â  Â  Â  Â  Â  contrast GammaContrast GammaContrast(per_channel=True) SigmoidContrast(cutoff=0.5) SigmoidContrast(gain=10) SigmoidContrast(per_channel=True) LogContrast LogContrast(per_channel=True) LinearContrast LinearContrast(per_channel=True) AllChannels-HistogramEqualization HistogramEqualization AllChannelsCLAHE AllChannelsCLAHE(per_channel=True) CLAHE Â  Â  convolutional Sharpen(alpha=1) Emboss(alpha=1) EdgeDetect DirectedEdgeDetect(alpha=1) Â  Â  flip Fliplr Flipud Â  Â  geometric Affine Affine: Modes Â  Â  Affine: cval PiecewiseAffine Â  Â  PerspectiveTransform ElasticTransformation(sigma=0.2) Â  Â  ElasticTransformation(sigma=5.0) Rot90 Â  Â  segmentation Superpixels(p_replace=1) Superpixels(n_segments=100) Â  Â  Â  Â  Â  Â  size CropAndPad Crop Â  Â  Pad PadToFixedSize(height'=height+32,width'=width+32) Â  Â  CropToFixedSize(height'=height-32,width'=width-32) Â  Â  Â  Â  Â  Â  weather FastSnowyLandscape(lightness_multiplier=2.0) Clouds Fog Snowflakes Â  Â  Code Examples A standard machine learning situation. Train on batches of images and augment each batch via crop, horizontal flip (\"Fliplr\") and gaussian blur: from imgaug import augmenters as iaa seq = iaa.Sequential([ iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen) iaa.Fliplr(0.5), # horizontally flip 50% of the images iaa.GaussianBlur(sigma=(0, 3.0)) # blur images with a sigma of 0 to 3.0 ]) for batch_idx in range(1000): # 'images' should be either a 4D numpy array of shape (N, height, width, channels) # or a list of 3D numpy arrays, each having shape (height, width, channels). # Grayscale images must have shape (height, width, 1) each. # All images must have numpy's dtype uint8. Values are expected to be in # range 0-255. images = load_batch(batch_idx) # you have to implement this function images_aug = seq.augment_images(images) # done by the library train_on_images(images_aug) # you have to implement this function Apply heavy augmentations to images (used to create the image at the very top of this readme): import imgaug as ia from imgaug import augmenters as iaa import numpy as np # random example images images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases, # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image. sometimes = lambda aug: iaa.Sometimes(0.5, aug) # Define our sequence of augmentation steps that will be applied to every image # All augmenters with per_channel=0.5 will sample one value _per image_ # in 50% of all cases. In all other cases they will sample new values # _per channel_. seq = iaa.Sequential( [ # apply the following augmenters to most images iaa.Fliplr(0.5), # horizontally flip 50% of all images iaa.Flipud(0.2), # vertically flip 20% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis) rotate=(-45, 45), # rotate by -45 to +45 degrees shear=(-16, 16), # shear by -16 to +16 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation iaa.OneOf([ iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0 iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7 iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images # search either for all edges or for directed edges, # blend the result with the original image using a blobby mask iaa.SimplexNoiseAlpha(iaa.OneOf([ iaa.EdgeDetect(alpha=(0.5, 1.0)), iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)), ])), iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images iaa.OneOf([ iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2), ]), iaa.Invert(0.05, per_channel=True), # invert color channels iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.5, 1.5), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-4, 0), first=iaa.Multiply((0.5, 1.5), per_channel=True), second=iaa.ContrastNormalization((0.5, 2.0)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast iaa.Grayscale(alpha=(0.0, 1.0)), sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths) sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1))) ], random_order=True ) ], random_order=True ) images_aug = seq.augment_images(images) Quickly show example results of your augmentation sequence: from imgaug import augmenters as iaa import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) seq = iaa.Sequential([iaa.Fliplr(0.5), iaa.GaussianBlur((0, 3.0))]) # show an image with 8*8 augmented versions of image 0 seq.show_grid(images[0], cols=8, rows=8) # Show an image with 8*8 augmented versions of image 0 and 8*8 augmented # versions of image 1. The identical augmentations will be applied to # image 0 and 1. seq.show_grid([images[0], images[1]], cols=8, rows=8) Augment two batches of images in exactly the same way (e.g. horizontally flip 1st, 2nd and 5th images in both batches, but do not alter 3rd and 4th images): from imgaug import augmenters as iaa # Standard scenario: You have N RGB-images and additionally 21 heatmaps per image. # You want to augment each image and its heatmaps identically. images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) heatmaps = np.random.randint(0, 255, (16, 128, 128, 21), dtype=np.uint8) seq = iaa.Sequential([iaa.GaussianBlur((0, 3.0)), iaa.Affine(translate_px={\"x\": (-40, 40)})]) # Convert the stochastic sequence of augmenters to a deterministic one. # The deterministic sequence will always apply the exactly same effects to the images. seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start images_aug = seq_det.augment_images(images) heatmaps_aug = seq_det.augment_images(heatmaps) Augment images and landmarks/keypoints on these images: import imgaug as ia from imgaug import augmenters as iaa import random import numpy as np images = np.random.randint(0, 50, (4, 128, 128, 3), dtype=np.uint8) # Generate random keypoints. # The augmenters expect a list of imgaug.KeypointsOnImage. keypoints_on_images = [] for image in images: height, width = image.shape[0:2] keypoints = [] for _ in range(4): x = random.randint(0, width-1) y = random.randint(0, height-1) keypoints.append(ia.Keypoint(x=x, y=y)) keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape)) seq = iaa.Sequential([iaa.GaussianBlur((0, 3.0)), iaa.Affine(scale=(0.5, 0.7))]) seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start # augment keypoints and images images_aug = seq_det.augment_images(images) keypoints_aug = seq_det.augment_keypoints(keypoints_on_images) # Example code to show each image and print the new keypoints coordinates for img_idx, (image_before, image_after, keypoints_before, keypoints_after) in enumerate(zip(images, images_aug, keypoints_on_images, keypoints_aug)): image_before = keypoints_before.draw_on_image(image_before) image_after = keypoints_after.draw_on_image(image_after) ia.imshow(np.concatenate((image_before, image_after), axis=1)) # before and after for kp_idx, keypoint in enumerate(keypoints_after.keypoints): keypoint_old = keypoints_on_images[img_idx].keypoints[kp_idx] x_old, y_old = keypoint_old.x, keypoint_old.y x_new, y_new = keypoint.x, keypoint.y print(\"[Keypoints for image #%d] before aug: x=%d y=%d | after aug: x=%d y=%d\" % (img_idx, x_old, y_old, x_new, y_new)) Apply single augmentations to images: from imgaug import augmenters as iaa import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) flipper = iaa.Fliplr(1.0) # always horizontally flip each input image images[0] = flipper.augment_image(images[0]) # horizontally flip image 0 vflipper = iaa.Flipud(0.9) # vertically flip each input image with 90% probability images[1] = vflipper.augment_image(images[1]) # probably vertically flip image 1 blurer = iaa.GaussianBlur(3.0) images[2] = blurer.augment_image(images[2]) # blur image 2 by a sigma of 3.0 images[3] = blurer.augment_image(images[3]) # blur image 3 by a sigma of 3.0 too translater = iaa.Affine(translate_px={\"x\": -16}) # move each input image by 16px to the left images[4] = translater.augment_image(images[4]) # move image 4 to the left scaler = iaa.Affine(scale={\"y\": (0.8, 1.2)}) # scale each input image to 80-120% on the y axis images[5] = scaler.augment_image(images[5]) # scale image 5 by 80-120% on the y axis Apply an augmenter to only specific image channels: from imgaug import augmenters as iaa import numpy as np # fake RGB images images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # add a random value from the range (-30, 30) to the first two channels of # input images (e.g. to the R and G channels) aug = iaa.WithChannels( channels=[0, 1], children=iaa.Add((-30, 30)) ) images_aug = aug.augment_images(images) You can use more unusual distributions for the stochastic parameters of each augmenter: from imgaug import augmenters as iaa from imgaug import parameters as iap import numpy as np images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8) # Blur by a value sigma which is sampled from a uniform distribution # of range 0.1 Images can be augmented in background processes using the method augment_batches(batches, background=True), where batches is expected to be a list of image batches or a list of batches/lists of imgaug.KeypointsOnImage or a list of imgaug.Batch. The following example augments a list of image batches in the background: import imgaug as ia from imgaug import augmenters as iaa import numpy as np from skimage import data # Number of batches and batch size for this example nb_batches = 10 batch_size = 32 # Example augmentation sequence to run in the background augseq = iaa.Sequential([ iaa.Fliplr(0.5), iaa.CoarseDropout(p=0.1, size_percent=0.1) ]) # For simplicity, we use the same image here many times astronaut = data.astronaut() astronaut = ia.imresize_single_image(astronaut, (64, 64)) # Make batches out of the example image (here: 10 batches, each 32 times # the example image) batches = [] for _ in range(nb_batches): batches.append( np.array( [astronaut for _ in range(batch_size)], dtype=np.uint8 ) ) # Show the augmented images. # Note that augment_batches() returns a generator. for images_aug in augseq.augment_batches(batches, background=True): ia.imshow(ia.draw_grid(images_aug, cols=8)) If you need a bit more control over the background augmentation process, you can work with augmenter.pool(), which allows you to define how many CPU cores to use, how often to restart child workers, which random number seed to use and how large the chunks of data transferred to each child worker should be. import numpy as np import imgaug as ia from imgaug import augmenters as iaa # Basic augmentation sequence. PiecewiseAffine is slow and therefore well suited # for augmentation on multiple CPU cores. aug = iaa.Sequential([ iaa.Fliplr(0.5), iaa.PiecewiseAffine((0.0, 0.1)) ]) # generator that yields images def create_image_generator(nb_batches, size): for _ in range(nb_batches): # Add e.g. keypoints=... or bounding_boxes=... here to also augment # keypoints / bounding boxes on these images. yield ia.Batch( images=np.random.randint(0, 255, size=size).astype(np.uint8) ) # 500 batches of images, each containing 10 images of size 128x128x3 my_generator = create_image_generator(500, (10, 128, 128, 3)) # Start a pool to augment on multiple CPU cores. # * processes=-1 means that all CPU cores except one are used for the # augmentation, so one is kept free to move data to the GPU # * maxtasksperchild=20 restarts child workers every 20 tasks -- only use this # if you encounter problems such as memory leaks. Restarting child workers # decreases performance. # * seed=123 makes the result of the whole augmentation process deterministic # between runs of this script, i.e. reproducible results. with aug.pool(processes=-1, maxtasksperchild=20, seed=123) as pool: # Augment on multiple CPU cores. # * The result of imap_batches() is also a generator. # * Use map_batches() if your input is a list. # * chunksize=10 controls how much data to send to each child worker per # transfer, set it higher for better performance. batches_aug_generator = pool.imap_batches(my_generator, chunksize=10) for i, batch_aug in enumerate(batches_aug_generator): # show first augmented image in first batch if i == 0: ia.imshow(batch_aug.images_aug[0]) # do something else with the batch here You can dynamically deactivate augmenters in an already defined sequence: import imgaug as ia from imgaug import augmenters as iaa import numpy as np # images and heatmaps, just arrays filled with value 30 images = np.ones((16, 128, 128, 3), dtype=np.uint8) * 30 heatmaps = np.ones((16, 128, 128, 21), dtype=np.uint8) * 30 # add vertical lines to see the effect of flip images[:, 16:128-16, 120:124, :] = 120 heatmaps[:, 16:128-16, 120:124, :] = 120 seq = iaa.Sequential([ iaa.Fliplr(0.5, name=\"Flipper\"), iaa.GaussianBlur((0, 3.0), name=\"GaussianBlur\"), iaa.Dropout(0.02, name=\"Dropout\"), iaa.AdditiveGaussianNoise(scale=0.01*255, name=\"MyLittleNoise\"), iaa.AdditiveGaussianNoise(loc=32, scale=0.0001*255, name=\"SomeOtherNoise\"), iaa.Affine(translate_px={\"x\": (-40, 40)}, name=\"Affine\") ]) # change the activated augmenters for heatmaps, # we only want to execute horizontal flip, affine transformation and one of # the gaussian noises def activator_heatmaps(images, augmenter, parents, default): if augmenter.name in [\"GaussianBlur\", \"Dropout\", \"MyLittleNoise\"]: return False else: # default value for all other augmenters return default hooks_heatmaps = ia.HooksImages(activator=activator_heatmaps) seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start images_aug = seq_det.augment_images(images) heatmaps_aug = seq_det.augment_images(heatmaps, hooks=hooks_heatmaps) List of augmenters The following is a list of available augmenters. Note that most of the below mentioned variables can be set to ranges, e.g. A=(0.0, 1.0) to sample a random value between 0 and 1.0 per image, or A=[0.0, 0.5, 1.0] to sample randomly either 0.0 or 0.5 or 1.0 per image. arithmetic Augmenter Description Add(V, PCH) Adds value V to each image. If PCH is true, then the sampled values may be different per channel. AddElementwise(V, PCH) Adds value V to each pixel. If PCH is true, then the sampled values may be different per channel (and pixel). AdditiveGaussianNoise(L, S, PCH) Adds white/gaussian noise pixelwise to an image. The noise comes from the normal distribution N(L,S). If PCH is true, then the sampled values may be different per channel (and pixel). AdditiveLaplaceNoise(L, S, PCH) Adds noise sampled from a laplace distribution following Laplace(L, S) to images. If PCH is true, then the sampled values may be different per channel (and pixel). AdditivePoissonNoise(L, PCH) Adds noise sampled from a poisson distribution with L being the lambda exponent. If PCH is true, then the sampled values may be different per channel (and pixel). Multiply(V, PCH) Multiplies each image by value V, leading to darker/brighter images. If PCH is true, then the sampled values may be different per channel. MultiplyElementwise(V, PCH) Multiplies each pixel by value V, leading to darker/brighter pixels. If PCH is true, then the sampled values may be different per channel (and pixel). Dropout(P, PCH) Sets pixels to zero with probability P. If PCH is true, then channels may be treated differently, otherwise whole pixels are set to zero. CoarseDropout(P, SPX, SPC, PCH) Like Dropout, but samples the locations of pixels that are to be set to zero from a coarser/smaller image, which has pixel size SPX or relative size SPC. I.e. if SPC has a small value, the coarse map is small, resulting in large rectangles being dropped. ReplaceElementwise(M, R, PCH) Replaces pixels in an image by replacements R. Replaces the pixels identified by mask M. M can be a probability, e.g. 0.05 to replace 5% of all pixels. If PCH is true, then the mask will be sampled per image, pixel and additionally channel. ImpulseNoise(P) Replaces P percent of all pixels with impulse noise, i.e. very light/dark RGB colors. This is an alias for SaltAndPepper(P, PCH=True). SaltAndPepper(P, PCH) Replaces P percent of all pixels with very white or black colors. If PCH is true, then different pixels will be replaced per channel. CoarseSaltAndPepper(P, SPX, SPC, PCH) Similar to CoarseDropout, but instead of setting regions to zero, they are replaced by very white or black colors. If PCH is true, then the coarse replacement masks are sampled once per image and channel. Salt(P, PCH) Similar to SaltAndPepper, but only replaces with very white colors, i.e. no black colors. CoarseSalt(P, SPX, SPC, PCH) Similar to CoarseSaltAndPepper, but only replaces with very white colors, i.e. no black colors. Pepper(P, PCH) Similar to SaltAndPepper, but only replaces with very black colors, i.e. no white colors. CoarsePepper(P, SPX, SPC, PCH) Similar to CoarseSaltAndPepper, but only replaces with very black colors, i.e. no white colors. Invert(P, PCH) Inverts with probability P all pixels in an image, i.e. sets them to (1-pixel_value). If PCH is true, each channel is treated individually (leading to only some channels being inverted). ContrastNormalization(S, PCH) Changes the contrast in images, by moving pixel values away or closer to 128. The direction and strength is defined by S. If PCH is set to true, the process happens channel-wise with possibly different S. JpegCompression(C) Applies JPEG compression of strength C (value range: 0 to 100) to an image. Higher values of C lead to more visual artifacts. blend Augmenter Description Alpha(A, FG, BG, PCH) Augments images using augmenters FG and BG independently, then blends the result using alpha A. Both FG and BG default to doing nothing if not provided. E.g. use Alpha(0.9, FG) to augment images via FG, then blend the result, keeping 10% of the original image (before FG). If PCH is set to true, the process happens channel-wise with possibly different A (FG and BG are computed once per image). AlphaElementwise(A, FG, BG, PCH) Same as Alpha, but performs the blending pixel-wise using a continuous mask (values 0.0 to 1.0) sampled from A. If PCH is set to true, the process happens both pixel- and channel-wise. SimplexNoiseAlpha(FG, BG, PCH, SM, UP, I, AGG, SIG, SIGT) Similar to Alpha, but uses a mask to blend the results from augmenters FG and BG. The mask is sampled from simplex noise, which tends to be blobby. The mask is gathered in I iterations (default: 1 to 3), each iteration is combined using aggregation method AGG (default max, i.e. maximum value from all iterations per pixel). Each mask is sampled in low resolution space with max resolution SM (default 2 to 16px) and upscaled to image size using method UP (default: linear or cubic or nearest neighbour upsampling). If SIG is true, a sigmoid is applied to the mask with threshold SIGT, which makes the blobs have values closer to 0.0 or 1.0. FrequencyNoiseAlpha(E, FG, BG, PCH, SM, UP, I, AGG, SIG, SIGT) Similar to SimplexNoiseAlpha, but generates noise masks from the frequency domain. Exponent E is used to increase/decrease frequency components. High values for E pronounce high frequency components. Use values in the range -4 to 4, with -2 roughly generated cloud-like patterns. blur Augmenter Description GaussianBlur(S) Blurs images using a gaussian kernel with size S. AverageBlur(K) Blurs images using a simple averaging kernel with size K. MedianBlur(K) Blurs images using a median over neihbourhoods of size K. BilateralBlur(D, SC, SS) Blurs images using a bilateral filter with distance D (like kernel size). SC is a sigma for the (influence) distance in color space, SS a sigma for the spatial distance. MotionBlur(K, A, D, O) Blurs an image using a motion blur kernel with size K. A is the angle of the blur in degrees to the y-axis (value range: 0 to 360, clockwise). D is the blur direction (value range: -1.0 to 1.0, 1.0 is forward from the center). O is the interpolation order (O=0 is fast, O=1 slightly slower but more accurate). color Augmenter Description WithColorspace(T, F, CH) Converts images from colorspace T to F, applies child augmenters CH and then converts back from F to T. AddToHueAndSaturation(V, PCH, F, C) Adds value V to each pixel in HSV space (i.e. modifying hue and saturation). Converts from colorspace F to HSV (default is F=RGB). Selects channels C before augmenting (default is C=[0,1]). If PCH is true, then the sampled values may be different per channel. ChangeColorspace(T, F, A) Converts images from colorspace F to T and mixes with the original image using alpha A. Grayscale remains at three channels. (Fairly untested augmenter, use at own risk.) Grayscale(A, F) Converts images from colorspace F (default: RGB) to grayscale and mixes with the original image using alpha A. contrast Augmenter Description GammaContrast(G, PCH) Applies gamma contrast adjustment following I_ij' = I_ij**G', where G' is a gamma value sampled from G and I_ij a pixel (converted to 0 to 1.0 space). If PCH is true, a different G' is sampled per image and channel. SigmoidContrast(G, C, PCH) Similar to GammaContrast, but applies I_ij' = 1/(1 + exp(G' * (C' - I_ij))), where G' is a gain value sampled from G and C' is a cutoff value sampled from C. LogContrast(G, PCH) Similar to GammaContrast, but applies I_ij = G' * log(1 + I_ij), where G' is a gain value sampled from G. LinearContrast(S, PCH) Similar to GammaContrast, but applies I_ij = 128 + S' * (I_ij - 128), where S' is a strength value sampled from S. This augmenter is identical to ContrastNormalization (which will be deprecated in the future). AllChannelsHistogramEqualization() Applies standard histogram equalization to each channel of each input image. HistogramEqualization(F, T) Similar to AllChannelsHistogramEqualization, but expects images to be in colorspace F, converts to colorspace T and normalizes only an intensity-related channel, e.g. L for T=Lab (default for T) or V for T=HSV. AllChannelsCLAHE(CL, K, Kmin, PCH) Contrast Limited Adaptive Histogram Equalization (histogram equalization in small image patches), applied to each image channel with clipping limit CL and kernel size K (clipped to range [Kmin, inf)). If PCH is true, different values for CL and K are sampled per channel. CLAHE(CL, K, Kmin, F, T) Similar to HistogramEqualization, this applies CLAHE only to intensity-related channels in Lab/HSV/HLS colorspace. (Usually this works significantly better than AllChannelsCLAHE.) convolutional Augmenter Description Convolve(M) Convolves images with matrix M, which can be a lambda function. Sharpen(A, L) Runs a sharpening kernel over each image with lightness L (low values result in dark images). Mixes the result with the original image using alpha A. Emboss(A, S) Runs an emboss kernel over each image with strength S. Mixes the result with the original image using alpha A. EdgeDetect(A) Runs an edge detection kernel over each image. Mixes the result with the original image using alpha A. DirectedEdgeDetect(A, D) Runs a directed edge detection kernel over each image, which detects each from direction D (default: random direction from 0 to 360 degrees, chosen per image). Mixes the result with the original image using alpha A. flip Augmenter Description Fliplr(P) Horizontally flips images with probability P. Flipud(P) Vertically flips images with probability P. geometric Augmenter Description Affine(S, TPX, TPC, R, SH, O, CVAL, FO, M, B) Applies affine transformations to images. Scales them by S (>1=zoom in, TPX pixels or TPC percent, rotates them by R degrees and shears them by SH degrees. Interpolation happens with order O (0 or 1 are good and fast). If FO is true, the output image plane size will be fitted to the distorted image size, i.e. images rotated by 45deg will not be partially outside of the image plane. M controls how to handle pixels in the output image plane that have no correspondence in the input image plane. If M='constant' then CVAL defines a constant value with which to fill these pixels. B allows to set the backend framework (currently cv2 or skimage). AffineCv2(S, TPX, TPC, R, SH, O, CVAL, M, B) Same as Affine, but uses only cv2 as its backend. Currently does not support FO=true. Might be deprecated in the future. PiecewiseAffine(S, R, C, O, M, CVAL) Places a regular grid of points on the image. The grid has R rows and C columns. Then moves the points (and the image areas around them) by amounts that are samples from normal distribution N(0,S), leading to local distortions of varying strengths. O, M and CVAL are defined as in Affine. PerspectiveTransform(S, KS) Applies a random four-point perspective transform to the image (kinda like an advanced form of cropping). Each point has a random distance from the image corner, derived from a normal distribution with sigma S. If KS is set to True (default), each image will be resized back to its original size. ElasticTransformation(S, SM, O, CVAL, M) Moves each pixel individually around based on distortion fields. SM defines the smoothness of the distortion field and S its strength. O is the interpolation order, CVAL a constant fill value for newly created pixels and M the fill mode (see also augmenter Affine). Rot90(K, KS) Rotate images K times clockwise by 90 degrees. (This is faster than Affine.) If KS is true, the resulting image will be resized to have the same size as the original input image. meta Augmenter Description Sequential(C, R) Takes a list of child augmenters C and applies them in that order to images. If R is true (default: false), then the order is random (chosen once per batch). SomeOf(N, C, R) Applies N randomly selected augmenters from a list of augmenters C to each image. The augmenters are chosen per image. R is the same as for Sequential. N can be a range, e.g. (1, 3) in order to pick 1 to 3. OneOf(C) Identical to SomeOf(1, C). Sometimes(P, C, D) Augments images with probability P by using child augmenters C, otherwise uses D. D can be None, then only P percent of all images are augmented via C. WithColorspace(T, F, C) Transforms images from colorspace F (default: RGB) to colorspace T, applies augmenters C and then converts back to F. WithChannels(H, C) Selects from each image channels H (e.g. [0,1] for red and green in RGB images), applies child augmenters C to these channels and merges the result back into the original images. Noop() Does nothing. (Useful for validation/test.) Lambda(I, K) Applies lambda function I to images and K to keypoints. AssertLambda(I, K) Checks images via lambda function I and keypoints via K and raises an error if false is returned by either of them. AssertShape(S) Raises an error if input images are not of shape S. ChannelShuffle(P, C) Permutes the order of the color channels for P percent of all images. Shuffles by default all channels, but may restrict to a subset using C (list of channel indices). segmentation Augmenter Description Superpixels(P, N, M) Generates N superpixels of the image at (max) resolution M and resizes back to the original size. Then P percent of all superpixel areas in the original image are replaced by the superpixel. (1-P) percent remain unaltered. size Augmenter Description Resize(S, I) Resizes images to size S. Common use case would be to use S={\"height\":H, \"width\":W} to resize all images to shape HxW. H and W may be floats (e.g. resize to 50% of original size). Either H or W may be \"keep-aspect-ratio\" to define only one side's new size and resize the other side correspondingly. I is the interpolation to use (default: cubic). CropAndPad(PX, PC, PM, PCV, KS) Crops away or pads PX pixels or PC percent of pixels at top/right/bottom/left of images. Negative values result in cropping, positive in padding. PM defines the pad mode (e.g. use uniform color for all added pixels). PCV controls the color of added pixels if PM=constant. If KS is true (default), the resulting image is resized back to the original size. Pad(PX, PC, PM, PCV, KS) Shortcut for CropAndPad(), which only adds pixels. Only positive values are allowed for PX and PC. Crop(PX, PC, KS) Shortcut for CropAndPad(), which only crops away pixels. Only positive values are allowed for PX and PC (e.g. a value of 5 results in 5 pixels cropped away). PadToFixedSize(W, H, PM, PCV, POS) Pads all images up to height H and width W. PM and PCV are the same as in Pad. POS defines the position around which to pad, e.g. POS=\"center\" pads equally on all sides, POS=\"left-top\" pads only the top and left sides. CropToFixedSize(W, H, POS) Similar to PadToFixedSize, but crops down to height H and width W instead of padding. KeepSizeByResize(CH, I, IH) Applies child augmenters CH (e.g. cropping) and afterwards resizes all images back to their original size. I is the interpolation used for images, IH the interpolation used for heatmaps. weather Augmenter Description FastSnowyLandscape(LT, LM) Converts landscape images to snowy landscapes by increasing in HLS colorspace the lightness L of all pixels with L by a factor of LM. Clouds() Adds clouds of various shapes and densities to images. Can be senseful to be combined with an overlay augmenter, e.g. SimplexNoiseAlpha. Fog() Adds fog-like cloud structures of various shapes and densities to images. Can be senseful to be combined with an overlay augmenter, e.g. SimplexNoiseAlpha. CloudLayer(IM, IFE, ICS, AMIN, AMUL, ASPXM, AFE, S, DMUL) Adds a single layer of clouds to an image. IM is the mean intensity of the clouds, IFE a frequency noise exponent for the intensities (leading to non-uniform colors), ICS controls the variance of a gaussian for intensity sampling, AM is the minimum opacity of the clouds (values >0 are typical of fog), AMUL a multiplier for opacity values, ASPXM controls the minimum grid size at which to sample opacity values, AFE is a frequency noise exponent for opacity values, S controls the sparsity of clouds and DMUL is a cloud density multiplier. This interface is not final and will likely change in the future. Snowflakes(D, DU, FS, FSU, A, S) Adds snowflakes with density D, density uniformity DU, snowflake size FS, snowflake size uniformity FSU, falling angle A and speed S to an image. One to three layers of snowflakes are added, hence the values should be stochastic. SnowflakesLayer(D, DU, FS, FSU, A, S, BSF, BSL) Adds a single layer of snowflakes to an image. See augmenter Snowflakes. BSF and BSL control a gaussian blur applied to the snowflakes. "},"math/":{"url":"math/","title":"æ•°å­¦","keywords":"","body":" çŸ©é˜µæ€»ç»“ åˆ†å¸ƒ ä»¿å°„å˜æ¢ å›¾çš„åŸºæœ¬æ¦‚å¿µ "},"math/matrix.html":{"url":"math/matrix.html","title":"çŸ©é˜µæ€»ç»“","keywords":"","body":"å¯¹ç§°çŸ©é˜µã€HermiteçŸ©é˜µã€æ­£äº¤çŸ©é˜µã€é…‰çŸ©é˜µã€å¥‡å¼‚çŸ©é˜µã€æ­£è§„çŸ©é˜µã€å¹‚ç­‰çŸ©é˜µ çœ‹æ–‡çŒ®çš„æ—¶å€™ï¼Œç»å¸¸è§åˆ°å„ç§å„æ ·çŸ©é˜µï¼Œæœ¬ç¯‡æ€»ç»“äº†å¸¸è§çš„å¯¹ç§°çŸ©é˜µã€HermiteçŸ©é˜µã€æ­£äº¤çŸ©é˜µã€é…‰çŸ©é˜µã€å¥‡å¼‚çŸ©é˜µã€æ­£è§„çŸ©é˜µã€å¹‚ç­‰çŸ©é˜µä¸ƒç§çŸ©é˜µçš„å®šä¹‰ï¼Œä½œä¸ºæ¦‚å¿µå¤‡å¿˜å½•å§ï¼Œå¿˜äº†å¯ä»¥éšæ—¶æŸ¥ä¸€ä¸‹ã€‚ 1ã€å¯¹ç§°çŸ©é˜µï¼ˆæ–‡çŒ®ã€1ã€‘ç¬¬40é¡µï¼‰ å…¶ä¸­ä¸Šæ ‡Tè¡¨ç¤ºæ±‚çŸ©é˜µçš„è½¬ç½®ï¼ˆæ–‡çŒ®ã€1ã€‘ç¬¬38-39é¡µï¼‰ 2ã€HermiteçŸ©é˜µï¼ˆæ–‡çŒ®ã€2ã€‘ç¬¬97é¡µï¼‰ å…¶ä¸­Hè¡¨ç¤ºæ±‚çŸ©é˜µçš„å¤å…±è½­è½¬ç½®ï¼šï¼ˆæ–‡çŒ®ã€2ã€‘ç¬¬96é¡µï¼‰ Hermiteé˜µæ˜¯å¯¹ç§°é˜µæ¦‚å¿µçš„æ¨å¹¿ï¼Œå¯¹ç§°é˜µé’ˆå¯¹å®çŸ©é˜µï¼ˆçŸ©é˜µå…ƒç´ å‡ä¸ºå®æ•°ï¼‰ï¼ŒHermiteé˜µé’ˆå¯¹å¤çŸ©é˜µã€‚ 3ã€æ­£äº¤çŸ©é˜µï¼ˆæ–‡çŒ®ã€1ã€‘ç¬¬115é¡µï¼‰ 4ã€é…‰çŸ©é˜µï¼ˆæ–‡çŒ®ã€2ã€‘ç¬¬102é¡µï¼‰ ç±»ä¼¼äºHermiteé˜µç›¸å¯¹äºå¯¹ç§°é˜µï¼Œé…‰çŸ©é˜µæ˜¯æ­£äº¤é˜µæ¦‚å¿µçš„æ¨å¹¿ã€‚ 5ã€å¥‡å¼‚çŸ©é˜µï¼ˆæ–‡çŒ®ã€1ã€‘ç¬¬43é¡µï¼‰ 6ã€æ­£è§„çŸ©é˜µï¼ˆæ–‡çŒ®ã€2ã€‘ç¬¬119é¡µï¼‰ 7ã€å¹‚ç­‰çŸ©é˜µï¼ˆæ–‡çŒ®ã€2ã€‘ç¬¬106-107é¡µï¼‰ å‚è€ƒæ–‡çŒ®ï¼š ã€1ã€‘åŒæµå¤§å­¦æ•°å­¦ç³» ç¼–. å·¥ç¨‹æ•°å­¦çº¿æ€§ä»£æ•°[M]. 5ç‰ˆ.é«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼Œ2007. ã€2ã€‘å²è£æ˜Œ, é­ä¸°. çŸ©é˜µåˆ†æ[M]. 3ç‰ˆ.åŒ—äº¬ï¼šåŒ—äº¬ç†å·¥å¤§å­¦å‡ºç‰ˆç¤¾, 2010. ä½œè€…ï¼šjbb0523 æ¥æºï¼šCSDN åŸæ–‡ï¼šhttps://blog.csdn.net/jbb0523/article/details/50596604 ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºåšä¸»åŸåˆ›æ–‡ç« ï¼Œè½¬è½½è¯·é™„ä¸Šåšæ–‡é“¾æ¥ï¼ "},"math/distribution_show.html":{"url":"math/distribution_show.html","title":"åˆ†å¸ƒ","keywords":"","body":"distribution-is-all-you-need distribution-is-all-you-need is the basic distribution probability tutorial for most common distribution focused on Deep learning using python library. Overview of distribution probability conjugate means it has relationship of conjugate distributions. In Bayesian probability theory, if the posterior distributions p(Î¸ | x) are in the same probability distribution family as the prior probability distribution p(Î¸), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. Conjugate prior, wikipedia Multi-Class means that Random Varivance are more than 2. N Times means that we also consider prior probability P(X). To learn more about probability, I recommend reading [pattern recognition and machine learning, Bishop 2006]. distribution probabilities and features Uniform distribution(continuous) code Uniform distribution has same probaility value on [a, b], easy probability. Bernoulli distribution(discrete) , code - Bernoulli distribution is not considered about prior probability P(X). Therefore, if we optimize to the maximum likelihood, we will be vulnerable to overfitting. - We use **binary cross entropy** to classify binary classification. It has same form like taking a negative log of the bernoulli distribution. Binomial distribution(discrete) , code - Binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments. - Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance. Multi-Bernoulli distribution, Categorical distribution(discrete) , code - Multi-bernoulli called categorical distribution, is a probability expanded more than 2. - **cross entopy** has same form like taking a negative log of the Multi-Bernoulli distribution. Multinomial distribution(discrete) , code - The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial. Beta distribution(continuous) , code - Beta distribution is conjugate to the binomial and Bernoulli distributions. - Using conjucation, we can get the posterior distribution more easily using the prior distribution we know. - Uniform distiribution is same when beta distribution met special case(alpha=1, beta=1). Dirichlet distribution(continuous) , code - Dirichlet distribution is conjugate to the MultiNomial distributions. - If k=2, it will be Beta distribution. Gamma distribution(continuous) , code - Gamma distribution will be beta distribution, if `Gamma(a,1) / Gamma(a,1) + Gamma(b,1)` is same with `Beta(a,b)`. - The exponential distribution and chi-squared distribution are special cases of the gamma distribution. Exponential distribution(continuous) , code - Exponential distribution is special cases of the gamma distribution when alpha is 1. Gaussian distribution(continuous) , code - Gaussian distribution is a very common continuous probability distribution Normal distribution(continuous) , code - Normal distribution is standarzed Gaussian distribution, it has 0 mean and 1 std. Chi-squared distribution(continuous) , code - Chi-square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. - Chi-square distribution is special case of Beta distribution Student-t distribution(continuous) , code - The t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. Author If you would like to see the details about relationship of distribution probability, please refer to this. Tae Hwan Jung @graykode, Kyung Hee Univ CE(Undergraduate). Author Email : nlkey2022@gmail.com If you leave the source, you can use it freely. "},"math/affine_transformation.html":{"url":"math/affine_transformation.html","title":"ä»¿å°„å˜æ¢","keywords":"","body":"æ–‡ç« ç»“æ„å¦‚ä¸‹ï¼š \\1. ä»¿å°„å˜æ¢ï¼ˆaffine transformationï¼‰ \\2. Jacobian çŸ©é˜µä¸è¡Œåˆ—å¼ 2.1 Jacobian çŸ©é˜µ 2.2 Jacobian è¡Œåˆ—å¼ 2.2.1 çº¿æ€§å˜æ¢å°†é¢ç§¯ä¼¸ç¼© 2.2.2 Jacobian è¡Œåˆ—å¼æ„ä¹‰ 2.2.3 Jacobian è¡Œåˆ—å¼ç”¨é€” \\3. Jacobian ä¸ Hessian 1. ä»¿å°„å˜æ¢ï¼ˆaffine transformationï¼‰ è®¾ ä¸ºä¸€ é˜¶å®çŸ©é˜µï¼Œ æ˜¯ ç»´å‘é‡ï¼Œå®šä¹‰äºå‡ ä½•ç©ºé—´ çš„ä»¿å°„å˜æ¢å…·æœ‰ä¸‹åˆ—å½¢å¼ï¼š ä¹Ÿå°±æ˜¯è¯´ï¼Œä»¿å°„å˜æ¢ç”±ä¸€çº¿æ€§å˜æ¢åŠ ä¸Šä¸€å¹³ç§»é‡æ„æˆã€‚å› ä¸º ã€‚é™¤éå¹³ç§»é‡ ä¸ºé›¶ï¼Œä»¿å°„å˜æ¢æ‰æ˜¯çº¿æ€§å˜æ¢ã€‚ä»¿å°„å˜æ¢æœ‰ä¸¤ä¸ªç‰¹æ®Šçš„æ€§è´¨ï¼šå…±çº¿(collinearity)ä¸å˜æ€§å’Œæ¯”ä¾‹ä¸å˜æ€§ï¼Œæ„æ€æ˜¯ çš„ä»»ä¸€ç›´çº¿ç»ä»¿å°„å˜æ¢çš„åƒ(image)ä»æ˜¯ä¸€ç›´çº¿ï¼Œè€Œä¸”ç›´çº¿ä¸Šå„ç‚¹ä¹‹é—´çš„è·ç¦»æ¯”ä¾‹ç»´æŒä¸å˜ã€‚ è¯æ˜ï¼š è®¾ ä¸º ä¸­ä»»ä¸€ç›´çº¿ï¼Œå…¶è¡¨è¾¾å¼ä¸º ï¼Œå‘é‡ ä»£è¡¨ç›´çº¿æŒ‡å‘ï¼Œ æ˜¯ç›´çº¿ä¸Šä¸€ç‚¹ã€‚ä»¤ ä¸ºç›´çº¿ ä¸Šçš„ä¸‰ä¸ªç›¸å¼‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯ ï¼Œæ­¤ä¸‰ç‚¹ç»ä»¿å°„å˜æ¢ åçš„æ–°ä½ç½®åˆ†åˆ«ä¸ºï¼š ä»¤ ï¼Œæ–°çš„ä½ç½®å¯ä»¥è¡¨ç¤ºæˆ ï¼Œæ‰€ä»¥ä¸‰ä¸ªç‚¹ç»è¿‡ä»¿å°„å˜æ¢åä»åœ¨åŒä¸€æ¡ç›´çº¿ ã€‚ä¸‹é¢è®¡ç®—ä¸‰ç‚¹ä¹‹é—´çš„è·ç¦»æ¯”ä¾‹ï¼š âˆ¥p2â€²âˆ’p1â€²âˆ¥âˆ¥p3â€²âˆ’p2â€²âˆ¥=âˆ¥(t2âˆ’t1)vâ€²âˆ¥âˆ¥(t3âˆ’t2)vâ€²âˆ¥=âˆ¥(t2âˆ’t1)vâˆ¥âˆ¥(t3âˆ’t2)vâˆ¥=âˆ¥p2âˆ’p1âˆ¥âˆ¥p3âˆ’p2âˆ¥ \\frac{\\left\\|\\mathbf{p}_{2}^{\\prime}-\\mathbf{p}_{1}^{\\prime}\\right\\|}{\\left\\|\\mathbf{p}_{3}^{\\prime}-\\mathbf{p}_{2}^{\\prime}\\right\\|}=\\frac{\\left\\|\\left(t_{2}-t_{1}\\right) \\mathbf{v}^{\\prime}\\right\\|}{\\left\\|\\left(t_{3}-t_{2}\\right) \\mathbf{v}^{\\prime}\\right\\|}=\\frac{\\left\\|\\left(t_{2}-t_{1}\\right) \\mathbf{v}\\right\\|}{\\left\\|\\left(t_{3}-t_{2}\\right) \\mathbf{v}\\right\\|}=\\frac{\\left\\|\\mathbf{p}_{2}-\\mathbf{p}_{1}\\right\\|}{\\left\\|\\mathbf{p}_{3}-\\mathbf{p}_{2}\\right\\|} â€‹âˆ¥pâ€‹3â€‹â€²â€‹â€‹âˆ’pâ€‹2â€‹â€²â€‹â€‹âˆ¥â€‹â€‹âˆ¥pâ€‹2â€‹â€²â€‹â€‹âˆ’pâ€‹1â€‹â€²â€‹â€‹âˆ¥â€‹â€‹=â€‹âˆ¥(tâ€‹3â€‹â€‹âˆ’tâ€‹2â€‹â€‹)vâ€‹â€²â€‹â€‹âˆ¥â€‹â€‹âˆ¥(tâ€‹2â€‹â€‹âˆ’tâ€‹1â€‹â€‹)vâ€‹â€²â€‹â€‹âˆ¥â€‹â€‹=â€‹âˆ¥(tâ€‹3â€‹â€‹âˆ’tâ€‹2â€‹â€‹)vâˆ¥â€‹â€‹âˆ¥(tâ€‹2â€‹â€‹âˆ’tâ€‹1â€‹â€‹)vâˆ¥â€‹â€‹=â€‹âˆ¥pâ€‹3â€‹â€‹âˆ’pâ€‹2â€‹â€‹âˆ¥â€‹â€‹âˆ¥pâ€‹2â€‹â€‹âˆ’pâ€‹1â€‹â€‹âˆ¥â€‹â€‹ è¯æ¯•ã€‚ 2. Jacobian çŸ©é˜µä¸è¡Œåˆ—å¼ å‡è®¾ æ˜¯è¿™æ ·çš„ä¸€ä¸ªå‡½æ•°ï¼Œå¯¹äº ç»´å®å‘é‡ å…·æœ‰ä¸‹åˆ—å½¢å¼ï¼š å…¶ä¸­ æ˜¯ çš„å®šä¹‰åŸŸã€‚å¦‚æœå‘é‡å‡½æ•° çš„æ•°å­¦å½¢å¼ç›¸å½“çš„å¤æ‚ï¼Œä¸€èˆ¬å¯ä»¥ç”¨çº¿æ€§åŒ–è¿›è¡Œç®€åŒ–ã€‚å¯¹äºå•å˜é‡å‡½æ•° ï¼Œåœ¨ é™„è¿‘æˆ‘ä»¬å¯ä»¥ç”¨ç›´çº¿ ï¼Œè¿‘ä¼¼ ã€‚æ¨ç†è‡³å¤šå˜é‡ï¼Œä»¤ ä¸ºä¸€ä¸ªä»¿å°„å˜æ¢ï¼Œè¡¨ç¤ºå¦‚ä¸‹ï¼š å…¶ä¸­ ä¸ºä¸€ä¸ª é˜¶å®çŸ©é˜µï¼Œ ã€‚ä¸‹é¢è§£é‡Šå¦‚ä½•ä»¥ä»¿å°„å˜åŒ– è¿‘ä¼¼å‘é‡å‡½æ•° ï¼Œç”±æ­¤è¡ç”Ÿå‡º çš„å¯¼æ•°çŸ©é˜µï¼Œå³JacobiançŸ©é˜µã€‚ 2.1 Jacobian çŸ©é˜µ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ çš„é™„è¿‘èŒƒå›´ä½¿ç”¨å½¢å¼ç®€å•çš„ä»¿å°„å˜æ¢ è¿‘ä¼¼ ï¼Œå› æ­¤åœ¨ ç‚¹ï¼Œæˆ‘ä»¬æœ‰ ã€‚å› ä¸º ï¼Œåˆæœ‰ æˆ– ï¼Œæ‰€ä»¥ æ¥ç€è¦æ‰¾å‡º ä½¿å¾—ä»¿å°„å˜æ¢ åœ¨ ç‚¹é™„è¿‘æœ€è¿‘ä¼¼äº ã€‚è‡ªç„¶åœ°ï¼Œå½“ ï¼Œç¬¦åˆæœ€è¿‘ä¼¼æ¡ä»¶çš„ åº”ä½¿è¯¯å·® æ›´å¿«çš„è¶‹äº ï¼ˆé›¶å‘é‡ï¼‰ã€‚è‹¥å­˜åœ¨ä¸€ä¸ª é˜¶å®çŸ©é˜µ ä½¿å¾— æˆ‘ä»¬è¯´ åœ¨ å¯å¯¼(differentiable)ã€‚ä¸ºäº†è®© ä»ä»»æ„æ–¹å‘é€¼è¿‘ ï¼Œæˆ‘ä»¬å¦è¦æ±‚ æ˜¯å®šä¹‰åŸŸ çš„ä¸€ä¸ªå†…ç‚¹(interior point)ï¼Œæ„æ€æ˜¯ çš„é™„è¿‘ç‚¹éƒ½å±äº ã€‚ ä¸‹é¢è¯æ˜è‹¥å‘é‡å‡½æ•° åœ¨ ç‚¹å¯å¯¼ï¼Œåˆ™ ç”± å”¯ä¸€å†³å®šã€‚è€ƒè™‘ çš„æ ‡å‡†åŸºåº• ï¼Œå…¶ä¸­ ç¬¬ å…ƒç´ ä¸º1ï¼Œå…¶ä½™ä¸º0ã€‚è®¾ ä¸ºä¸€æå°æ•°ä½¿å¾— ï¼Œå±äºå®šä¹‰åŸŸ ã€‚å¦‚æœ åœ¨ å¯å¯¼ï¼Œå°±æœ‰ å› ä¸º ï¼Œå¯çŸ¥ï¼š ä¸Šå¼ç­‰å·å·¦è¾¹å³ä¸ºåå¯¼æ•° ç­‰å·å³è¾¹ç­‰äºçŸ©é˜µ çš„ç¬¬ åˆ—ï¼Œæ•…ï¼š è¿™ä¸ª é˜¶å®ä¸¾è¯ç§°ä¸ºå‘é‡å‡½æ•° åœ¨ çš„JacobiançŸ©é˜µæˆ–å¯¼æ•°çŸ©é˜µ(derivative matrix)ï¼Œè®°ä¸º ã€‚å› æ­¤ï¼Œå¯å¯¼å‡½æ•° åœ¨ çš„æœ€ä½³ä»¿å°„è¿‘ä¼¼æ˜¯ è®¾ ï¼Œåˆ™ çš„æ³°å‹’å±•å¼€å¼ä¸ºï¼š å› ä¸º ï¼Œ ä¸Šå¼æŒ‡å‡ºä»¿å°„å˜æ¢çš„æ”¹å˜é‡ æ˜¯è‡ªå˜é‡çš„æ”¹å˜é‡ çš„çº¿æ€§å‡½æ•°ã€‚JacobiançŸ©é˜µ å³ä¸ºçº¿æ€§å˜æ¢çŸ©é˜µã€‚ç›´ç™½çš„è¯´ï¼Œå½“æˆ‘ä»¬å°†éçº¿æ€§å‡½æ•°ç»™äºˆçº¿æ€§è½¬æ¢æ—¶ï¼ŒJacobiançŸ©é˜µå°±æ˜¯æè¿°è¯¥çº¿æ€§å…³ç³»çš„çŸ©é˜µã€‚ ä¸¾ä¸ªä¾‹å­ï¼šæåæ ‡ä¸å¡æ°åæ ‡çš„è½¬æ¢ï¼š å…¶ä¸­ ã€‚ JacobiançŸ©é˜µè®¡ç®—å¦‚ä¸‹ï¼š è‹¥ æ˜¯æåæ ‡å¹³é¢ä¸Šçš„ä¸€æ¡æ›²çº¿ï¼Œ æ˜¯å¡å¼åæ ‡å¹³é¢çš„æ˜ å°„æ›²çº¿ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™(chain rule)ï¼Œ å’Œ å…·æœ‰ä¸‹åˆ—å…³ç³»ï¼š ä»å‡ ä½•ä¸Šè¯´ï¼ŒJacobiançŸ©é˜µå°†æåæ ‡å¹³é¢çš„åˆ‡å‘é‡æ˜ è‡³å¡å¼åæ ‡å¹³é¢çš„åˆ‡å‘é‡ã€‚ 2.2 Jacobian è¡Œåˆ—å¼ 2.2.1 çº¿æ€§å˜æ¢å°†é¢ç§¯ä¼¸ç¼© è€ƒè™‘ä» æ˜ å°„è‡³ çš„çº¿æ€§å˜æ¢ï¼š è®¾åŒºåŸŸ ï¼Œä¸” ä»£è¡¨ ä¸­æ‰€æœ‰ç‚¹ç» æ˜ å°„åçš„é›†åˆï¼Œæ±‚ çš„é¢ç§¯ã€‚ å› ä¸ºå•ä½æ­£æ–¹å½¢å¯ç›´æ¥ç”¨ä¸¤è¾¹ å’Œ è¡¨ç¤ºï¼Œå³ è®¾ ä¸ºçº¿æ€§å˜æ¢ å‚è€ƒæ ‡å‡†åŸºåº•çš„è¡¨ç¤ºçŸ©é˜µï¼Œå°±æœ‰ ï¼Œå…¶ä¸­ ï¼Œä¸Šä¾‹ä¸­ ã€‚ç®—å‡º ç»è¿‡ æ˜ å°„çš„åƒå¦‚ä¸‹ï¼š ä¹Ÿå°±æœ‰ï¼š è¿™è¡¨æ˜ å…¶å®å°±æ˜¯ çš„åˆ—å‘é‡ å’Œ æ‰€è¡¨ç¤ºçš„å¹³è¡Œå››è¾¹å½¢ã€‚äºŒé˜¶çŸ©é˜µ çš„è¡Œåˆ—å¼ç»å¯¹å€¼ä¸º å’Œ æ‰€å½¢æˆçš„å¹³è¡Œå››è¾¹å½¢çš„é¢ç§¯ï¼Œæ•… ï¼Œè¿™é‡Œ è¡¨ç¤º çš„é¢ç§¯ã€‚ æˆ‘ä»¬æ¨å¹¿ä¸€ä¸‹ï¼Œè€ƒè™‘ ä¸ºäºŒç»´å‘é‡ å’Œ æ‰€å½¢æˆçš„å¹³è¡Œå››è¾¹å½¢ï¼Œä»¥åŒæ ·çš„æ–¹å¼å¯å¾—ï¼š å› æ­¤ å…¶å®ä¸ºå‘é‡ å’Œ æ‰€è¡¨ç¤ºçš„å¹³è¡Œå››è¾¹å½¢ã€‚è®¾ ï¼Œ ç­‰äºçŸ©é˜µç›¸ä¹˜ çš„ä¸¤è¾¹æ‰€å½¢æˆçš„å¹³è¡Œå››è¾¹å½¢ï¼Œå¯å¾—ï¼š å…¶ä¸­ ç­‰äºå¹³è¡Œå››è¾¹å½¢ çš„é¢ç§¯ï¼Œå¯å¾—åˆ°ï¼š è¿™ä¸ªç»“æœè¡¨æ˜å¹³è¡Œå››è¾¹å½¢ ç»çº¿æ€§å˜æ¢ ï¼ˆå‚è€ƒæ ‡å‡†åŸºåº•çš„çŸ©é˜µè¡¨ç¤ºä¸º ï¼‰ï¼Œé¢ç§¯ä¼¸ç¼©äº† å€ã€‚ å¦‚æœå¹³è¡Œå››è¾¹å½¢ åœ¨å¹³é¢ä¸Šç§»åŠ¨ï¼Œå‰è¿°çš„ç»“æœä¾ç„¶æˆç«‹å—ï¼Ÿè€ƒè™‘ å…¶ä¸­ ä»£è¡¨å¹³ç§»å‘é‡ã€‚è®¡ç®— å„ç‚¹ç»è¿‡æ˜ å°„çš„åƒï¼š å› æ­¤å¯ä»¥å¾—åˆ° ï¼Œå¹³ç§»é‡ å¹¶ä¸ä¼šæ”¹å˜é¢ç§¯çš„å¤§å°ï¼Œæ•…ä»»ç„¶æœ‰ ã€‚ 2.2.2 Jacobian è¡Œåˆ—å¼æ„ä¹‰ è‹¥ æ˜¯å¯å¯¼å‡½æ•°ï¼Œåˆ™Jacobianæ˜¯ä¸€ä¸ª é˜¶çŸ©é˜µï¼Œå› æ­¤å¯è®¡ç®—è¡Œåˆ—å¼ã€‚ä¸ºæ–¹ä¾¿è¯´æ˜ï¼Œè®¾ ï¼Œå‘é‡å‡½æ•° å°† æ˜ å°„è‡³ ï¼Œåˆ™ çš„Jacobianè¡Œåˆ—å¼ä¸ºï¼š ä»¤ è¡¨ç¤º å’Œ è¡¨ç¤ºé•¿æ–¹å½¢ï¼Œå…¶ä¸­ å’Œ éƒ½æ˜¯å¾®å°çš„é‡ã€‚è‹¥ å’Œ è¶³å¤Ÿæ¥è¿‘0ï¼Œåˆ™ è¿‘ä¼¼å¦‚ä¸‹é¢å‘é‡æ‰€å½¢æˆçš„å¹³è¡Œå››è¾¹å½¢ï¼š ä»¤ ä»£è¡¨å¹³è¡Œå››è¾¹å½¢ çš„é¢ç§¯ã€‚å› ä¸ºäºŒé˜¶è¡Œåˆ—å¼çš„è¡Œå‘é‡æ‰€å½¢æˆçš„å¹³è¡Œå››è¾¹å½¢é¢ç§¯ç­‰äºè¡Œåˆ—å¼çš„ç»å¯¹å€¼ã€‚ æ‰€ä»¥ï¼Œå¾®å°åŒºåŸŸ ç»å‘é‡å‡½æ•° æ˜ å°„è‡³ ï¼Œå…¶é¢ç§¯ä¼¸ç¼©äº† å€ã€‚ 2.2.3 Jacobian è¡Œåˆ—å¼ç”¨é€” Jacobianè¡Œåˆ—å¼æœ€ä¸»è¦çš„åº”ç”¨åœ¨å¤šé‡ç§¯åˆ†çš„æ¢å…ƒç§¯åˆ†æ³•(integration by substitution)ã€‚ä»¤ ä¸ºä¸€ä¸ªè¿ç»­å®å‡½æ•°ï¼Œä¸” å’Œ æ˜¯ä¸€å¯¹ä¸€å¯å¯¼å‡½æ•°ã€‚æ ¹æ®ä¸Šè¿°é¢ç§¯å˜åŒ–å…³ç³»ï¼Œå¯å¾—åˆ°ä¸‹é¢çš„å˜æ¢ç§¯åˆ†å…¬å¼ï¼š 3. Jacobian ä¸ Hessian è®¾ ä¸ºäºŒæ¬¡å¯å¯¼å‡½æ•°ï¼Œ é˜¶å®å¯¹ç§°çŸ©é˜µ ç§°ä¸º çš„Hessianã€‚å®šä¹‰å…¥ä¸‹ï¼š åˆ å…³äº çš„æ¢¯åº¦(gradient) å®šä¹‰ä¸ºä¸€ä¸ª ç»´å‘é‡ï¼Œå…¶ä¸­ç¬¬ ä¸ªå…ƒç´ æ˜¯ å¯¹ çš„ä¸€æ¬¡åå¯¼æ•°ï¼Œå³ï¼š æ¢¯åº¦ æ˜¯ä¸€ä¸ªå‘é‡å‡½æ•°ï¼Œ çš„JacobiançŸ©é˜µå¦‚ä¸‹ï¼š å› æ­¤è¯æ˜æ¢¯åº¦ çš„Jacobianå³ä¸ºHessiançŸ©é˜µã€‚ 4. å‚è€ƒ å‘¨å¿—æˆï¼šã€Šçº¿æ€§ä»£æ•°ã€‹ï¼Œå›½ç«‹äº¤é€šå¤§å­¦å‡ºç‰ˆç¤¾ "},"math/graph.html":{"url":"math/graph.html","title":"å›¾çš„åŸºæœ¬æ¦‚å¿µ","keywords":"","body":"å›¾çš„åŸºæœ¬æ¦‚å¿µ äº 2016 å¹´ 06 æœˆ 20 æ—¥ å›¾çš„åŸºæœ¬æ¦‚å¿µ æœ‰å‘å›¾ä¸æ— å‘å›¾ å®Œå…¨å›¾ã€ç¨€ç–å›¾ã€ç¨ å¯†å›¾ é¡¶ç‚¹ä¸é¡¶ç‚¹ã€é¡¶ç‚¹ä¸è¾¹çš„å…³ç³» é¡¶ç‚¹çš„åº¦æ•°åŠåº¦åºåˆ— ä¸é¡¶ç‚¹åº¦æ•°æœ‰å…³çš„æ¦‚å¿µ åº¦åºåˆ—ä¸Havel-Hakimiå®šç† äºŒéƒ¨å›¾ä¸å®Œå…¨äºŒéƒ¨å›¾ å›¾çš„åŒæ„ å­å›¾ä¸ç”Ÿæˆæ ‘ è·¯å¾„ è¿é€šæ€§ æƒå€¼ã€æœ‰å‘ç½‘ä¸æ— å‘ç½‘ å‚è€ƒæ–‡æ¡£ æœ‰å‘å›¾ä¸æ— å‘å›¾ å›¾ï¼ˆGraphï¼‰æ˜¯ç”±é¡¶ç‚¹é›†åˆå’Œé¡¶ç‚¹é—´çš„äºŒå…ƒå…³ç³»é›†åˆï¼ˆå³è¾¹çš„é›†åˆæˆ–å¼§çš„é›†åˆï¼‰ç»„æˆçš„æ•°æ®ç»“æ„ï¼Œé€šå¸¸å¯ä»¥ç”¨Gï¼ˆVï¼ŒEï¼‰æ¥è¡¨ç¤ºã€‚å…¶ä¸­é¡¶ç‚¹é›†åˆï¼ˆVertext Setï¼‰å’Œè¾¹çš„é›†åˆï¼ˆEdge Setï¼‰åˆ†åˆ«ç”¨Vï¼ˆGï¼‰å’ŒEï¼ˆGï¼‰è¡¨ç¤ºã€‚Vï¼ˆGï¼‰ä¸­çš„å…ƒç´ ç§°ä¸ºé¡¶ç‚¹ï¼ˆVertexï¼‰ï¼Œç”¨uã€vç­‰ç¬¦å·è¡¨ç¤ºï¼›é¡¶ç‚¹ä¸ªæ•°ç§°ä¸ºå›¾çš„é˜¶ï¼ˆOrderï¼‰ï¼Œé€šå¸¸ç”¨nè¡¨ç¤ºã€‚Eï¼ˆGï¼‰ä¸­çš„å…ƒç´ ç§°ä¸ºè¾¹ï¼ˆEdgeï¼‰ï¼Œç”¨eç­‰ç¬¦å·è¡¨ç¤ºï¼›è¾¹çš„ä¸ªæ•°ç§°ä¸ºå›¾çš„è¾¹æ•°ï¼ˆSizeï¼‰ï¼Œé€šå¸¸ç”¨mè¡¨ç¤ºã€‚ ä¾‹å¦‚ï¼Œå›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„å›¾å¯ä»¥è¡¨ç¤ºä¸ºG1ï¼ˆVï¼ŒEï¼‰ã€‚å…¶ä¸­ï¼Œé¡¶ç‚¹é›†åˆVï¼ˆG1ï¼‰ï¼ï½›1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï½ï¼Œé›†åˆä¸­çš„å…ƒç´ ä¸ºé¡¶ç‚¹ï¼ˆç”¨åºå·ä»£è¡¨ï¼Œåœ¨å…¶ä»–å›¾ä¸­ï¼Œé¡¶ç‚¹é›†åˆä¸­çš„å…ƒç´ ä¹Ÿå¯ä»¥æ˜¯å…¶ä»–æ ‡è¯†é¡¶ç‚¹çš„ç¬¦å·ï¼Œå¦‚å­—æ¯Aã€Bã€Cç­‰ï¼‰ï¼›è¾¹çš„é›†åˆä¸ºï¼š Eï¼ˆG1ï¼‰ï¼ï½›ï¼ˆ1ï¼Œ2ï¼‰ï¼Œï¼ˆ1ï¼Œ3ï¼‰ï¼Œï¼ˆ2ï¼Œ3ï¼‰ï¼Œï¼ˆ2ï¼Œ4ï¼‰ï¼Œï¼ˆ2ï¼Œ5ï¼‰ï¼Œï¼ˆ2ï¼Œ6ï¼‰ï¼Œï¼ˆ3ï¼Œ4ï¼‰ï¼Œï¼ˆ3ï¼Œ5ï¼‰ï¼Œï¼ˆ4ï¼Œ5ï¼‰ï½ åœ¨ä¸Šè¿°è¾¹çš„é›†åˆä¸­ï¼Œæ¯ä¸ªå…ƒç´ ï¼ˆuï¼Œvï¼‰ä¸ºä¸€å¯¹é¡¶ç‚¹æ„æˆçš„æ— åºå¯¹ï¼ˆç”¨åœ†æ‹¬å·æ‹¬èµ·æ¥ï¼‰ï¼Œè¡¨ç¤ºä¸é¡¶ç‚¹uå’Œvç›¸å…³è”çš„ä¸€æ¡æ— å‘è¾¹ï¼ˆUndirected Edgeï¼‰ï¼Œè¿™æ¡è¾¹æ²¡æœ‰ç‰¹å®šçš„æ–¹å‘ï¼Œå› æ­¤ï¼ˆuï¼Œvï¼‰ä¸ï¼ˆvï¼Œuï¼‰æ˜¯åŒä¸€æ¡çš„è¾¹ã€‚å¦‚æœå›¾ä¸­æ‰€æœ‰çš„è¾¹éƒ½æ²¡æœ‰æ–¹å‘æ€§ï¼Œè¿™ç§å›¾ç§°ä¸ºæ— å‘å›¾ï¼ˆUndirected Graphï¼‰ã€‚ å›¾1ï¼ˆbï¼‰æ‰€ç¤ºçš„å›¾å¯ä»¥è¡¨ç¤ºä¸ºG2ï¼ˆVï¼ŒEï¼‰ï¼Œå…¶ä¸­é¡¶ç‚¹é›†åˆVï¼ˆG2ï¼‰ï¼ï½›1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï¼Œ7ï½ï¼Œé›†åˆä¸­çš„å…ƒç´ ä¹Ÿä¸ºé¡¶ç‚¹çš„åºå·ï¼›è¾¹çš„é›†åˆä¸ºï¼š Eï¼ˆG2ï¼‰ï¼ï½›ï¼œ1ï¼Œ2ï¼ï¼Œï¼œ2ï¼Œ3ï¼ï¼Œï¼œ2ï¼Œ5ï¼ï¼Œï¼œ2ï¼Œ6ï¼ï¼Œï¼œ3ï¼Œ5ï¼ï¼Œï¼œ4ï¼Œ3ï¼ï¼Œï¼œ5ï¼Œ2ï¼ï¼Œï¼œ5ï¼Œ4ï¼ï¼Œï¼œ6ï¼Œ7ï¼ï½ åœ¨ä¸Šè¿°è¾¹çš„é›†åˆä¸­ï¼Œæ¯ä¸ªå…ƒç´ ï¼œuï¼Œvï¼ä¸ºä¸€å¯¹é¡¶ç‚¹æ„æˆçš„æœ‰åºå¯¹ï¼ˆç”¨å°–æ‹¬å·æ‹¬èµ·æ¥ï¼‰ï¼Œè¡¨ç¤ºä»é¡¶ç‚¹uåˆ°é¡¶ç‚¹vçš„æœ‰å‘è¾¹ï¼ˆDirected Edgeï¼‰ï¼Œå…¶ä¸­uæ˜¯è¿™æ¡æœ‰å‘è¾¹çš„èµ·å§‹é¡¶ç‚¹ï¼ˆStart Vertexï¼‰ï¼Œç®€ç§°èµ·ç‚¹ï¼Œvæ˜¯è¿™æ¡æœ‰å‘è¾¹çš„ç»ˆæ­¢é¡¶ç‚¹ï¼ˆEnd Vertexï¼‰ï¼Œç®€ç§°ç»ˆç‚¹ï¼Œè¿™æ¡è¾¹æœ‰ç‰¹å®šçš„æ–¹å‘ï¼Œç”±uæŒ‡å‘vï¼Œå› æ­¤ï¼œuï¼Œvï¼ä¸ï¼œvï¼Œuï¼æ˜¯ä¸¤æ¡ä¸åŒçš„è¾¹ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆbï¼‰æœ‰å‘å›¾G2ä¸­ï¼Œï¼œ2ï¼Œ5ï¼å’Œï¼œ5ï¼Œ2ï¼æ˜¯ä¸¤æ¡ä¸åŒçš„è¾¹ã€‚å¦‚æœå›¾ä¸­æ‰€æœ‰çš„è¾¹éƒ½æ˜¯æœ‰æ–¹å‘æ€§çš„ï¼Œè¿™ç§å›¾ç§°ä¸ºæœ‰å‘å›¾ï¼ˆDirected Graphï¼‰ã€‚æœ‰å‘å›¾ä¸­çš„è¾¹ä¹Ÿå¯ä»¥ç§°ä¸ºå¼§ï¼ˆArcï¼‰ã€‚æœ‰å‘å›¾ä¹Ÿå¯ä»¥è¡¨ç¤ºæˆDï¼ˆVï¼ŒAï¼‰ï¼Œå…¶ä¸­Aä¸ºå¼§çš„é›†åˆã€‚ æœ‰å‘å›¾çš„åŸºå›¾ï¼ˆGround Graphï¼‰ï¼šå¿½ç•¥æœ‰å‘å›¾æ‰€æœ‰è¾¹çš„æ–¹å‘ï¼Œå¾—åˆ°çš„æ— å‘å›¾ç§°ä¸ºè¯¥æœ‰å‘å›¾çš„åŸºå›¾ã€‚ä¾‹å¦‚ï¼Œå›¾1ï¼ˆcï¼‰æ‰€ç¤ºä¸ºå›¾1ï¼ˆbï¼‰ä¸­æœ‰å‘å›¾G2çš„åŸºå›¾ã€‚ è¯´æ˜ï¼šå¦‚æœä¸€ä¸ªå›¾ä¸­æŸäº›è¾¹å…·æœ‰æ–¹å‘æ€§ï¼Œè€Œå…¶ä»–è¾¹æ²¡æœ‰æ–¹å‘æ€§ï¼Œè¿™ç§å›¾å¯ä»¥ç§°ä¸ºæ··åˆå›¾ï¼ˆMixed Graphï¼‰ï¼› å®Œå…¨å›¾ã€ç¨€ç–å›¾ã€ç¨ å¯†å›¾ è®¸å¤šå›¾è®ºç®—æ³•çš„å¤æ‚åº¦éƒ½ä¸å›¾ä¸­é¡¶ç‚¹ä¸ªæ•°næˆ–è¾¹çš„æ•°ç›®mæœ‰å…³ï¼Œç”šè‡³mä¸nÃ—ï¼ˆnï¼1ï¼‰ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ä¹Ÿä¼šå½±å“å›¾è®ºç®—æ³•çš„é€‰æ‹©ã€‚ä¸‹é¢ä»‹ç»å‡ ä¸ªä¸é¡¶ç‚¹ä¸ªæ•°ã€è¾¹çš„æ•°ç›®ç›¸å…³çš„æ¦‚å¿µã€‚ å®Œå…¨å›¾ï¼ˆComplete Graphï¼‰ï¼šå¦‚æœæ— å‘å›¾ä¸­ä»»ä½•ä¸€å¯¹é¡¶ç‚¹ä¹‹é—´éƒ½æœ‰ä¸€æ¡è¾¹ï¼Œè¿™ç§æ— å‘å›¾ç§°ä¸ºå®Œå…¨å›¾ã€‚åœ¨å®Œå…¨å›¾ä¸­ï¼Œé˜¶æ•°å’Œè¾¹æ•°å­˜åœ¨å…³ç³»å¼ï¼šmï¼nÃ—ï¼ˆnï¼1ï¼‰ï¼2ã€‚ä¾‹å¦‚ï¼Œå›¾2ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾å°±æ˜¯å®Œå…¨å›¾ã€‚é˜¶ä¸ºnçš„å®Œå…¨å›¾ç”¨Knè¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå›¾2ï¼ˆaï¼‰æ‰€ç¤ºçš„å®Œå…¨å›¾ä¸º4é˜¶å®Œå…¨å›¾K4ã€‚ æœ‰å‘å®Œå…¨å›¾ï¼ˆDirected Complete Graphï¼‰ï¼šå¦‚æœæœ‰å‘å›¾ä¸­ä»»ä½•ä¸€å¯¹é¡¶ç‚¹uå’Œvï¼Œéƒ½å­˜åœ¨ï¼œuï¼Œvï¼å’Œï¼œvï¼Œuï¼ä¸¤æ¡æœ‰å‘è¾¹ï¼Œè¿™ç§æœ‰å‘å›¾ç§°ä¸ºæœ‰å‘å®Œå…¨å›¾ã€‚åœ¨æœ‰å‘å®Œå…¨å›¾ä¸­ï¼Œé˜¶æ•°å’Œè¾¹æ•°å­˜åœ¨å…³ç³»å¼ï¼šmï¼nÃ—ï¼ˆnï¼1ï¼‰ã€‚ä¾‹å¦‚ï¼Œå›¾2ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾å°±æ˜¯æœ‰å‘å®Œå…¨å›¾ã€‚ ç¨€ç–å›¾ï¼ˆSparse Graphï¼‰ï¼šè¾¹æˆ–å¼§çš„æ•°ç›®ç›¸å¯¹è¾ƒå°‘ï¼ˆè¿œå°äºnÃ—ï¼ˆnï¼1ï¼‰ï¼‰çš„å›¾ç§°ä¸ºç¨€ç–å›¾ã€‚æœ‰çš„æ–‡çŒ®è®¤ä¸ºï¼Œè¾¹æˆ–å¼§çš„æ•°ç›®mï¼œnlogï¼ˆnï¼‰çš„æ— å‘å›¾æˆ–æœ‰å‘å›¾ï¼Œç§°ä¸ºç¨€ç–å›¾ã€‚ä¾‹å¦‚ï¼Œå›¾3ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾å¯ä»¥ç§°ä¸ºç¨€ç–å›¾ã€‚ ç¨ å¯†å›¾ï¼ˆDense Graphï¼‰ï¼šè¾¹æˆ–å¼§çš„æ•°ç›®ç›¸å¯¹è¾ƒå¤šçš„å›¾ï¼ˆæ¥è¿‘äºå®Œå…¨å›¾æˆ–æœ‰å‘å®Œå…¨å›¾ï¼‰ç§°ä¸ºç¨ å¯†å›¾ã€‚ä¾‹å¦‚ï¼Œå›¾3ï¼ˆbï¼‰æ‰€ç¤ºçš„æ— å‘å›¾å¯ä»¥ç§°ä¸ºç¨ å¯†å›¾ã€‚ å¹³å‡¡å›¾ï¼ˆTrivial Graphï¼‰ï¼šåªæœ‰ä¸€ä¸ªé¡¶ç‚¹çš„å›¾ï¼Œå³é˜¶nï¼1çš„å›¾ç§°ä¸ºå¹³å‡¡å›¾ã€‚ç›¸åï¼Œé˜¶nï¼1çš„å›¾ç§°ä¸ºéå¹³å‡¡å›¾ï¼ˆNontrivial Graphï¼‰ã€‚ é›¶å›¾ï¼ˆNull Graphï¼‰ï¼šè¾¹çš„é›†åˆEï¼ˆGï¼‰ä¸ºç©ºçš„å›¾ï¼Œç§°ä¸ºé›¶å›¾ã€‚ é¡¶ç‚¹ä¸é¡¶ç‚¹ã€é¡¶ç‚¹ä¸è¾¹çš„å…³ç³» åœ¨æ— å‘å›¾å’Œæœ‰å‘å›¾ä¸­ï¼Œé¡¶ç‚¹ä¸é¡¶ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠé¡¶ç‚¹ä¸è¾¹çš„å…³ç³»æ˜¯é€šè¿‡â€œé‚»æ¥ï¼ˆAdjacencyï¼‰â€è¿™ä¸ªæ¦‚å¿µæ¥è¡¨ç¤ºçš„ã€‚ åœ¨æ— å‘å›¾Gï¼ˆVï¼ŒEï¼‰ä¸­ï¼Œå¦‚æœï¼ˆuï¼Œvï¼‰æ˜¯Eï¼ˆGï¼‰ä¸­çš„å…ƒç´ ï¼Œå³ï¼ˆuï¼Œvï¼‰æ˜¯å›¾ä¸­çš„ä¸€æ¡æ— å‘è¾¹ï¼Œåˆ™ç§°é¡¶ç‚¹uä¸é¡¶ç‚¹väº’ä¸ºé‚»æ¥é¡¶ç‚¹ï¼ˆAdjacent Vertexï¼‰ï¼Œè¾¹ï¼ˆuï¼Œvï¼‰ä¾é™„äºï¼ˆAttach Toï¼‰é¡¶ç‚¹uå’Œvï¼Œæˆ–ç§°è¾¹ï¼ˆuï¼Œvï¼‰ä¸é¡¶ç‚¹uå’Œvç›¸å…³è”ï¼ˆIncidentï¼‰ã€‚æ­¤å¤–ï¼Œç§°æœ‰ä¸€ä¸ªå…±åŒé¡¶ç‚¹çš„ä¸¤æ¡ä¸åŒè¾¹ä¸ºé‚»æ¥è¾¹ï¼ˆAdjacent Edgeï¼‰ã€‚ ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1ä¸­ï¼Œä¸é¡¶ç‚¹2ç›¸é‚»æ¥çš„é¡¶ç‚¹æœ‰1ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï¼Œè€Œä¾é™„äºé¡¶ç‚¹2çš„è¾¹æœ‰ï¼ˆ2ï¼Œ1ï¼‰ï¼Œï¼ˆ2ï¼Œ3ï¼‰ï¼Œï¼ˆ2ï¼Œ4ï¼‰ï¼Œï¼ˆ2ï¼Œ5ï¼‰å’Œï¼ˆ2ï¼Œ6ï¼‰ã€‚ åœ¨æœ‰å‘å›¾Gï¼ˆVï¼ŒEï¼‰ä¸­ï¼Œå¦‚æœï¼œuï¼Œvï¼æ˜¯Eï¼ˆGï¼‰ä¸­çš„å…ƒç´ ï¼Œå³ï¼œuï¼Œvï¼æ˜¯å›¾ä¸­çš„ä¸€æ¡æœ‰å‘è¾¹ï¼Œåˆ™ç§°é¡¶ç‚¹ué‚»æ¥åˆ°ï¼ˆAdjacent Toï¼‰é¡¶ç‚¹vï¼Œé¡¶ç‚¹vé‚»æ¥è‡ªï¼ˆAdjacent Fromï¼‰é¡¶ç‚¹uï¼Œè¾¹ï¼œuï¼Œvï¼ä¸é¡¶ç‚¹uå’Œvç›¸å…³è”ã€‚ ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾G2ä¸­ï¼Œé¡¶ç‚¹2åˆ†åˆ«é‚»æ¥åˆ°é¡¶ç‚¹3ï¼Œ5ï¼Œ6ï¼Œé‚»æ¥è‡ªé¡¶ç‚¹1å’Œ5ï¼›æœ‰å‘è¾¹ï¼œ2ï¼Œ6ï¼çš„é¡¶ç‚¹2é‚»æ¥åˆ°é¡¶ç‚¹6ï¼Œé¡¶ç‚¹6é‚»æ¥è‡ªé¡¶ç‚¹2ï¼›é¡¶ç‚¹2åˆ†åˆ«ä¸è¾¹ï¼œ2ï¼Œ3ï¼ï¼Œï¼œ2ï¼Œ5ï¼ï¼Œï¼œ2ï¼Œ6ï¼ï¼Œï¼œ1ï¼Œ2ï¼å’Œï¼œ5ï¼Œ2ï¼ç›¸å…³è”ç­‰ã€‚ é¡¶ç‚¹çš„åº¦æ•°åŠåº¦åºåˆ— ä¸é¡¶ç‚¹åº¦æ•°æœ‰å…³çš„æ¦‚å¿µ é¡¶ç‚¹çš„åº¦æ•°ï¼ˆDegreeï¼‰ï¼šä¸€ä¸ªé¡¶ç‚¹uçš„åº¦æ•°æ˜¯ä¸å®ƒç›¸å…³è”çš„è¾¹çš„æ•°ç›®ï¼Œè®°ä½œdegï¼ˆuï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1ä¸­ï¼Œé¡¶ç‚¹2çš„åº¦æ•°ä¸º5ï¼Œé¡¶ç‚¹5çš„åº¦æ•°ä¸º3ç­‰ã€‚ åœ¨æœ‰å‘å›¾ä¸­ï¼Œé¡¶ç‚¹çš„åº¦æ•°ç­‰äºè¯¥é¡¶ç‚¹çš„å‡ºåº¦ä¸å…¥åº¦ä¹‹å’Œã€‚å…¶ä¸­ï¼Œé¡¶ç‚¹uçš„å‡ºåº¦ï¼ˆOutdegreeï¼‰æ˜¯ä»¥uä¸ºèµ·å§‹é¡¶ç‚¹çš„æœ‰å‘è¾¹ï¼ˆå³ä»é¡¶ç‚¹uå‡ºå‘çš„æœ‰å‘è¾¹ï¼‰çš„æ•°ç›®ï¼Œè®°ä½œodï¼ˆuï¼‰ï¼›é¡¶ç‚¹uçš„å…¥åº¦ï¼ˆIndegreeï¼‰æ˜¯ä»¥uä¸ºç»ˆç‚¹çš„æœ‰å‘è¾¹ï¼ˆå³è¿›å…¥åˆ°é¡¶ç‚¹uçš„æœ‰å‘è¾¹ï¼‰çš„æ•°ç›®ï¼Œè®°ä½œidï¼ˆuï¼‰ã€‚é¡¶ç‚¹uçš„åº¦æ•°ï¼šdegï¼ˆuï¼‰ï¼odï¼ˆuï¼‰ï¼‹idï¼ˆuï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾G2ä¸­ï¼Œé¡¶ç‚¹2çš„å‡ºåº¦ä¸º3ï¼Œå…¥åº¦ä¸º2ï¼Œåº¦æ•°ä¸ºï¼š3ï¼‹2ï¼5ã€‚ åœ¨æ— å‘å›¾å’Œæœ‰å‘å›¾ä¸­ï¼Œè¾¹æ•°må’Œæ‰€æœ‰é¡¶ç‚¹åº¦æ•°æ€»å’Œéƒ½å­˜åœ¨å¦‚ä¸‹å…³ç³»ã€‚ å®šç†1ã€€åœ¨æ— å‘å›¾å’Œæœ‰å‘å›¾ä¸­ï¼Œæ‰€æœ‰é¡¶ç‚¹åº¦æ•°æ€»å’Œï¼Œç­‰äºè¾¹æ•°çš„ä¸¤å€ï¼Œå³ï¼š è¿™æ˜¯å› ä¸ºï¼Œä¸ç®¡æ˜¯æœ‰å‘å›¾è¿˜æ˜¯æ— å‘å›¾ï¼Œåœ¨ç»Ÿè®¡æ‰€æœ‰é¡¶ç‚¹åº¦æ•°æ€»å’Œæ—¶ï¼Œæ¯æ¡è¾¹éƒ½ç»Ÿè®¡äº†ä¸¤æ¬¡ã€‚ å¶ç‚¹ä¸å¥‡ç‚¹ï¼šä¸ºæ–¹ä¾¿èµ·è§ï¼ŒæŠŠåº¦æ•°ä¸ºå¶æ•°çš„é¡¶ç‚¹ç§°ä¸ºå¶ç‚¹ï¼ˆEven Vertexï¼‰ï¼ŒæŠŠåº¦æ•°ä¸ºå¥‡æ•°çš„é¡¶ç‚¹ç§°ä¸ºå¥‡ç‚¹ï¼ˆOdd Vertexï¼‰ã€‚ æ¨è®º1ã€€æ¯ä¸ªå›¾éƒ½æœ‰å¶æ•°ä¸ªå¥‡ç‚¹ã€‚ å­¤ç«‹é¡¶ç‚¹ï¼ˆIsolated Vertexï¼‰ï¼šåº¦æ•°ä¸º0çš„é¡¶ç‚¹ï¼Œç§°ä¸ºå­¤ç«‹é¡¶ç‚¹ã€‚å­¤ç«‹é¡¶ç‚¹ä¸ä¸å…¶ä»–ä»»ä½•é¡¶ç‚¹é‚»æ¥ã€‚ å¶ï¼ˆLeafï¼‰ï¼šåº¦æ•°ä¸º1çš„é¡¶ç‚¹ï¼Œç§°ä¸ºå¶é¡¶ç‚¹ï¼Œä¹Ÿç§°å¶é¡¶ç‚¹ï¼ˆLeaf Vertexï¼‰æˆ–ç«¯ç‚¹ï¼ˆEnd Vertexï¼‰ã€‚å…¶ä»–é¡¶ç‚¹ç§°ä¸ºéå¶é¡¶ç‚¹ã€‚ å›¾Gçš„æœ€å°åº¦ï¼ˆMinimum Degreeï¼‰ï¼šå›¾Gæ‰€æœ‰é¡¶ç‚¹çš„æœ€å°çš„åº¦æ•°ï¼Œè®°ä¸ºÎ´ï¼ˆGï¼‰ã€‚ å›¾Gçš„æœ€å¤§åº¦ï¼ˆMaximum Degreeï¼‰ï¼šå›¾Gæ‰€æœ‰é¡¶ç‚¹çš„æœ€å¤§çš„åº¦æ•°ï¼Œè®°ä¸ºâˆ†ï¼ˆGï¼‰ã€‚ ä¾‹å¦‚ï¼Œå›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾æ²¡æœ‰å­¤ç«‹é¡¶ç‚¹ï¼Œé¡¶ç‚¹6ä¸ºå¶é¡¶ç‚¹ï¼›Î´ï¼ˆGï¼‰ï¼1ï¼Œâˆ†ï¼ˆGï¼‰ï¼4ï¼›ç­‰ã€‚ åº¦åºåˆ—ä¸Havel-Hakimiå®šç† åº¦åºåˆ—ï¼ˆDegree Sequenceï¼‰ï¼šè‹¥æŠŠå›¾Gæ‰€æœ‰é¡¶ç‚¹çš„åº¦æ•°æ’æˆä¸€ä¸ªåºåˆ—sï¼Œåˆ™ç§°sä¸ºå›¾Gçš„åº¦åºåˆ—ã€‚ä¾‹å¦‚ï¼Œå›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1çš„åº¦åºåˆ—ä¸º sï¼š2ï¼Œ5ï¼Œ4ï¼Œ3ï¼Œ3ï¼Œ1ï¼›æˆ–sâ€²ï¼š1ï¼Œ2ï¼Œ3ï¼Œ3ï¼Œ4ï¼Œ5ï¼›æˆ–sâ€³ï¼š5ï¼Œ4ï¼Œ3ï¼Œ3ï¼Œ2ï¼Œ1ã€‚ å…¶ä¸­åºåˆ—sæ˜¯æŒ‰é¡¶ç‚¹åºå·æ’åºçš„ï¼Œåºåˆ—sâ€²æ˜¯æŒ‰åº¦æ•°éå‡é¡ºåºæ’åˆ—çš„ï¼Œåºåˆ—sâ€³æ˜¯æŒ‰åº¦æ•°éå¢é¡ºåºæ’åˆ—çš„ã€‚ç»™å®šä¸€ä¸ªå›¾ï¼Œç¡®å®šå®ƒçš„åº¦åºåˆ—å¾ˆç®€å•ï¼Œä½†æ˜¯å…¶é€†é—®é¢˜å¹¶ä¸å®¹æ˜“ï¼Œå³ç»™å®šä¸€ä¸ªç”±éè´Ÿæ•´æ•°ç»„æˆçš„æœ‰é™åºåˆ—sï¼Œåˆ¤æ–­sæ˜¯å¦æ˜¯æŸä¸ªå›¾çš„åº¦åºåˆ—ã€‚ åºåˆ—æ˜¯å¯å›¾çš„ï¼ˆGraphicï¼‰ï¼šä¸€ä¸ªéè´Ÿæ•´æ•°ç»„æˆçš„æœ‰é™åºåˆ—å¦‚æœæ˜¯æŸä¸ªæ— å‘å›¾çš„åº¦åºåˆ—ï¼Œåˆ™ç§°è¯¥åºåˆ—æ˜¯å¯å›¾çš„ã€‚åˆ¤å®šä¸€ä¸ªåºåˆ—æ˜¯å¦æ˜¯å¯å›¾çš„ï¼Œæœ‰ä»¥ä¸‹Havel-Hakimiå®šç†ã€‚ å®šç†2ï¼ˆHavel-Hakimiå®šç†ï¼‰ã€€ç”±éè´Ÿæ•´æ•°ç»„æˆçš„éå¢åºåˆ—sï¼šd1ï¼Œd2ï¼Œâ€¦ï¼Œdnï¼ˆnâ‰¥2ï¼Œd1â‰¥1ï¼‰æ˜¯å¯å›¾çš„ï¼Œå½“ä¸”ä»…å½“åºåˆ— æ˜¯å¯å›¾çš„ã€‚åºåˆ—s1ä¸­æœ‰nï¼1ä¸ªéè´Ÿæ•´æ•°ï¼Œsåºåˆ—ä¸­d1åçš„å‰d1ä¸ªåº¦æ•°ï¼ˆå³d2ï½dd1ï¼‹1ï¼‰å‡1åæ„æˆs1ä¸­çš„å‰d1ä¸ªæ•°ã€‚ ä¾‹å¦‚ï¼Œåˆ¤æ–­åºåˆ—sï¼š7ï¼Œ7ï¼Œ4ï¼Œ3ï¼Œ3ï¼Œ3ï¼Œ2ï¼Œ1æ˜¯å¦æ˜¯å¯å›¾çš„ã€‚åˆ é™¤åºåˆ—sçš„é¦–é¡¹7ï¼Œå¯¹å…¶åçš„7é¡¹æ¯é¡¹å‡1ï¼Œå¾—åˆ°ï¼š6ï¼Œ3ï¼Œ2ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ0ã€‚ç»§ç»­åˆ é™¤åºåˆ—çš„é¦–é¡¹6ï¼Œå¯¹å…¶åçš„6é¡¹æ¯é¡¹å‡1ï¼Œå¾—åˆ°ï¼š2ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ-1ï¼Œåˆ°è¿™ä¸€æ­¥å‡ºç°äº†è´Ÿæ•°ã€‚ç”±äºå›¾ä¸­ä¸å¯èƒ½å­˜åœ¨è´Ÿåº¦æ•°çš„é¡¶ç‚¹ï¼Œå› æ­¤è¯¥åºåˆ—ä¸æ˜¯å¯å›¾çš„ã€‚ å†ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œåˆ¤æ–­åºåˆ—sï¼š5ï¼Œ4ï¼Œ3ï¼Œ3ï¼Œ2ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ1ï¼Œ1æ˜¯å¦æ˜¯å¯å›¾çš„ã€‚åˆ é™¤åºåˆ—sçš„é¦–é¡¹5ï¼Œå¯¹å…¶åçš„5é¡¹æ¯é¡¹å‡1ï¼Œå¾—åˆ°ï¼š3ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ1ï¼Œ2ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œé‡æ–°æ’åºåä¸ºï¼š3ï¼Œ2ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ã€‚ç»§ç»­åˆ é™¤åºåˆ—çš„é¦–é¡¹3ï¼Œå¯¹å…¶åçš„3é¡¹æ¯é¡¹å‡1ï¼Œå¾—åˆ°ï¼š1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ã€‚å¦‚æ­¤å†é™†ç»­å¾—åˆ°åºåˆ—ï¼š1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ0ï¼›1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ0ï¼›1ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0ï¼›0ï¼Œ0ï¼Œ0ï¼Œ0ã€‚ç”±æ­¤å¯åˆ¤å®šè¯¥åºåˆ—æ˜¯å¯å›¾çš„ã€‚ Havel-Hakimiå®šç†å®é™…ä¸Šç»™å‡ºäº†æ ¹æ®ä¸€ä¸ªåºåˆ—sæ„é€ å›¾ï¼ˆæˆ–åˆ¤å®šsä¸æ˜¯å¯å›¾çš„ï¼‰çš„æ–¹æ³•ï¼šæŠŠåºåˆ—sæŒ‰ç…§éå¢é¡ºåºæ’åºä»¥åï¼Œå…¶é¡ºåºä¸ºd1ï¼Œd2ï¼Œâ€¦ï¼Œdnï¼›åº¦æ•°æœ€å¤§çš„é¡¶ç‚¹è®¾ä¸ºv1ï¼Œå°†å®ƒä¸åº¦æ•°æ¬¡å¤§çš„å‰d1ä¸ªé¡¶ç‚¹ä¹‹é—´è¿è¾¹ï¼Œç„¶åè¿™ä¸ªé¡¶ç‚¹å°±å¯ä»¥ä¸ç®¡äº†ï¼Œå³åœ¨åºåˆ—ä¸­åˆ é™¤é¦–é¡¹d1ï¼Œå¹¶æŠŠåé¢çš„d1ä¸ªåº¦æ•°å‡1ï¼›å†æŠŠå‰©ä¸‹çš„åºåˆ—é‡æ–°æŒ‰éå¢é¡ºåºæ’åºï¼ŒæŒ‰ç…§ä¸Šè¿°è¿‡ç¨‹è¿è¾¹ï¼›â€¦â€¦ï¼›ç›´åˆ°å»ºå‡ºå®Œæ•´çš„å›¾ï¼Œæˆ–å‡ºç°è´Ÿåº¦æ•°ç­‰æ˜æ˜¾ä¸åˆç†çš„æƒ…å†µä¸ºæ­¢ã€‚ ä¾‹å¦‚ï¼Œå¯¹åºåˆ—sï¼š3ï¼Œ3ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ1æ„é€ å›¾ï¼Œè®¾åº¦æ•°ä»å¤§åˆ°å°çš„6ä¸ªé¡¶ç‚¹ä¸ºv1ï½v6ã€‚é¦–å…ˆv1ä¸v2ã€v3ã€v4è¿ä¸€æ¡è¾¹ï¼Œå¦‚å›¾4ï¼ˆaï¼‰æ‰€ç¤ºï¼›å‰©ä¸‹çš„åºåˆ—ä¸º2ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ã€‚å¦‚æœåé¢4ä¸ª1å¯¹åº”é¡¶ç‚¹v3ã€v4ã€v5ã€v6ï¼Œåˆ™åº”è¯¥åœ¨v2ä¸v3ã€v2ä¸v4ä¹‹é—´è¿è¾¹ï¼Œæœ€ååœ¨v5ä¸v6ä¹‹é—´è¿è¾¹ï¼Œå¦‚å›¾4ï¼ˆbï¼‰æ‰€ç¤ºã€‚å¦‚æœåé¢4ä¸ª1å¯¹åº”é¡¶ç‚¹v5ã€v6ã€v3ã€v4ï¼Œåˆ™åº”è¯¥åœ¨v2ä¸v5ã€v2ä¸v6ä¹‹é—´è¿è¾¹ï¼Œæœ€ååœ¨v3ä¸v4ä¹‹é—´è¿è¾¹ï¼Œå¦‚å›¾4ï¼ˆcï¼‰æ‰€ç¤ºã€‚å¯è§ï¼Œç”±åŒä¸€ä¸ªå¯å›¾çš„åºåˆ—æ„é€ å‡ºæ¥çš„å›¾ä¸ä¸€å®šæ˜¯å”¯ä¸€çš„ã€‚ äºŒéƒ¨å›¾ä¸å®Œå…¨äºŒéƒ¨å›¾ äºŒéƒ¨å›¾ï¼ˆBipartite Graphï¼‰ï¼šè®¾æ— å‘å›¾ä¸ºGï¼ˆVï¼ŒEï¼‰ï¼Œå®ƒçš„é¡¶ç‚¹é›†åˆVåŒ…å«ä¸¤ä¸ªæ²¡æœ‰å…¬å…±å…ƒç´ çš„å­é›†ï¼šXï¼ï½›x1ï¼Œx2ï¼Œâ€¦ï¼Œxsï½å’ŒYï¼ï½›y1ï¼Œy2ï¼Œâ€¦ï¼Œytï½ï¼Œå…ƒç´ ä¸ªæ•°åˆ†åˆ«ä¸ºså’Œtï¼›å¹¶ä¸”xiä¸xjä¹‹é—´ï¼ˆ1â‰¤iï¼Œjâ‰¤sï¼‰ã€ylä¸yrä¹‹é—´ï¼ˆ1â‰¤lï¼Œrâ‰¤tï¼‰æ²¡æœ‰è¾¹è¿æ¥ï¼Œåˆ™ç§°Gä¸ºäºŒéƒ¨å›¾ï¼Œæœ‰çš„æ–‡çŒ®ä¹Ÿç§°ä¸ºäºŒåˆ†å›¾ã€‚ ä¾‹å¦‚ï¼Œå›¾5ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾å°±æ˜¯ä¸€ä¸ªäºŒéƒ¨å›¾ã€‚ å®Œå…¨äºŒéƒ¨å›¾ï¼ˆComplete Bipartite Graphï¼‰ï¼šåœ¨äºŒéƒ¨å›¾Gä¸­ï¼Œå¦‚æœé¡¶ç‚¹é›†åˆXä¸­æ¯ä¸ªé¡¶ç‚¹xiä¸é¡¶ç‚¹é›†åˆYä¸­æ¯ä¸ªé¡¶ç‚¹yléƒ½æœ‰è¾¹ç›¸è¿ï¼Œåˆ™ç§°Gä¸ºå®Œå…¨äºŒéƒ¨å›¾ï¼Œè®°ä¸ºKsï¼Œtï¼Œså’Œtåˆ†åˆ«ä¸ºé›†åˆXå’Œé›†åˆYä¸­çš„é¡¶ç‚¹ä¸ªæ•°ã€‚åœ¨å®Œå…¨äºŒéƒ¨å›¾Ksï¼Œtä¸­ä¸€å…±æœ‰sÃ—tæ¡è¾¹ã€‚ ä¾‹å¦‚ï¼Œå¦‚å›¾5ï¼ˆbï¼‰æ‰€ç¤ºçš„K2ï¼Œ3å’Œå›¾5ï¼ˆcï¼‰æ‰€ç¤ºçš„K3ï¼Œ3éƒ½æ˜¯å®Œå…¨äºŒéƒ¨å›¾ã€‚ è§‚å¯Ÿå›¾6ï¼ˆaï¼‰å’Œå›¾6ï¼ˆcï¼‰æ‰€ç¤ºä¸¤ä¸ªå›¾ï¼Œè¡¨é¢ä¸Šçœ‹èµ·æ¥è¿™ä¸¤ä¸ªå›¾éƒ½ä¸æ˜¯äºŒéƒ¨å›¾ã€‚ä½†ä»”ç»†è§‚å¯Ÿï¼Œå‘ç°å›¾6ï¼ˆaï¼‰ä¸­3ä¸ªé»‘è‰²é¡¶ç‚¹äº’ä¸ç›¸é‚»ï¼Œ3ä¸ªç™½è‰²é¡¶ç‚¹ä¹Ÿäº’ä¸ç›¸é‚»ï¼Œæ¯ä¸ªé»‘è‰²é¡¶ç‚¹éƒ½ä¸3ä¸ªç™½è‰²é¡¶ç‚¹ç›¸é‚»ï¼Œå› æ­¤å›¾6ï¼ˆaï¼‰å®é™…ä¸Šä¹Ÿæ˜¯K3ï¼Œ3ï¼Œå¦‚å›¾6ï¼ˆbï¼‰æ‰€ç¤ºã€‚åŒæ ·ï¼Œå›¾6ï¼ˆcï¼‰ä¸­4ä¸ªé»‘è‰²é¡¶ç‚¹äº’ä¸ç›¸é‚»ï¼Œ4ä¸ªç™½è‰²é¡¶ç‚¹ä¹Ÿäº’ä¸ç›¸é‚»ï¼Œå¯¹è¿™8ä¸ªé¡¶ç‚¹è¿›è¡Œç¼–å·åï¼Œé‡æ–°ç”»æˆå›¾6ï¼ˆdï¼‰æ‰€ç¤ºçš„å›¾ï¼Œå‘ç°å›¾6ï¼ˆcï¼‰å®é™…ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªäºŒéƒ¨å›¾ã€‚ ä¸€ä¸ªå›¾æ˜¯å¦ä¸ºäºŒéƒ¨å›¾ï¼Œå¯ç”±ä¸‹é¢çš„å®šç†åˆ¤åˆ«ã€‚ å®šç†3ã€€ä¸€ä¸ªæ— å‘å›¾Gæ˜¯äºŒéƒ¨å›¾å½“ä¸”ä»…å½“Gä¸­æ— å¥‡æ•°é•¿åº¦çš„å›è·¯ã€‚ å›¾çš„åŒæ„ ä»å›¾6å¯çŸ¥ï¼Œæœ‰äº›å›¾ä¹‹é—´çœ‹èµ·æ¥å·®åˆ«å¾ˆå¤§ï¼Œæ¯”å¦‚å›¾6ï¼ˆaï¼‰å’Œå›¾6ï¼ˆbï¼‰ã€å›¾6ï¼ˆcï¼‰å’Œå›¾6ï¼ˆdï¼‰ï¼Œä½†ç»è¿‡æ”¹ç”»åï¼Œå®ƒä»¬å®é™…ä¸Šæ˜¯åŒä¸€ä¸ªå›¾ã€‚ åˆå¦‚ï¼Œå›¾7ï¼ˆaï¼‰å’Œå›¾7ï¼ˆbï¼‰ä¸¤ä¸ªå›¾è¡¨é¢ä¸Šçœ‹å·®åˆ«ä¹Ÿå¾ˆå¤§ï¼Œä½†æ˜¯å¯¹å›¾7ï¼ˆbï¼‰æŒ‰ç…§å›¾ä¸­çš„é¡ºåºç»™æ¯ä¸ªé¡¶ç‚¹ç¼–å·åå‘ç°ï¼Œè¿™ä¸¤ä¸ªå›¾å®é™…ä¸Šä¹Ÿæ˜¯åŒä¸€ä¸ªå›¾ã€‚ å›¾çš„åŒæ„ï¼ˆIsomorphismï¼‰ï¼šè®¾æœ‰ä¸¤ä¸ªå›¾G1å’ŒG2ï¼Œå¦‚æœè¿™ä¸¤ä¸ªå›¾åŒºåˆ«ä»…åœ¨äºå›¾çš„ç”»æ³•ä¸ï¼ˆæˆ–ï¼‰é¡¶ç‚¹çš„æ ‡å·æ–¹å¼ï¼Œåˆ™ç§°å®ƒä»¬æ˜¯åŒæ„çš„ã€‚æ„æ€å°±æ˜¯è¯´è¿™ä¸¤ä¸ªå›¾æ˜¯åŒä¸€ä¸ªå›¾ã€‚å…³äºåŒæ„çš„ä¸¥æ ¼å®šä¹‰ï¼Œè¯·å‚è€ƒå…¶ä»–æ•™æã€‚ å­å›¾ä¸ç”Ÿæˆæ ‘ è®¾æœ‰ä¸¤ä¸ªå›¾Gï¼ˆVï¼ŒEï¼‰å’ŒGâ€²ï¼ˆVâ€²ï¼ŒEâ€²ï¼‰ï¼Œå¦‚æœVâ€²âŠ†Vï¼Œä¸”Eâ€²âŠ†Eï¼Œåˆ™ç§°å›¾Gâ€²æ˜¯å›¾Gçš„å­å›¾ï¼ˆSubgraphï¼‰ã€‚ä¾‹å¦‚ï¼Œå›¾8ï¼ˆaï¼‰ã€å›¾8ï¼ˆbï¼‰æ‰€ç¤ºçš„æ— å‘å›¾éƒ½æ˜¯å›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1çš„å­å›¾ï¼Œè€Œå›¾8ï¼ˆcï¼‰ã€å›¾8ï¼ˆdï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾éƒ½æ˜¯å›¾1ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾G2çš„å­å›¾ã€‚ ç”Ÿæˆæ ‘ï¼ˆSpanning Treeï¼‰ï¼šä¸€ä¸ªæ— å‘è¿é€šå›¾çš„ç”Ÿæˆæ ‘æ˜¯å®ƒçš„åŒ…å«æ‰€æœ‰é¡¶ç‚¹çš„æå°è¿é€šå­å›¾ï¼Œè¿™é‡Œæ‰€è°“çš„æå°å°±æ˜¯è¾¹çš„æ•°ç›®æå°ã€‚å¦‚æœå›¾ä¸­æœ‰nä¸ªé¡¶ç‚¹ï¼Œåˆ™ç”Ÿæˆæ ‘æœ‰nï¼1æ¡è¾¹ã€‚ä¸€ä¸ªæ— å‘è¿é€šå›¾å¯èƒ½æœ‰å¤šä¸ªç”Ÿæˆæ ‘ã€‚ ä¾‹å¦‚ï¼Œå›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1çš„ä¸€ä¸ªç”Ÿæˆæ ‘å¦‚å›¾9ï¼ˆaï¼‰æ‰€ç¤ºã€‚ä¸ºäº†æ›´å½¢è±¡åœ°è¡¨ç¤ºè¿™ä¸ªç”Ÿæˆæ ‘ï¼Œåœ¨å›¾9ï¼ˆbï¼‰ä¸­æŠŠå®ƒç”»æˆäº†ä»¥é¡¶ç‚¹1ä¸ºæ ¹ç»“ç‚¹çš„æ ‘ï¼Œåœ¨å›¾9ï¼ˆcï¼‰ä¸­æŠŠå®ƒç”»æˆäº†ä»¥é¡¶ç‚¹3ä¸ºæ ¹ç»“ç‚¹çš„æ ‘ã€‚ è§‚å¯Ÿå›¾10ï¼Œå…¶ä¸­å›¾10ï¼ˆbï¼‰å’Œå›¾10ï¼ˆcï¼‰éƒ½æ˜¯å›¾10ï¼ˆaï¼‰çš„å­å›¾ï¼Œè¿™ä¸¤ä¸ªå­å›¾çš„é¡¶ç‚¹é›†ç›¸åŒï¼Œä¸ºVâ€²ï¼ï½›2ï¼Œ3ï¼Œ4ï¼Œ5ï½ï¼Œä½†è¾¹é›†ä¸ç›¸åŒã€‚å›¾10ï¼ˆbï¼‰ä¿ç•™äº†åŸå›¾ä¸­Vâ€²å†…å„é¡¶ç‚¹é—´çš„è¾¹ï¼Œè€Œåœ¨å›¾10ï¼ˆcï¼‰ä¸­ï¼ŒåŸå›¾çš„è¾¹ï¼ˆ3ï¼Œ5ï¼‰å’Œï¼ˆ3ï¼Œ2ï¼‰è¢«å»æ‰äº†ã€‚å› æ­¤æœ‰å¿…è¦è¿›ä¸€æ­¥è®¨è®ºå­å›¾ã€‚ è®¾å›¾Gâ€²ï¼ˆVâ€²ï¼ŒEâ€²ï¼‰æ˜¯å›¾Gï¼ˆVï¼ŒEï¼‰çš„å­å›¾ï¼Œä¸”å¯¹äºVâ€²ä¸­çš„ä»»æ„ä¸¤ä¸ªé¡¶ç‚¹uå’Œvï¼Œåªè¦ï¼ˆuï¼Œvï¼‰æ˜¯Gä¸­çš„è¾¹ï¼Œåˆ™ä¸€å®šæ˜¯Gâ€²ä¸­çš„è¾¹ï¼Œæ­¤æ—¶ç§°å›¾Gâ€²ä¸ºç”±é¡¶ç‚¹é›†åˆVâ€²è¯±å¯¼çš„Gçš„å­å›¾ï¼ˆSubgraph of G Induced By Vâ€²ï¼‰ï¼Œç®€ç§°ä¸ºé¡¶ç‚¹è¯±å¯¼å­å›¾ï¼ˆVertex-Induced Subgraphï¼‰ï¼Œè®°ä¸ºGï¼»Vâ€²ï¼½ã€‚æ ¹æ®å®šä¹‰ï¼Œåœ¨å›¾10ä¸­ï¼Œå›¾10ï¼ˆbï¼‰æ˜¯ç”±Vâ€²ï¼ï½›2ï¼Œ3ï¼Œ4ï¼Œ5ï½è¯±å¯¼çš„å­å›¾ï¼Œå›¾10ï¼ˆcï¼‰å’Œå›¾10ï¼ˆdï¼‰éƒ½ä¸æ˜¯é¡¶ç‚¹è¯±å¯¼å­å›¾ã€‚ ç±»ä¼¼åœ°ï¼Œå¯¹äºå›¾Gçš„ä¸€ä¸ªéç©ºçš„è¾¹é›†åˆEâ€²ï¼Œç”±è¾¹é›†åˆEâ€²è¯±å¯¼çš„Gçš„å­å›¾æ˜¯ä»¥Eâ€²ä½œä¸ºè¾¹é›†ï¼Œä»¥è‡³å°‘ä¸Eâ€²ä¸­ä¸€æ¡è¾¹å…³è”çš„é‚£äº›é¡¶ç‚¹æ„æˆé¡¶ç‚¹é›†Vâ€²ï¼Œè¿™ä¸ªå­å›¾Gâ€²ï¼ˆVâ€²ï¼ŒEâ€²ï¼‰ç§°ä¸ºæ˜¯Gçš„ä¸€ä¸ªè¾¹è¯±å¯¼å­å›¾ï¼ˆEdge-Induced Subgraphï¼‰ï¼Œè®°ä¸ºGï¼»Eâ€²ï¼½ã€‚æ ¹æ®å®šä¹‰ï¼Œåœ¨å›¾10ä¸­ï¼Œå›¾10ï¼ˆbï¼‰ã€å›¾10ï¼ˆcï¼‰å’Œå›¾10ï¼ˆdï¼‰éƒ½æ˜¯è¾¹è¯±å¯¼å­å›¾ã€‚ è¯´æ˜ï¼šç”±äºè¾¹å¿…é¡»ä¾é™„äºé¡¶ç‚¹è€Œå­˜åœ¨ï¼Œæ‰€ä»¥å¯¹äºâ€œæŸæ¡è¾¹å±äºå­å›¾ï¼Œä½†è¯¥è¾¹æŸä¸ªé¡¶ç‚¹ä¸å±äºå­å›¾â€çš„æƒ…å½¢ï¼Œæ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚ è·¯å¾„ è·¯å¾„æ˜¯å›¾è®ºä¸­ä¸€ä¸ªå¾ˆé‡è¦çš„æ¦‚å¿µã€‚åœ¨å›¾Gï¼ˆVï¼ŒEï¼‰ä¸­ï¼Œè‹¥ä»é¡¶ç‚¹viå‡ºå‘ï¼Œæ²¿ç€ä¸€äº›è¾¹ç»è¿‡ä¸€äº›é¡¶ç‚¹vp1ï¼Œvp2ï¼Œâ€¦ï¼Œvpmï¼Œåˆ°è¾¾é¡¶ç‚¹vjï¼Œåˆ™ç§°é¡¶ç‚¹åºåˆ—ï¼ˆviï¼Œvp1ï¼Œvp2ï¼Œâ€¦ï¼Œvpmï¼Œvjï¼‰ä¸ºä»é¡¶ç‚¹viåˆ°é¡¶ç‚¹vjçš„ä¸€æ¡è·¯å¾„ï¼ˆPathï¼‰ï¼Œæˆ–ç§°ä¸ºé€šè·¯ï¼Œå…¶ä¸­ï¼ˆviï¼Œvp1ï¼‰ï¼Œï¼ˆvp1ï¼Œvp2ï¼‰ï¼Œâ€¦ï¼Œï¼ˆvpmï¼Œvjï¼‰ä¸ºå›¾Gä¸­çš„è¾¹ã€‚å¦‚æœGæ˜¯æœ‰å‘å›¾ï¼Œåˆ™ï¼œviï¼Œvp1ï¼ï¼Œï¼œvp1ï¼Œvp2ï¼ï¼Œâ€¦ï¼Œï¼œvpmï¼Œvjï¼ä¸ºå›¾Gä¸­çš„æœ‰å‘è¾¹ã€‚ è·¯å¾„é•¿åº¦ï¼ˆLengthï¼‰ï¼šè·¯å¾„ä¸­è¾¹çš„æ•°ç›®é€šå¸¸ç§°ä¸ºè·¯å¾„çš„é•¿åº¦ã€‚ ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1ä¸­ï¼Œé¡¶ç‚¹åºåˆ—ï¼ˆ1ï¼Œ2ï¼Œ5ï¼Œ4ï¼‰æ˜¯ä»é¡¶ç‚¹1åˆ°é¡¶ç‚¹4çš„è·¯å¾„ï¼Œè·¯å¾„é•¿åº¦ä¸º3ï¼Œå…¶ä¸­ï¼ˆ1ï¼Œ2ï¼‰ï¼Œï¼ˆ2ï¼Œ5ï¼‰ï¼Œï¼ˆ5ï¼Œ4ï¼‰éƒ½æ˜¯å›¾G1ä¸­çš„è¾¹ï¼›å¦å¤–ï¼Œé¡¶ç‚¹åºåˆ—ï¼ˆ1ï¼Œ3ï¼Œ4ï¼‰ä¹Ÿæ˜¯ä»é¡¶ç‚¹1åˆ°é¡¶ç‚¹4çš„è·¯å¾„ï¼Œè·¯å¾„é•¿åº¦ä¸º2ã€‚ åœ¨å›¾1ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾G2ä¸­ï¼Œé¡¶ç‚¹åºåˆ—ï¼ˆ3ï¼Œ5ï¼Œ2ï¼Œ6ï¼‰æ˜¯ä»é¡¶ç‚¹3åˆ°é¡¶ç‚¹6çš„è·¯å¾„ï¼Œè·¯å¾„é•¿åº¦ä¸º3ï¼Œå…¶ä¸­ï¼œ3ï¼Œ5ï¼ï¼Œï¼œ5ï¼Œ2ï¼ï¼Œï¼œ2ï¼Œ6ï¼éƒ½æ˜¯å›¾G2ä¸­çš„æœ‰å‘è¾¹ï¼›è€Œä»é¡¶ç‚¹7åˆ°é¡¶ç‚¹1æ²¡æœ‰è·¯å¾„ã€‚ ç®€å•è·¯å¾„ï¼ˆSimple Pathï¼‰ï¼šè‹¥è·¯å¾„ä¸Šå„é¡¶ç‚¹viï¼Œvp1ï¼Œvp2ï¼Œâ€¦ï¼Œvpmï¼Œvjå‡äº’ç›¸ä¸é‡å¤ï¼Œåˆ™è¿™æ ·çš„è·¯å¾„ç§°ä¸ºç®€å•è·¯å¾„ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1ä¸­ï¼Œè·¯å¾„ï¼ˆ1ï¼Œ2ï¼Œ5ï¼Œ4ï¼‰å°±æ˜¯ä¸€æ¡ç®€å•è·¯å¾„ã€‚ å›è·¯ï¼ˆCircuitï¼‰ï¼šè‹¥è·¯å¾„ä¸Šç¬¬ä¸€ä¸ªé¡¶ç‚¹viä¸æœ€åä¸€ä¸ªé¡¶ç‚¹vjé‡åˆï¼Œåˆ™ç§°è¿™æ ·çš„è·¯å¾„ä¸ºå›è·¯ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ä¸­ï¼Œå›¾G1ä¸­çš„è·¯å¾„ï¼ˆ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ2ï¼‰å’Œå›¾G2ä¸­çš„è·¯å¾„ï¼ˆ5ï¼Œ4ï¼Œ3ï¼Œ5ï¼‰éƒ½æ˜¯å›è·¯ã€‚å›è·¯ä¹Ÿç§°ä¸ºç¯ï¼ˆLoopï¼‰ã€‚ ç®€å•å›è·¯ï¼ˆSimple Circuitï¼‰ï¼šé™¤ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªé¡¶ç‚¹å¤–ï¼Œæ²¡æœ‰é¡¶ç‚¹é‡å¤çš„å›è·¯ç§°ä¸ºç®€å•å›è·¯ã€‚ç®€å•å›è·¯ä¹Ÿç§°ä¸ºåœˆï¼ˆCycleï¼‰ã€‚é•¿åº¦ä¸ºå¥‡æ•°çš„åœˆç§°ä¸ºå¥‡åœˆï¼ˆOdd Cycleï¼‰ï¼Œé•¿åº¦ä¸ºå¶æ•°çš„åœˆç§°ä¸ºå¶åœˆï¼ˆEven Cycleï¼‰ã€‚ è¿é€šæ€§ è¿é€šæ€§ä¹Ÿæ˜¯å›¾è®ºä¸­ä¸€ä¸ªå¾ˆé‡è¦çš„æ¦‚å¿µã€‚åœ¨æ— å‘å›¾ä¸­ï¼Œè‹¥ä»é¡¶ç‚¹uåˆ°væœ‰è·¯å¾„ï¼Œåˆ™ç§°é¡¶ç‚¹uå’Œvæ˜¯è¿é€šï¼ˆConnectedï¼‰çš„ã€‚å¦‚æœæ— å‘å›¾ä¸­ä»»æ„ä¸€å¯¹é¡¶ç‚¹éƒ½æ˜¯è¿é€šçš„ï¼Œåˆ™ç§°æ­¤å›¾æ˜¯è¿é€šå›¾ï¼ˆConnected Graphï¼‰ï¼›ç›¸åï¼Œå¦‚æœä¸€ä¸ªæ— å‘å›¾ä¸æ˜¯è¿é€šå›¾ï¼Œåˆ™ç§°ä¸ºéè¿é€šå›¾ï¼ˆDisconnected Graphï¼‰ã€‚ å¦‚æœä¸€ä¸ªæ— å‘å›¾ä¸æ˜¯è¿é€šçš„ï¼Œåˆ™å…¶æå¤§è¿é€šå­å›¾ç§°ä¸ºè¿é€šåˆ†é‡ï¼ˆConnected Componentï¼‰ï¼Œè¿™é‡Œæ‰€è°“çš„æå¤§æ˜¯æŒ‡å­å›¾ä¸­åŒ…å«çš„é¡¶ç‚¹ä¸ªæ•°æå¤§ã€‚ ä¾‹å¦‚ï¼Œå›¾1ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘å›¾G1å°±æ˜¯ä¸€ä¸ªè¿é€šå›¾ã€‚åœ¨å›¾G1ä¸­ï¼Œå¦‚æœå»æ‰è¾¹ï¼ˆ2ï¼Œ6ï¼‰ï¼Œåˆ™å‰©ä¸‹çš„å›¾å°±æ˜¯éè¿é€šçš„ï¼Œä¸”åŒ…å«ä¸¤ä¸ªè¿é€šåˆ†é‡ï¼Œä¸€ä¸ªæ˜¯ç”±é¡¶ç‚¹1ã€2ã€3ã€4ã€5ç»„æˆçš„è¿é€šåˆ†é‡ï¼Œå¦ä¸€ä¸ªæ˜¯ç”±é¡¶ç‚¹6æ„æˆçš„è¿é€šåˆ†é‡ã€‚ åˆå¦‚ï¼Œå›¾11æ‰€ç¤ºçš„æ— å‘å›¾ä¹Ÿæ˜¯éè¿é€šå›¾ã€‚å…¶ä¸­é¡¶ç‚¹1ã€2ã€3å’Œ5æ„æˆä¸€ä¸ªè¿é€šåˆ†é‡ï¼Œé¡¶ç‚¹4ã€6ã€7å’Œ8æ„æˆå¦ä¸€ä¸ªè¿é€šåˆ†é‡ã€‚ åœ¨æœ‰å‘å›¾ä¸­ï¼Œè‹¥å¯¹æ¯ä¸€å¯¹é¡¶ç‚¹uå’Œvï¼Œæ—¢å­˜åœ¨ä»uåˆ°vçš„è·¯å¾„ï¼Œä¹Ÿå­˜åœ¨ä»våˆ°uçš„è·¯å¾„ï¼Œåˆ™ç§°æ­¤æœ‰å‘å›¾ä¸ºå¼ºè¿é€šå›¾ï¼ˆStrongly Connected Digraphï¼‰ã€‚ä¾‹å¦‚ï¼Œå›¾12ï¼ˆaï¼‰å’Œå›¾12ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾å°±æ˜¯å¼ºè¿é€šå›¾ã€‚ å¯¹äºéå¼ºè¿é€šå›¾ï¼Œå…¶æå¤§å¼ºè¿é€šå­å›¾ç§°ä¸ºå…¶å¼ºè¿é€šåˆ†é‡ï¼ˆStrongly Connected Componentï¼‰ã€‚ä¾‹å¦‚ï¼Œå›¾13ï¼ˆaï¼‰æ‰€ç¤ºçš„æœ‰å‘å›¾G2å°±æ˜¯éå¼ºè¿é€šå›¾ï¼Œå®ƒåŒ…å«3ä¸ªå¼ºè¿é€šåˆ†é‡ï¼Œå¦‚å›¾13ï¼ˆbï¼‰æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œé¡¶ç‚¹2ã€3ã€4ã€5æ„æˆä¸€ä¸ªå¼ºè¿é€šåˆ†é‡ï¼Œåœ¨è¿™ä¸ªå­å›¾ä¸­ï¼Œæ¯ä¸€å¯¹é¡¶ç‚¹uå’Œvï¼Œæ—¢å­˜åœ¨ä»uåˆ°vçš„è·¯å¾„ï¼Œä¹Ÿå­˜åœ¨ä»våˆ°uçš„è·¯å¾„ï¼›é¡¶ç‚¹1ã€6ã€8ä¹Ÿæ„æˆä¸€ä¸ªå¼ºè¿é€šåˆ†é‡ï¼Œé¡¶ç‚¹7è‡ªæˆä¸€ä¸ªå¼ºè¿é€šåˆ†é‡ã€‚ æƒå€¼ã€æœ‰å‘ç½‘ä¸æ— å‘ç½‘ æƒå€¼ï¼ˆWeightï¼‰ï¼šæŸäº›å›¾çš„è¾¹å…·æœ‰ä¸å®ƒç›¸å…³çš„æ•°ï¼Œç§°ä¸ºæƒå€¼ã€‚è¿™äº›æƒå€¼å¯ä»¥è¡¨ç¤ºä»ä¸€ä¸ªé¡¶ç‚¹åˆ°å¦ä¸€ä¸ªé¡¶ç‚¹çš„è·ç¦»ã€èŠ±è´¹çš„ä»£ä»·ã€æ‰€éœ€çš„æ—¶é—´ç­‰ã€‚å¦‚æœä¸€ä¸ªå›¾ï¼Œå…¶æ‰€æœ‰è¾¹éƒ½å…·æœ‰æƒå€¼ï¼Œåˆ™ç§°ä¸ºåŠ æƒå›¾ï¼ˆWeighted Graphï¼‰ï¼Œæˆ–è€…ç§°ä¸ºç½‘ç»œï¼ˆNetï¼‰ã€‚æ ¹æ®ç½‘ç»œä¸­çš„è¾¹æ˜¯å¦å…·æœ‰æ–¹å‘æ€§ï¼Œåˆå¯ä»¥åˆ†ä¸ºæœ‰å‘ç½‘ï¼ˆDirected Netï¼‰å’Œæ— å‘ç½‘ï¼ˆUndirected Netï¼‰ã€‚ç½‘ç»œä¹Ÿå¯ä»¥ç”¨Gï¼ˆVï¼ŒEï¼‰è¡¨ç¤ºï¼Œå…¶ä¸­è¾¹çš„é›†åˆEä¸­æ¯ä¸ªå…ƒç´ åŒ…å«3ä¸ªåˆ†é‡ï¼šè¾¹çš„ä¸¤ä¸ªé¡¶ç‚¹å’Œæƒå€¼ã€‚ ä¾‹å¦‚ï¼Œå›¾14ï¼ˆaï¼‰æ‰€ç¤ºçš„æ— å‘ç½‘å¯è¡¨ç¤ºä¸ºG1ï¼ˆVï¼ŒEï¼‰ï¼Œå…¶ä¸­é¡¶ç‚¹é›†åˆVï¼ˆG1ï¼‰ï¼ï½›1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï¼Œ7ï½ï¼›è¾¹çš„é›†åˆä¸ºï¼š Eï¼ˆG1ï¼‰ï¼ï½›ï¼ˆ1ï¼Œ2ï¼Œ28ï¼‰ï¼Œï¼ˆ1ï¼Œ6ï¼Œ10ï¼‰ï¼Œï¼ˆ2ï¼Œ3ï¼Œ16ï¼‰ï¼Œï¼ˆ2ï¼Œ7ï¼Œ14ï¼‰ï¼Œï¼ˆ3ï¼Œ4ï¼Œ12ï¼‰ï¼Œï¼ˆ4ï¼Œ5ï¼Œ22ï¼‰ï¼Œï¼ˆ4ï¼Œ7ï¼Œ18ï¼‰ï¼Œï¼ˆ5ï¼Œ6ï¼Œ25ï¼‰ï¼Œï¼ˆ5ï¼Œ7ï¼Œ24ï¼‰ï½ åœ¨è¾¹çš„é›†åˆä¸­ï¼Œæ¯ä¸ªå…ƒç´ çš„ç¬¬3ä¸ªåˆ†é‡è¡¨ç¤ºè¯¥è¾¹çš„æƒå€¼ã€‚ å¦‚å›¾14ï¼ˆbï¼‰æ‰€ç¤ºçš„æœ‰å‘ç½‘å¯ä»¥è¡¨ç¤ºä¸ºG2ï¼ˆVï¼ŒEï¼‰ï¼Œå…¶ä¸­é¡¶ç‚¹é›†åˆVï¼ˆG1ï¼‰ï¼ï½›1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï¼Œ7ï½ï¼›è¾¹çš„é›†åˆä¸ºï¼š Eï¼ˆG2ï¼‰ï¼ï½›ï¼œ1ï¼Œ2ï¼Œ12ï¼ï¼Œï¼œ2ï¼Œ4ï¼Œ85ï¼ï¼Œï¼œ3ï¼Œ2ï¼Œ43ï¼ï¼Œï¼œ4ï¼Œ3ï¼Œ65ï¼ï¼Œï¼œ5ï¼Œ1ï¼Œ58ï¼ï¼Œï¼œ5ï¼Œ2ï¼Œ90ï¼ï¼Œï¼œ5ï¼Œ6ï¼Œ19ï¼ï¼Œï¼œ5ï¼Œ7ï¼Œ70ï¼ï¼Œï¼œ6ï¼Œ4ï¼Œ24ï¼ï¼Œï¼œ7ï¼Œ6ï¼Œ50ï¼ï½ åŒæ ·åœ¨è¾¹çš„é›†åˆä¸­ï¼Œæ¯ä¸ªå…ƒç´ çš„ç¬¬3ä¸ªåˆ†é‡ä¹Ÿè¡¨ç¤ºè¯¥è¾¹çš„æƒå€¼ã€‚ å‚è€ƒæ–‡æ¡£ å›¾è®ºç®—æ³•ç†è®ºã€å®ç°åŠåº”ç”¨ ç¬¬ä¸€ç«  å›¾çš„åŸºæœ¬æ¦‚å¿µåŠå›¾çš„å­˜å‚¨ Name: Email: Site: Â© 2013 On the Road | designed by veganshe | on FarBox to Top "},"other/":{"url":"other/","title":"å…¶ä»–","keywords":"","body":" åˆ¤åˆ«å™¨è®­ç»ƒ ç½‘ç»œFLOPSè®¡ç®— "},"other/discriminator_train.html":{"url":"other/discriminator_train.html","title":"åˆ¤åˆ«å™¨è®­ç»ƒ","keywords":"","body":"ç”Ÿæˆå¯¹æŠ—ç½‘ç»œGANå¦‚æœåªè®­ç»ƒä¸€ä¸ªç½‘ç»œä¼šæœ‰æ•ˆæœä¹ˆï¼Ÿ ä½œè€…ï¼šChuang é“¾æ¥ï¼šhttps://www.zhihu.com/question/276070438/answer/825816888 æ¥æºï¼šçŸ¥ä¹ è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ è¿™å…¶å®æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„é—®é¢˜ã€‚åœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œå¦‚æœæŠŠåˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰è®­ç»ƒå¾—å¤ªå¥½äº†ï¼Œçœ‹ä¼¼èƒ½å¤Ÿåœ¨å¯¹æŠ—ä¸­æ›´åŠ æœ‰æ•ˆçš„æ‹’ç»ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ç”Ÿæˆçš„å‡æ ·æœ¬ï¼Œä½†æ˜¯å…¶å®ä¸€æ ·ä¼šäº§ç”Ÿè¯¸å¤šé—®é¢˜ã€‚ åˆ¤åˆ«å™¨æœ€ä¸»è¦çš„ä½œç”¨å°±æ˜¯ä¸ºç”Ÿæˆå™¨æä¾›ä¸‹é™æ¢¯åº¦ã€‚å¦‚æœåˆ¤åˆ«å™¨å¤ªå·®ï¼Œåˆ™æ— æ³•æä¾›æœ‰æ•ˆçš„æ¢¯åº¦ï¼ŒåŒæ—¶åˆ¤åˆ«å™¨è¶Šå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è¶Šä¸¥é‡ã€‚ å›é¡¾ä¸€ä¸‹ï¼ŒåŸå§‹GANä¸­åˆ¤åˆ«å™¨è¦æœ€å°åŒ–å¦‚ä¸‹æŸå¤±å‡½æ•°ï¼Œå°½å¯èƒ½æŠŠçœŸå®æ ·æœ¬åˆ†ä¸ºæ­£ä¾‹ï¼Œç”Ÿæˆæ ·æœ¬åˆ†ä¸ºè´Ÿä¾‹ï¼š ï¼ˆå…¬å¼1ï¼‰ å…¶ä¸­æ˜¯çœŸå®æ ·æœ¬åˆ†å¸ƒï¼Œæ˜¯ç”±ç”Ÿæˆå™¨äº§ç”Ÿçš„æ ·æœ¬åˆ†å¸ƒã€‚ å‡è®¾æˆ‘ä»¬å¦‚æœè¦å­¦ä¹ å¾—åˆ°ä¸€ä¸ªæœ€ä¼˜ç½‘ç»œï¼Œå¿…ç„¶ä¸Šå¼æ±‚å¯¼ç­‰äº0ï¼Œåˆ™ä¼šå¾—åˆ°ï¼š (å…¬å¼2) ä»å…¬å¼ä¸Šå°±ä¸éš¾çœ‹å‡ºï¼Œæ­¤æ—¶æœ€ä¼˜çš„åˆ¤åˆ«å™¨ï¼Œå°±æ˜¯åˆ¤æ–­ä¸ºçœŸå®å›¾åƒæ¦‚ç‡å¯¹äºåˆ¤æ–­ä¸ºçœŸå’Œå‡çš„æ¦‚ç‡å’Œçš„æ¯”å€¼ã€‚å½“ä¸”ï¼Œæœ€ä¼˜åˆ¤åˆ«å™¨å°±åº”è¯¥éå¸¸è‡ªä¿¡åœ°ç»™å‡ºæ¦‚ç‡0ï¼›å¦‚æœè¯´æ˜è¯¥æ ·æœ¬æ˜¯çœŸæ˜¯å‡çš„å¯èƒ½æ€§åˆšå¥½ä¸€åŠä¸€åŠï¼Œæ­¤æ—¶æœ€ä¼˜åˆ¤åˆ«å™¨ä¹Ÿåº”è¯¥ç»™å‡ºæ¦‚ç‡0.5ã€‚ åœ¨æç«¯æƒ…å†µâ€”â€”åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶ï¼Œç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°å˜æˆä»€ä¹ˆã€‚ç»™æŸå¤±å‡½æ•°åŠ ä¸Šä¸€ä¸ªä¸ä¾èµ–äºç”Ÿæˆå™¨çš„é¡¹ï¼Œä½¿ä¹‹å˜æˆ ï¼ˆå…¬å¼3ï¼‰ æ³¨æ„ï¼Œå®ƒåˆšå¥½æ˜¯åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°çš„åã€‚ä»£å…¥æœ€ä¼˜åˆ¤åˆ«å™¨å³å…¬å¼2ï¼Œå†è¿›è¡Œç®€å•çš„å˜æ¢å¯ä»¥å¾—åˆ° (å…¬å¼4) å˜æ¢æˆè¿™ä¸ªæ ·å­æ˜¯ä¸ºäº†å¼•å…¥Kullbackâ€“Leibler divergenceï¼ˆç®€ç§°KLæ•£åº¦ï¼‰å’ŒJensen-Shannon divergenceï¼ˆç®€ç§°JSæ•£åº¦ï¼‰è¿™ä¸¤ä¸ªé‡è¦çš„ç›¸ä¼¼åº¦è¡¡é‡æŒ‡æ ‡ã€‚ KLæ•£åº¦å’ŒJSæ•£åº¦ï¼š (å…¬å¼5ï¼šKLæ•£åº¦) (å…¬å¼6ï¼šJSæ•£åº¦) äºæ˜¯å…¬å¼4å°±å¯ä»¥ç»§ç»­å†™æˆ (å…¬å¼7) æ ¹æ®åŸå§‹GANå®šä¹‰çš„åˆ¤åˆ«å™¨lossï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æœ€ä¼˜åˆ¤åˆ«å™¨çš„å½¢å¼ï¼›è€Œåœ¨æœ€ä¼˜åˆ¤åˆ«å™¨çš„ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠåŸå§‹GANå®šä¹‰çš„ç”Ÿæˆå™¨lossç­‰ä»·å˜æ¢ä¸ºæœ€å°åŒ–çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒä¹‹é—´çš„JSæ•£åº¦ã€‚æˆ‘ä»¬è¶Šè®­ç»ƒåˆ¤åˆ«å™¨ï¼Œå®ƒå°±è¶Šæ¥è¿‘æœ€ä¼˜ï¼Œæœ€å°åŒ–ç”Ÿæˆå™¨çš„lossä¹Ÿå°±ä¼šè¶Šè¿‘ä¼¼äºæœ€å°åŒ–å’Œä¹‹é—´çš„JSæ•£åº¦ã€‚ é—®é¢˜å°±å‡ºåœ¨è¿™ä¸ªJSæ•£åº¦ä¸Šã€‚æˆ‘ä»¬ä¼šå¸Œæœ›å¦‚æœä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´è¶Šæ¥è¿‘å®ƒä»¬çš„JSæ•£åº¦è¶Šå°ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–JSæ•£åº¦å°±èƒ½å°†â€œæ‹‰å‘â€ï¼Œæœ€ç»ˆä»¥å‡ä¹±çœŸã€‚è¿™ä¸ªå¸Œæœ›åœ¨ä¸¤ä¸ªåˆ†å¸ƒæœ‰æ‰€é‡å çš„æ—¶å€™æ˜¯æˆç«‹çš„ï¼Œä½†æ˜¯å¦‚æœä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼Œæˆ–è€…å®ƒä»¬é‡å çš„éƒ¨åˆ†å¯å¿½ç•¥ï¼Œå®ƒä»¬çš„JSæ•£åº¦æ˜¯å¤šå°‘å‘¢ï¼Ÿ ç­”æ¡ˆæ˜¯ï¼Œå› ä¸ºå¯¹äºä»»æ„ä¸€ä¸ªxåªæœ‰å››ç§å¯èƒ½ï¼š ä¸” ä¸” ä¸” ä¸” ç¬¬ä¸€ç§å¯¹è®¡ç®—JSæ•£åº¦æ— è´¡çŒ®ï¼Œç¬¬äºŒç§æƒ…å†µç”±äºé‡å éƒ¨åˆ†å¯å¿½ç•¥æ‰€ä»¥è´¡çŒ®ä¹Ÿä¸º0ï¼Œç¬¬ä¸‰ç§æƒ…å†µå¯¹å…¬å¼7å³è¾¹ç¬¬ä¸€ä¸ªé¡¹çš„è´¡çŒ®æ˜¯ï¼Œç¬¬å››ç§æƒ…å†µä¸ä¹‹ç±»ä¼¼ï¼Œæ‰€ä»¥æœ€ç»ˆï¼šã€‚ æ¢å¥è¯è¯´ï¼Œæ— è®ºè·Ÿæ˜¯è¿œåœ¨å¤©è¾¹ï¼Œè¿˜æ˜¯è¿‘åœ¨çœ¼å‰ï¼Œåªè¦å®ƒä»¬ä¿©æ²¡æœ‰ä¸€ç‚¹é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥ï¼ŒJSæ•£åº¦å°±å›ºå®šæ˜¯å¸¸æ•°ï¼Œè€Œè¿™å¯¹äºæ¢¯åº¦ä¸‹é™æ–¹æ³•æ„å‘³ç€â€”â€”æ¢¯åº¦ä¸º0ï¼æ­¤æ—¶å¯¹äºæœ€ä¼˜åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨è‚¯å®šæ˜¯å¾—ä¸åˆ°ä¸€ä¸ç‚¹æ¢¯åº¦ä¿¡æ¯çš„ï¼›å³ä½¿å¯¹äºæ¥è¿‘æœ€ä¼˜çš„åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨ä¹Ÿæœ‰å¾ˆå¤§æœºä¼šé¢ä¸´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚ ä½†æ˜¯ä¸ä¸é‡å æˆ–é‡å éƒ¨åˆ†å¯å¿½ç•¥çš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿä¸ä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šéå¸¸å¤§ã€‚æ¯”è¾ƒä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šå½“ä¸çš„æ”¯æ’‘é›†ï¼ˆsupportï¼‰æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ï¼ˆmanifoldï¼‰æ—¶ï¼Œä¸é‡å éƒ¨åˆ†æµ‹åº¦ï¼ˆmeasureï¼‰ä¸º0çš„æ¦‚ç‡ä¸º1ã€‚ æ‰€ä»¥å…¶å®åœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œç”¨ä¸€ä¸ªå¾ˆä¼˜çš„åˆ¤åˆ«å™¨å»è®­ç»ƒå¥½ä¸€ä¸ªç½‘ç»œçš„æ¦‚ç‡å…¶å®å¾ˆå°ã€‚ ä¸ç„¶è¿˜æœ‰ä»€ä¹ˆå¯¹æŠ—çš„æ„ä¹‰å‘¢ï¼Œå¯¹å§ã€‚ ãƒ¾(â‰§âˆ‡â‰¦*)ã‚ å‚è€ƒï¼š éƒ‘åæ»¨ï¼šä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GANzhuanlan.zhihu.com "},"other/FLOPS.html":{"url":"other/FLOPS.html","title":"ç½‘ç»œFLOPSè®¡ç®—","keywords":"","body":"torchprof A minimal dependency library for layer-by-layer profiling of Pytorch models. All metrics are derived using the PyTorch autograd profiler. Quickstart pip install torchprof import torch import torchvision import torchprof model = torchvision.models.alexnet(pretrained=False).cuda() x = torch.rand([1, 3, 224, 224]).cuda() with torchprof.Profile(model, use_cuda=True) as prof: model(x) print(prof.display(show_events=False)) # equivalent to `print(prof)` and `print(prof.display())` Module | Self CPU total | CPU total | CUDA total ---------------|----------------|-----------|----------- AlexNet | | | â”œâ”€â”€ features | | | â”‚â”œâ”€â”€ 0 | 1.956ms | 7.714ms | 7.787ms â”‚â”œâ”€â”€ 1 | 68.880us | 68.880us | 69.632us â”‚â”œâ”€â”€ 2 | 85.639us | 155.948us | 155.648us â”‚â”œâ”€â”€ 3 | 253.419us | 970.386us | 1.747ms â”‚â”œâ”€â”€ 4 | 18.919us | 18.919us | 19.584us â”‚â”œâ”€â”€ 5 | 30.910us | 54.900us | 55.296us â”‚â”œâ”€â”€ 6 | 132.839us | 492.367us | 652.192us â”‚â”œâ”€â”€ 7 | 17.990us | 17.990us | 18.432us â”‚â”œâ”€â”€ 8 | 87.219us | 310.776us | 552.544us â”‚â”œâ”€â”€ 9 | 17.620us | 17.620us | 17.536us â”‚â”œâ”€â”€ 10 | 85.690us | 303.120us | 437.248us â”‚â”œâ”€â”€ 11 | 17.910us | 17.910us | 18.400us â”‚â””â”€â”€ 12 | 29.239us | 51.488us | 52.288us â”œâ”€â”€ avgpool | 49.230us | 85.740us | 88.960us â””â”€â”€ classifier | | | â”œâ”€â”€ 0 | 626.236us | 1.239ms | 1.362ms â”œâ”€â”€ 1 | 235.669us | 235.669us | 635.008us â”œâ”€â”€ 2 | 17.990us | 17.990us | 18.432us â”œâ”€â”€ 3 | 31.890us | 56.770us | 57.344us â”œâ”€â”€ 4 | 39.280us | 39.280us | 212.128us â”œâ”€â”€ 5 | 16.800us | 16.800us | 17.600us â””â”€â”€ 6 | 38.459us | 38.459us | 79.872us To see the low level operations that occur within each layer, print the contents of prof.display(show_events=True). Module | Self CPU total | CPU total | CUDA total ------------------------------|----------------|-----------|----------- AlexNet | | | â”œâ”€â”€ features | | | â”‚â”œâ”€â”€ 0 | | | â”‚â”‚â”œâ”€â”€ conv2d | 15.740us | 1.956ms | 1.972ms â”‚â”‚â”œâ”€â”€ convolution | 12.000us | 1.940ms | 1.957ms â”‚â”‚â”œâ”€â”€ _convolution | 36.590us | 1.928ms | 1.946ms â”‚â”‚â”œâ”€â”€ contiguous | 6.600us | 6.600us | 6.464us â”‚â”‚â””â”€â”€ cudnn_convolution | 1.885ms | 1.885ms | 1.906ms â”‚â”œâ”€â”€ 1 | | | â”‚â”‚â””â”€â”€ relu_ | 68.880us | 68.880us | 69.632us â”‚â”œâ”€â”€ 2 | | | â”‚â”‚â”œâ”€â”€ max_pool2d | 15.330us | 85.639us | 84.992us â”‚â”‚â””â”€â”€ max_pool2d_with_indices | 70.309us | 70.309us | 70.656us â”‚â”œâ”€â”€ 3 | | | ... The original Pytorch EventList can be returned by calling raw() on the profile instance. trace, event_lists_dict = prof.raw() print(trace[2]) # Trace(path=('AlexNet', 'features', '0'), leaf=True, module=Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))) print(event_lists_dict[trace[2].path][0]) --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- Name Self CPU total % Self CPU total CPU total % CPU total CPU time avg CUDA total % CUDA total CUDA time avg Number of Calls --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- conv2d 0.80% 15.740us 100.00% 1.956ms 1.956ms 25.32% 1.972ms 1.972ms 1 convolution 0.61% 12.000us 99.20% 1.940ms 1.940ms 25.14% 1.957ms 1.957ms 1 _convolution 1.87% 36.590us 98.58% 1.928ms 1.928ms 24.99% 1.946ms 1.946ms 1 contiguous 0.34% 6.600us 0.34% 6.600us 6.600us 0.08% 6.464us 6.464us 1 cudnn_convolution 96.37% 1.885ms 96.37% 1.885ms 1.885ms 24.47% 1.906ms 1.906ms 1 --------------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- --------------- Self CPU time total: 1.956ms CUDA time total: 7.787ms Layers can be selected for individually using the optional paths kwarg. Profiling is ignored for all other layers. model = torchvision.models.alexnet(pretrained=False) x = torch.rand([1, 3, 224, 224]) # Layer does not have to be a leaf layer paths = [(\"AlexNet\", \"features\", \"3\"), (\"AlexNet\", \"classifier\")] with torchprof.Profile(model, paths=paths) as prof: model(x) print(prof) Module | Self CPU total | CPU total | CUDA total ---------------|----------------|-----------|----------- AlexNet | | | â”œâ”€â”€ features | | | â”‚â”œâ”€â”€ 0 | | | â”‚â”œâ”€â”€ 1 | | | â”‚â”œâ”€â”€ 2 | | | â”‚â”œâ”€â”€ 3 | 2.846ms | 11.368ms | 0.000us â”‚â”œâ”€â”€ 4 | | | â”‚â”œâ”€â”€ 5 | | | â”‚â”œâ”€â”€ 6 | | | â”‚â”œâ”€â”€ 7 | | | â”‚â”œâ”€â”€ 8 | | | â”‚â”œâ”€â”€ 9 | | | â”‚â”œâ”€â”€ 10 | | | â”‚â”œâ”€â”€ 11 | | | â”‚â””â”€â”€ 12 | | | â”œâ”€â”€ avgpool | | | â””â”€â”€ classifier | 12.016ms | 12.206ms | 0.000us â”œâ”€â”€ 0 | | | â”œâ”€â”€ 1 | | | â”œâ”€â”€ 2 | | | â”œâ”€â”€ 3 | | | â”œâ”€â”€ 4 | | | â”œâ”€â”€ 5 | | | â””â”€â”€ 6 | | | Self CPU Time vs CPU Time "}}