
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Image-to-Image 的论文汇总（含 GitHub 代码） · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="GNN.html" />
    
    
    <link rel="prev" href="paper_write.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../introduction/0.html">
            
                <a href="../introduction/0.html">
            
                    
                    送给研一入学的你们—炼丹师入门手册
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../introduction/AI_system.html">
            
                <a href="../introduction/AI_system.html">
            
                    
                    为什么要使得AI System具备可解释性呢？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../code_technique/">
            
                <a href="../code_technique/">
            
                    
                    编程技巧
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../code_technique/python/python_technique.html">
            
                <a href="../code_technique/python/python_technique.html">
            
                    
                    python编程技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../code_technique/python/sort.html">
            
                <a href="../code_technique/python/sort.html">
            
                    
                    python常见排序
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../code_technique/python/opencv.html">
            
                <a href="../code_technique/python/opencv.html">
            
                    
                    opencv-python极速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../code_technique/pytorch/pytorch1.html">
            
                <a href="../code_technique/pytorch/pytorch1.html">
            
                    
                    pytorch常用代码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../code_technique/pytorch/pytorch2.html">
            
                <a href="../code_technique/pytorch/pytorch2.html">
            
                    
                    pytorch常用代码段合集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="../code_technique/pytorch/pytorch_train.html">
            
                <a href="../code_technique/pytorch/pytorch_train.html">
            
                    
                    pytorch训练技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="../code_technique/pytorch/pytorch_.html">
            
                <a href="../code_technique/pytorch/pytorch_.html">
            
                    
                    pytorch解冻
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="../code_technique/pytorch/pytorch_vision.html">
            
                <a href="../code_technique/pytorch/pytorch_vision.html">
            
                    
                    pytorch网络可视化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="../code_technique/pytorch/PSNR_SSIM.html">
            
                <a href="../code_technique/pytorch/PSNR_SSIM.html">
            
                    
                    PSNR&&SSIM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="../code_technique/pytorch/SPP.html">
            
                <a href="../code_technique/pytorch/SPP.html">
            
                    
                    SPP
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="../code_technique/pytorch/Tensor_to_img_imge_to_tensor.html">
            
                <a href="../code_technique/pytorch/Tensor_to_img_imge_to_tensor.html">
            
                    
                    Tensor to img && imge to tensor
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../linux/">
            
                <a href="../linux/">
            
                    
                    Linux
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../linux/linux_technique.html">
            
                <a href="../linux/linux_technique.html">
            
                    
                    Linux技巧
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../linux/linux_GPU.html">
            
                <a href="../linux/linux_GPU.html">
            
                    
                    Linux显卡驱动修复
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="./">
            
                <a href="./">
            
                    
                    论文
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="paper_write.html">
            
                <a href="paper_write.html">
            
                    
                    论文写作
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.6.2" data-path="Image_to_Image.html">
            
                <a href="Image_to_Image.html">
            
                    
                    Image-to-Image 的论文汇总（含 GitHub 代码）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="GNN.html">
            
                <a href="GNN.html">
            
                    
                    GNN综述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.4" data-path="Perceptual_GAN_for_Small_Object_Detection.html">
            
                <a href="Perceptual_GAN_for_Small_Object_Detection.html">
            
                    
                    Perceptual GAN for Small Object Detection阅读笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.5" data-path="GMMN.html">
            
                <a href="GMMN.html">
            
                    
                    GAN变体-GMMN 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.6" data-path="Deformable_Kernels.html">
            
                <a href="Deformable_Kernels.html">
            
                    
                    图像视频去噪中的Deformable Kernels
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.7" data-path="Isolating_Sources_of_Disentanglement_in_VAEs.html">
            
                <a href="Isolating_Sources_of_Disentanglement_in_VAEs.html">
            
                    
                    Isolating Sources of Disentanglement in VAEs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.8" data-path="Spectral_Normalization.html">
            
                <a href="Spectral_Normalization.html">
            
                    
                    Spectral Normalization 谱归一化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.9" data-path="Unbalanced_sample_loss.html">
            
                <a href="Unbalanced_sample_loss.html">
            
                    
                    不均衡样本loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.10" data-path="NN.html">
            
                <a href="NN.html">
            
                    
                    论文神经网络示意图
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../super_resolution/">
            
                <a href="../super_resolution/">
            
                    
                    超分辨率
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../super_resolution/SR_summarize.html">
            
                <a href="../super_resolution/SR_summarize.html">
            
                    
                    超分辨率方向综述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../super_resolution/SR.html">
            
                <a href="../super_resolution/SR.html">
            
                    
                    超分辨率技术
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../super_resolution/code_dataset.html">
            
                <a href="../super_resolution/code_dataset.html">
            
                    
                    超分辨率代码数据集合集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../super_resolution/baseline.html">
            
                <a href="../super_resolution/baseline.html">
            
                    
                    超分辨率baseline
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../super_resolution/loss.html">
            
                <a href="../super_resolution/loss.html">
            
                    
                    超分辨率的损失函数总结
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../data/">
            
                <a href="../data/">
            
                    
                    图片和数据处理
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="../data/picture.html">
            
                <a href="../data/picture.html">
            
                    
                    图片处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.2" data-path="../data/picture_enhance.html">
            
                <a href="../data/picture_enhance.html">
            
                    
                    图像数据增强
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.3" data-path="../data/data.html">
            
                <a href="../data/data.html">
            
                    
                    数据增强
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.4" data-path="../data/imaaug.html">
            
                <a href="../data/imaaug.html">
            
                    
                    imaaug 数据增强大杀器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../math/">
            
                <a href="../math/">
            
                    
                    数学
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.2.1" data-path="../math/matrix.html">
            
                <a href="../math/matrix.html">
            
                    
                    矩阵总结
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.2" data-path="../math/distribution_show.html">
            
                <a href="../math/distribution_show.html">
            
                    
                    分布
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.3" data-path="../math/affine_transformation.html">
            
                <a href="../math/affine_transformation.html">
            
                    
                    仿射变换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.4" data-path="../math/graph.html">
            
                <a href="../math/graph.html">
            
                    
                    图的基本概念
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../other/">
            
                <a href="../other/">
            
                    
                    其他
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.3.1" data-path="../other/discriminator_train.html">
            
                <a href="../other/discriminator_train.html">
            
                    
                    判别器训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.2" data-path="../other/FLOPS.html">
            
                <a href="../other/FLOPS.html">
            
                    
                    网络FLOPS计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Image-to-Image 的论文汇总（含 GitHub 代码）</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="image-to-image-&#x7684;&#x8BBA;&#x6587;&#x6C47;&#x603B;&#xFF08;&#x542B;-github-&#x4EE3;&#x7801;&#xFF09;">Image-to-Image &#x7684;&#x8BBA;&#x6587;&#x6C47;&#x603B;&#xFF08;&#x542B; GitHub &#x4EE3;&#x7801;&#xFF09;</h1>
<p>&#x56FE;&#x50CF;&#x751F;&#x6210;&#x4E00;&#x76F4;&#x662F;&#x8BA1;&#x7B97;&#x673A;&#x89C6;&#x89C9;&#x9886;&#x57DF;&#x975E;&#x5E38;&#x6709;&#x610F;&#x601D;&#x7684;&#x65B9;&#x5411;&#xFF0C;&#x56FE;&#x50CF;&#x5230;&#x56FE;&#x50CF;&#x7684;&#x53D8;&#x6362;&#x662F;&#x5176;&#x4E2D;&#x4E00;&#x4E2A;&#x975E;&#x5E38;&#x91CD;&#x8981;&#x7684;&#x5E94;&#x7528;&#xFF0C;&#x4F7F;&#x7528;&#x56FE;&#x50CF;&#x5230;&#x56FE;&#x50CF;&#x7684;&#x53D8;&#x6362;&#xFF0C;&#x53EF;&#x4EE5;&#x5B8C;&#x6210;&#x975E;&#x5E38;&#x591A;&#x6709;&#x8DA3;&#x7684;&#x5E94;&#x7528;&#xFF0C;&#x53EF;&#x4EE5;&#x628A;&#x9ED1;&#x718A;&#x53D8;&#x6210;&#x718A;&#x732B;&#xFF0C;&#x628A;&#x4F60;&#x7684;&#x7167;&#x7247;&#x6362;&#x6210;&#x522B;&#x4EBA;&#x7684;&#x8868;&#x60C5;&#xFF0C;&#x8FD8;&#x53EF;&#x4EE5;&#x628A;&#x666E;&#x901A;&#x7684;&#x7167;&#x7247;&#x53D8;&#x6210;&#x6BD5;&#x52A0;&#x7D22;&#x98CE;&#x683C;&#x7684;&#x6CB9;&#x753B;&#xFF0C;&#x81EA;&#x4ECE;GAN&#x6A2A;&#x7A7A;&#x51FA;&#x4E16;&#x4E4B;&#x540E;&#xFF0C;&#x8FD9;&#x65B9;&#x9762;&#x7684;&#x5E94;&#x7528;&#x4E5F;&#x8D8A;&#x6765;&#x8D8A;&#x591A;&#xFF0C;&#x4E0B;&#x9762;&#x662F;&#x5BF9;&#x8FD9;&#x4E2A;&#x9886;&#x57DF;&#x7684;&#x76F8;&#x5173;&#x8BBA;&#x6587;&#x7684;&#x4E00;&#x4E2A;&#x6574;&#x7406;&#xFF0C;&#x800C;&#x4E14;&#x5927;&#x90E8;&#x5206;&#x90FD;&#x6709;&#x4EE3;&#x7801;&#xFF01;</p>
<p>github&#x5730;&#x5740;&#xFF1A;<a href="https://github.com/ExtremeMart/image-to-image-papers" target="_blank">https://github.com/ExtremeMart/image-to-image-papers</a></p>
<p>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x56FE;&#x50CF;&#x5230;&#x56FE;&#x50CF;&#x7684;&#x8BBA;&#x6587;&#x7684;&#x6C47;&#x603B;&#x3002;&#x8BBA;&#x6587;&#x6309;&#x7167;arXiv&#x4E0A;&#x7B2C;&#x4E00;&#x6B21;&#x63D0;&#x4EA4;&#x65F6;&#x95F4;&#x6392;&#x5E8F;&#x3002;</p>
<h1 id="&#x76D1;&#x7763;&#x5B66;&#x4E60;">&#x76D1;&#x7763;&#x5B66;&#x4E60;</h1>
<table>
<thead>
<tr>
<th style="text-align:left">Note</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">Paper</th>
<th style="text-align:left">Conference</th>
<th style="text-align:left">paper link</th>
<th style="text-align:left">code link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">pix2pix</td>
<td style="text-align:left">Image-to-Image Translation with Conditional Adversarial Networks</td>
<td style="text-align:left">CVPR 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1611.07004" target="_blank">1611.07004</a></td>
<td style="text-align:left"><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank">junyanz/pytorch-CycleGAN-and-pix2pix</a></td>
</tr>
<tr>
<td style="text-align:left">texture guided</td>
<td style="text-align:left">TextureGAN</td>
<td style="text-align:left">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1706.02823" target="_blank">1706.02823</a></td>
<td style="text-align:left"><a href="https://github.com/janesjanes/Pytorch-TextureGAN" target="_blank">janesjanes/Pytorch-TextureGAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Contextual GAN</td>
<td style="text-align:left">Image Generation from Sketch Constraint Using Contextual GAN</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.08972" target="_blank">1711.08972</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">pix2pix-HD</td>
<td style="text-align:left">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.11585" target="_blank">1711.11585</a></td>
<td style="text-align:left"><a href="https://github.com/NVIDIA/pix2pixHD" target="_blank">NVIDIA/pix2pixHD</a></td>
</tr>
<tr>
<td style="text-align:left">one-to-many</td>
<td style="text-align:left">BicycleGAN</td>
<td style="text-align:left">Toward Multimodal Image-to-Image Translation</td>
<td style="text-align:left">NIPS 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.11586" target="_blank">1711.11586</a></td>
<td style="text-align:left"><a href="https://github.com/junyanz/BicycleGAN" target="_blank">junyanz/BicycleGAN</a></td>
</tr>
<tr>
<td style="text-align:left">keypoint guided</td>
<td style="text-align:left">G2-GAN</td>
<td style="text-align:left">Geometry Guided Adversarial Facial Expression Synthesis</td>
<td style="text-align:left">MM 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1712.03474" target="_blank">1712.03474</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">contour2im</td>
<td style="text-align:left">Smart, Sparse Contours to Represent and Edit Images</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1712.08232" target="_blank">1712.08232</a></td>
<td style="text-align:left"><a href="https://contour2im.github.io/" target="_blank">website</a></td>
</tr>
<tr>
<td style="text-align:left">disentangle</td>
<td style="text-align:left">Cross-domain disentanglement networks</td>
<td style="text-align:left">Image-to-image translation for cross-domain disentanglement</td>
<td style="text-align:left">NIPS 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1805.09730" target="_blank">1805.09730</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">video</td>
<td style="text-align:left">vid2vid</td>
<td style="text-align:left">Video-to-Video Synthesis</td>
<td style="text-align:left">NIPS 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1808.06601" target="_blank">1808.06601</a></td>
<td style="text-align:left"><a href="https://github.com/NVIDIA/vid2vid" target="_blank">NVIDIA/vid2vid</a></td>
</tr>
<tr>
<td style="text-align:left">video</td>
<td style="text-align:left">pix2pix-HD + Temporal Smoothing + faceGAN</td>
<td style="text-align:left">Everybody Dance Now</td>
<td style="text-align:left">ECCVW 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1808.07371" target="_blank">1808.07371</a></td>
<td style="text-align:left"><a href="https://carolineec.github.io/everybody_dance_now/" target="_blank">website</a></td>
</tr>
</tbody>
</table>
<h1 id="&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;">&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;</h1>
<p><strong>&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;-&#x901A;&#x7528;</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Note</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">Paper</th>
<th style="text-align:left">Conference</th>
<th style="text-align:left">paper link</th>
<th style="text-align:left">code link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">DTN</td>
<td style="text-align:left">Unsupervised Cross-Domain Image Generation</td>
<td style="text-align:left">ICLR 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1611.02200" target="_blank">1611.02200</a></td>
<td style="text-align:left"><a href="https://github.com/yunjey/domain-transfer-network" target="_blank">yunjey/domain-transfer-network (unofficial)</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">UNIT</td>
<td style="text-align:left">Unsupervised image-to-image translation networks</td>
<td style="text-align:left">NIPS 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1703.00848" target="_blank">1703.00848</a></td>
<td style="text-align:left"><a href="https://github.com/mingyuliutw/UNIT" target="_blank">mingyuliutw/UNIT</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">DiscoGAN</td>
<td style="text-align:left">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</td>
<td style="text-align:left">ICML 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1703.05192" target="_blank">1703.05192</a></td>
<td style="text-align:left"><a href="https://github.com/SKTBrain/DiscoGAN" target="_blank">SKTBrain/DiscoGAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CycleGAN</td>
<td style="text-align:left">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</td>
<td style="text-align:left">ICCV 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1703.10593" target="_blank">1703.10593</a></td>
<td style="text-align:left"><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank">junyanz/pytorch-CycleGAN-and-pix2pix</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">DualGAN</td>
<td style="text-align:left">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</td>
<td style="text-align:left">ICCV 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1704.02510" target="_blank">1704.02510</a></td>
<td style="text-align:left"><a href="https://github.com/duxingren14/DualGAN" target="_blank">duxingren14/DualGAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">DistanceGAN</td>
<td style="text-align:left">One-Sided Unsupervised Domain Mapping</td>
<td style="text-align:left">NIPS 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1706.00826" target="_blank">1706.00826</a></td>
<td style="text-align:left"><a href="https://github.com/sagiebenaim/DistanceGAN" target="_blank">sagiebenaim/DistanceGAN</a></td>
</tr>
<tr>
<td style="text-align:left">semi supervised</td>
<td style="text-align:left">Triangle GAN</td>
<td style="text-align:left">Triangle Generative Adversarial Networks</td>
<td style="text-align:left">NIPS 2017</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1709.06548" target="_blank">1709.06548</a></td>
<td style="text-align:left"><a href="https://github.com/LiqunChen0606/Triangle-GAN" target="_blank">LiqunChen0606/Triangle-GAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CartoonGAN</td>
<td style="text-align:left">CartoonGAN: Generative Adversarial Networks for Photo Cartoonization</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf" target="_blank">thecvf</a></td>
<td style="text-align:left"><a href="https://github.com/Yijunmaverick/CartoonGAN-Test-Pytorch-Torch" target="_blank">unofficial test</a>, <a href="https://github.com/znxlwm/pytorch-CartoonGAN" target="_blank">unofficial pytorch</a></td>
</tr>
<tr>
<td style="text-align:left">non-adversarial</td>
<td style="text-align:left">NAM</td>
<td style="text-align:left">NAM: Non-Adversarial Unsupervised Domain Mapping</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1806.00804" target="_blank">1806.00804</a></td>
<td style="text-align:left"><a href="https://github.com/facebookresearch/nam" target="_blank">facebookresearch/nam</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">SCAN</td>
<td style="text-align:left">Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1807.08536" target="_blank">1807.08536</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">dilated conv, improve shape deform.</td>
<td style="text-align:left">GANimorph</td>
<td style="text-align:left">Improved Shape Deformation in Unsupervised Image to Image Translation</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1808.04325" target="_blank">1808.04325</a></td>
<td style="text-align:left"><a href="https://github.com/brownvc/ganimorph/" target="_blank">brownvc/ganimorph</a></td>
</tr>
<tr>
<td style="text-align:left">instance aware</td>
<td style="text-align:left">InstaGAN</td>
<td style="text-align:left">Instance-aware image-to-image translation</td>
<td style="text-align:left">ICLR 2019 (in review)</td>
<td style="text-align:left"><a href="https://openreview.net/pdf?id=ryxwJhC9YX" target="_blank">openreview</a></td>
</tr>
</tbody>
</table>
<p><strong>&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;-&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;&#x6216;&#x8005;&#x6A21;&#x677F;&#x5BFC;&#x5411;&#x673A;&#x5236;</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Note</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">Paper</th>
<th style="text-align:left">Conference</th>
<th style="text-align:left">paper link</th>
<th style="text-align:left">code link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">mask</td>
<td style="text-align:left">ContrastGAN</td>
<td style="text-align:left">Generative Semantic Manipulation with Mask-Contrasting GAN</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1708.00315" target="_blank">1708.00315</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">attention</td>
<td style="text-align:left">DA-GAN</td>
<td style="text-align:left">DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1802.06454" target="_blank">1802.06454</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">mask / attention</td>
<td style="text-align:left">Attention-GAN</td>
<td style="text-align:left">Attention-GAN for Object Trans&#xFB01;guration in Wild Images</td>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1803.06798" target="_blank">1803.06798</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">attention</td>
<td style="text-align:left">Attention guided GAN</td>
<td style="text-align:left">Unsupervised Attention-guided Image to Image Translation</td>
<td style="text-align:left">NIPS 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1806.02311" target="_blank">1806.02311</a></td>
<td style="text-align:left"><a href="https://github.com/AlamiMejjati/Unsupervised-Attention-guided-Image-to-Image-Translation" target="_blank">AlamiMejjati/Unsupervised-Attention-guided-Image-to-Image-Translation</a></td>
</tr>
<tr>
<td style="text-align:left">attention, one-sided</td>
<td style="text-align:left"></td>
<td style="text-align:left">Show, Attend and Translate: Unsupervised Image Translation with Self-Regularization and Attention</td>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1806.06195" target="_blank">1806.06195</a></td>
</tr>
</tbody>
</table>
<p><strong>&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;-&#x591A;&#x5BF9;&#x591A;&#xFF08;&#x5C5E;&#x6027;&#xFF09;</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Note</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">Paper</th>
<th style="text-align:left">Conference</th>
<th style="text-align:left">paper link</th>
<th style="text-align:left">code link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Conditional CycleGAN</td>
<td style="text-align:left">Conditional CycleGAN for Attribute Guided Face Image Generation</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1705.09966" target="_blank">1705.09966</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">StarGAN</td>
<td style="text-align:left">StarGAN: Uni&#xFB01;ed Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.09020" target="_blank">1711.09020</a></td>
<td style="text-align:left"><a href="https://github.com/yunjey/StarGAN" target="_blank">yunjey/StarGAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">AttGAN</td>
<td style="text-align:left">AttGAN: Facial Attribute Editing by Only Changing What You Want</td>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.10678" target="_blank">1711.10678</a></td>
<td style="text-align:left"><a href="https://github.com/LynnHo/AttGAN-Tensorflow" target="_blank">LynnHo/AttGAN-Tensorflow</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">ComboGAN</td>
<td style="text-align:left">ComboGAN: Unrestrained Scalability for Image Domain Translation</td>
<td style="text-align:left">CVPRW 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1712.06909" target="_blank">1712.06909</a></td>
<td style="text-align:left"><a href="https://github.com/AAnoosheh/ComboGAN" target="_blank">AAnoosheh/ComboGAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">AugCGAN (Augmented CycleGAN)</td>
<td style="text-align:left">Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data</td>
<td style="text-align:left">ICML 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1802.10151" target="_blank">1802.10151</a></td>
<td style="text-align:left"><a href="https://github.com/aalmah/augmented_cyclegan" target="_blank">aalmah/augmented_cyclegan</a></td>
</tr>
<tr>
<td style="text-align:left">sparsely grouped dataset</td>
<td style="text-align:left">SG-GAN</td>
<td style="text-align:left">Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation</td>
<td style="text-align:left">MM 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1805.07509" target="_blank">1805.07509</a></td>
<td style="text-align:left"><a href="https://github.com/zhangqianhui/Sparsely-Grouped-GAN" target="_blank">zhangqianhui/Sparsely-Grouped-GAN</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">GANimation</td>
<td style="text-align:left">GANimation: Anatomically-aware Facial Animation from a Single Image</td>
<td style="text-align:left">ECCV 2018 (honorable mention)</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1807.09251" target="_blank">1807.09251</a></td>
<td style="text-align:left"><a href="https://github.com/albertpumarola/GANimation" target="_blank">albertpumarola/GANimation</a></td>
</tr>
</tbody>
</table>
<p><strong>&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;-&#x5206;&#x79BB;&#xFF08;&#x4E0E;/&#x6216;&#x6837;&#x672C;&#x5BFC;&#x5411;&#xFF09;</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Note</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">Paper</th>
<th style="text-align:left">Conference</th>
<th style="text-align:left">paper link</th>
<th style="text-align:left">code link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">XGAN</td>
<td style="text-align:left">XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings</td>
<td style="text-align:left">ICML 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1711.05139" target="_blank">1711.05139</a></td>
<td style="text-align:left"><a href="https://google.github.io/cartoonset/" target="_blank">dataset</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">ELEGANT</td>
<td style="text-align:left">ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1803.10562" target="_blank">1803.10562</a></td>
<td style="text-align:left"><a href="https://github.com/Prinsphield/ELEGANT" target="_blank">Prinsphield/ELEGANT</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">MUNIT</td>
<td style="text-align:left">Multimodal Unsupervised Image-to-Image Translation</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1804.04732" target="_blank">1804.04732</a></td>
<td style="text-align:left"><a href="https://github.com/NVlabs/MUNIT" target="_blank">NVlabs/MUNIT</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">cd-GAN (Conditional DualGAN)</td>
<td style="text-align:left">Conditional Image-to-Image Translation</td>
<td style="text-align:left">CVPR 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1805.00251" target="_blank">1805.00251</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">EG-UNIT</td>
<td style="text-align:left">Exemplar Guided Unsupervised Image-to-Image Translation</td>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1805.11145" target="_blank">1805.11145</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">DRIT</td>
<td style="text-align:left">Diverse Image-to-Image Translation via Disentangled Representations</td>
<td style="text-align:left">ECCV 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1808.00948" target="_blank">1808.00948</a></td>
<td style="text-align:left"><a href="https://github.com/HsinYingLee/DRIT" target="_blank">HsinYingLee/DRIT</a></td>
</tr>
<tr>
<td style="text-align:left">non-disentangle, face makeup guided</td>
<td style="text-align:left">BeautyGAN</td>
<td style="text-align:left">BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network</td>
<td style="text-align:left">MM 2018</td>
<td style="text-align:left"><a href="https://liusi-group.com/pdf/BeautyGAN-camera-ready.pdf" target="_blank">author</a></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">UFDN</td>
<td style="text-align:left">A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</td>
<td style="text-align:left">NIPS 2018</td>
<td style="text-align:left"><a href="https://arxiv.org/abs/1809.01361" target="_blank">1809.01361</a></td>
<td style="text-align:left"><a href="https://github.com/Alexander-H-Liu/UFDN" target="_blank">Alexander-H-Liu/UFDN</a></td>
</tr>
</tbody>
</table>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="paper_write.html" class="navigation navigation-prev " aria-label="Previous page: 论文写作">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="GNN.html" class="navigation navigation-next " aria-label="Next page: GNN综述">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Image-to-Image 的论文汇总（含 GitHub 代码）","level":"1.6.2","depth":2,"next":{"title":"GNN综述","level":"1.6.3","depth":2,"path":"paper/GNN.md","ref":"paper/GNN.md","articles":[]},"previous":{"title":"论文写作","level":"1.6.1","depth":2,"path":"paper/paper_write.md","ref":"paper/paper_write.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-search","search-pro","back-to-top-button","expandable-chapters-small","back-to-top-button","chapter-fold","expandable-chapters-small","github","katex","include-codeblock","livereload"],"root":"./content","styles":{"website":"assets/styles/website.less","ebook":"assets/styles/ebook.less","pdf":"assets/styles/pdf.less","mobi":"assets/styles/mobi.less","epub":"assets/styles/epub.less"},"pluginsConfig":{"chapter-fold":{},"prism":{"css":["prismjs/themes/prism-solarizedlight.css"],"lang":{"flow":"typescript"}},"github":{"url":"https://github.com/OUCMachineLearning/OUCML"},"livereload":{},"search-pro":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"back-to-top-button":{},"expandable-chapters-small":{},"include-codeblock":{"check":false,"edit":false,"fixlang":false,"lang":"","template":"default","theme":"chrome","unindent":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"language":"zh-hans","gitbook":"*"},"file":{"path":"paper/Image_to_Image.md","mtime":"2019-10-24T04:35:18.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-10-25T10:54:29.358Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

